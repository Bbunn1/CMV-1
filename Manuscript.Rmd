---
title: "You've Gotta Keep em' Separated? Examining the Efficacy of Proximal Remedies for Causes of Method Variance"

author:
- address: 906 East 1st. St., Thibodaux, LA 70301
  affiliation: '1'
  corresponding: yes
  email: chris_castille@mac.com
  name: Christopher M. Castille
- affiliation: '2'
  name: Wayne S. Crawford
- affilitation: '3'
  name: Marcia J. Simmering
affiliation:
- id: '1'
  institution: Nicholls State University
- id: '2'
  institution: University of Texas at Arlington
- id: '3'
  institution: Louisiana Tech University
output: pdf_document
bibliography: r-references.bib
class: man
figsintext: no
figurelist: no
footnotelist: no
keywords: method variance, procedural remedies, statistical remedies
lang: english
lineno: yes
author_note: null
shorttitle: PROXIMAL CMV REMEDIES
tablelist: no
abstract: "Scholars have argued that method factors, such as common rater effects, bias estimates of covariation from same-source and single time-point investigations. In response, researchers have proposed procedural remedies. For such studies, recommended remedies include (1) presenting participants with a cover story to disguise the purpose of the survey (which addresses respondents' ability to produce data consistent with researchers' hypotheses), (2) randomizing item and scale presentation around filler scales (which addresses item and scale context effects), and (3) introducing a brief temporal separation (which addresses respondents' momentary mood). Though researchers have relied upon these proximal method variance remedies, there are no studies examining whether they nullify method variance. Here, we present the findings from two experiments utilizing the same measurement model and demonstrate that such remedies do, indeed, reduce (and in some instances, eliminate) the presence of method variance attributable to (1) consistency motifs, (2) context effects, and (3) mood. However, these sources of method variance did not substantially bias our findings. Rather, other sources of method variance (i.e., positive affectivity and negative item wording) consistently biased estimates. We conclude with recommendations for researchers wishing to addresses method bias in their same-source investigations."    
wordcount: X
---
```{r load_packages, include = FALSE}
library("papaja")
# Seed for random number generation
set.seed(42)
```
#Introduction
  Few methodological problems have been discussed more frequently than the presence and impact of method variance [@SpectorNewPerspectiveMethod2017], or variation in observations attributable to methodolgoical sources than to substantive/theoretical constructs of interest [@CampbellConvergentDiscriminantValidation1959]. Though long debated over the past several decades [e.g., @CampbellConvergentDiscriminantValidation1959; @LanceUseindependentmeasures2015; @RichardsonTaleThreePerspectives2009; @SpectorMethodVarianceOrganizational2006], the problem of method variance is often viewed as a serious one in the organizational sciences, where researchers often rely upon observations made using sources believed to share common method variance [i.e., variation in observations that is attributable to a common cause, such mood held by respondents; see @PodsakoffCommonmethodbiases2003]. Those who believe strongly that CMV is persistent problem believe that method variance threatens the substantive knowledgebase upon which practitioners rely, and so many procedural solutions have been proposed to address this eminent problem [@PodsakoffCommonmethodbiases2003; @PodsakoffSourcesMethodBias2012]. The proposed solutions are often simple (e.g., use a cover story to disguise the purpose of a study, counterbalance item presentation, and separate measurement by an interval of time) and routinely encouraged by journal editors [e.g., @AshkanasySubmittingyourmanuscript2008].   
  Unfortunately, these procedural method variance remedies have not received close examination, particularly using experimental designs that compare observations obtained using these remedies to those obtained without. These experiments are needed because they can demonstrate the extent to which a remedy addresses a proposed cause of method variance (e.g., participant beliefs about item covariation, respondent mood), resulting in reduced method variance and less biased results. Therefore, we set out to conduct these experiments and to this end we make the following contributions to the literature. First, we test the causal role of hypothesized proximal 8causes of method variance, specifically resondent consistency motifs, item context effects, and mood. Second, drawing on recent literature distinguishing between common and uncommon method variance [see @SpectorNewPerspectiveMethod2017], we demonstrate the extent to which these different sources of method variance differentially affect observations, affecting reserachers' ability to produce accurate effect size estimates. Thirdly, we explore causes of method variance that are specific to the substantive theory under investigation in our study, allowing a comparison of general and specific causes of method variance. 
##Theoretical Overview of Method Variance
	Before we dive too deeply into the literature on method bias, we should clarify the terms 'method,' 'method variance,' and 'method bias,' as there are differing perspectives. Summarizing the available literature, @PodsakoffSourcesMethodBias2012 distinguished between a broad and narrow definition. Drawing on @FiskeConvergentdiscriminantvalidationmeasurements1982 and citing several researchers who support this position [e.g., @Edwardsprosperorganizationalpsychology2008; @JohnsonAssessingimpactcommon2011; @SiemsenCommonMethodBias2010; @Weijtersstabilityindividualresponse2010], Podsakoff et al. noted that method encompasses several abstract elements (e.g., taking a paper-and-pencil instrument, responding using Likert scales, characteristics of the examiner) and when these elements are shared across methods or measures in the same investigation, there will be a convergence resulting in bias. By contrast, Lance and colleagues [@LanceIfitain2009; @LanceMethodEffectsMeasurement2010; @LanceUseindependentmeasures2015] restricted method to aspects that refer to alternative means of enumerating observations to indicate standings on latent traits (e.g., self- vs. other-report), which has been critized as omitting common rater sources of method variance [e.g., @PodsakoffSourcesMethodBias2012]. Method variance refers to the impact of a methodological aspect of study on a particular observation(s) [e.g.,  mood affecting observations obtained with a single-source single time-point design), which differs from the extent to which method variance is shared across methods, causing biased/inflated parameter estimates (e.g., negative mood both contaminating observations of two variables, inflating their covariances; see @Williamsalternativeapproachmethod1994]. This distinction is important because method variance can be unique to a specific set of observations within a dataset (e.g., halo effects contaminate supervisor appraisals of employee performance, but not subordinate variables). When such uncommon method variance is present in one's observations, observed covariances with other observations are attentuated [@SpectorNewPerspectiveMethod2017].
	Conventionally, method variance is a concern when researchers's conclusions dervie from self-report surveys administered at a single time-point because it is believed that methods affect the same item response process. Therefore, it is important to understand how methods affect the underlying item response process resulting in contaminated observations. Research from cognitive psychology suggests that four cognitive processes are associated with responding to survey items [see @Tourangeaupsychologysurveyresponse2000]: (1) comprehension (i.e., understanding items), (2) retrieval (i.e., recalling item-relevant memories), (3) judgment (i.e., assessing the completeness of a relevant memory), and (4) response (i.e., mapping one's judgement onto the available item responses). @PodsakoffCommonmethodbiases2003 later divided the last process into response selection and (5) response reporting to highlight how individuals might modify their responses to adhere to certain criteria (e.g., adjusting previous item responses to appear more consistent). @PodsakoffCommonmethodbiases2003 summarized the method variance literature into four broad potential sources of common method bias (i.e., common rater effects, item characteristic effects, item context effeccts, and measurement context effects) to explain how response processes are corrupted, resulting in contaminated observations. 
	In our manuscript, we examine the role of a few sources identified by @PodsakoffCommonmethodbiases2003 that are believed to play a role in all same-source surveys. These sources of method variance are referred to as proximal causes of method variance. In two studies, we examine the role of a specific common rater effects (i.e., implicit theories, consistency motifs, mood states, trait affectivity factors), item context effects (i.e., item/scale priming and item/scale embeddedness effects), and measurement context effects (i.e., gathering observations on predictor and criterion variables at the same point in time vs a differnt point in time), which are believed to corrupt the item response process, contaminate observations, and ultimately bias estimates of covariation. We have decided to focus on proximal causes of method variance for single-source designs because these designs are likely to be the most prevalent in the organizational and social science (applied and academic settings) due to their relatively lower cost and reduced administrative burden. As such, ensuring that our remedies work is paramount to helping researchers make the most of their research designs. Furthermore, single-source designs should be ripe for method variance because the cognitive processes of retrieval (i.e., remembering responses to previously answered items rather than the whole of one's experience), judgment (i.e., falsely judging that one has effectively retrieved relevant memories), and in some instances reporting (i.e., modifying one's responses to be consistent) may not be addressed unless a remedy is applied [see @PodsakoffCommonmethodbiases2003; @PodsakoffSourcesMethodBias2012; @Tourangeaupsychologysurveyresponse2000]. 
	
[Question: Why not examine response style indicators as well (e.g., acquiescence, disacquiescence, intra-individual response consistency)? More recently, researchers have examined the stability of response styles, or the preference for particular response categories, which reflects a clear consistency motif. In an 8-year ongoing panel study, @WetzelStabilityExtremeResponse2016 found that between 49 and 59% of the variance in state response style factors could be explained by trait response style factors, indicating a remarkable amount of response consistency across time and also raising questions about their state-like properties. Answer: Don't bring this up unless reviewers ask you to look into this. 
[Concern: Be sure to maintain a distinction between "causes" and "effects" when writing. Others don't do this, but we should be more responsible.]

###Relevant Common Rater Effects  
	Common rater effects are any artifactual covariation introduced by gathering observations on (or from) the same rater @PodsakoffCommonmethodbiases2003. Though there are many common rater effects [@PodsakoffCommonmethodbiases2003], there are four that are particularly relevant for our study: consistency motifs, implicit theories, mood states, and trait affectivity factors. Consistency motifs refer to the propensity for respondents to maintain consistency in their responses to questions. Clearly, the drive to appear consistent should affect response reporting, as individuals will desire to appear rational [@PodsakoffCommonmethodbiases2003]. Early research suggested that some individuals edit their responses to appear more consistent (Smith, 1983) or avoid inconsistencies (McGuire, 1960). Later research revealed consistency motifs might bias estimates of covariation (Harrison, McLaughlin, & Coalter, 1996). Similarly, implicit theories or illusory correlations refer to respondents beliefs about the covariation among particular traits, behaviors, and or outcomes that may not accurately reflect reality (Sternberg, Conway, Ketron, & Bernstein, 1981). These biases are believed to affect judgments of response appriopriateness (Podsakoff et al., 2003).
	To address consistency motifs and implicit theories, researchers have used cover stories (e.g., Harrison et al., 1996), which are narratives designed to both disguise the hypotheses under investigation and dispel implicit theories held by the participants (Podsakoff et al., 2003). Importantly, there is no experimental research evaluating this procedural remedy for these sources of method variance. 
	Somewhat differntly from consistency motifs and implicit theories are mood states and trait affectivity factors, which are clearly interrelated. Mood states refer to respondents momentary or brief mood state. A great deal of research indicates that one's emotional state influences the contents of what is recalled from memory by priming similarly valenced material stored in memory (cf. Blaney, 1986; Blower, 1981; Isen & Baron, 1991; Parrott & Sabini, 1990). Sometimes this has a desirable effect, such as when mood-congruency facilitates recall (Bower, 1981; Isen & Baron, 1991). However, when individuals are highly aroused, errors in recall seem more likley (Carson & Verrier, 2007). One's mood state should also be related to general affectivity (positive and negative), which refers to a respondent's propensity to view themselves or their environments in positive or negative ways (Podsakoff et al., 2003). Trait affectivity should exert similar effects on the item response process as mood, and may explain the effect of momentary mood state on the item response process. Brief, Burke, George, Robinson, and Webster (1988) observed that negative affectivity contaminates observations of stress, job and life satisfaction, depression, and the amount of affect experienced at work, resulting in biased estimates of covariation. Similarly, Williams and Anderson (1994) found that positive emotionality (analogous to positive affectivity) contaminates self-report measurements of several commonly studied organizational variables (sp. job satisfaction, organizational commitment, leader-contingent reward behavior, and job characteristics). Additionally, they found that negative emotionality (alaogous to negative affectivity) played a weaker role, only affecting  observations of job satisfaction. Similarly, Chen and Spector (1991) and Jex and Spector (1996) observed weak influences of negative affectivity on the relationships between self-reported job stress and strain variables. In short, both trait affectivity and momentary mood states may be a common source of method variance.
	To address the role of mood states, researchers have proposed using a temporal separation of measurement that increases the amount of time between when observations of  predictor and criterion variables are made (Podsakoff et al., 2003; 2012). Though it involves separation by time, this remedy addresses a proximal cause of method variance (i.e., mood state). It is assumed that introducing a temporal separation (e.g., separating measures by one week) will reduce the systematic influence of momentary mood states, resulting in estimates that are less baised. Again, there is no research examining the efficacy of this remedy.
=======
Note: Reviewers might point out that social desirability would be a better control variable than positive or negative affectivity for our substnative measurement model because the factors that we have measured are all socially desirable. However, we would like to point out that social desirabilty reflects a personality disposition high in need for approval that would become activated in contexts where there are clear cues/signals that changes in item response behavior to appear socially desirable would be desired/rewarded. We took efforts to ensure that no such cues were present in both of our conditions. Rather, trait affectivity would relate to cognitive biases (e.g., self-However, this trait could be relevant in settings where the need for approval could be motivated, resulting in changes to item response behavior (e.g., high-stakes employee selection contexts).
###Relevant Context Effects	
	Context effects refer to any artifactual covariation among observations caused by an item or scale’s location or relationship to other items or scales in a survey (e.g., item/scale priming and item/scale embeddedness effects). Podakoff et al. (2003) proposed that item context can affect retrieval and judgments of response appropriateness. Though these effects are elusive (i.e., they can be difficult to predict and control a prioi; see Tourangeau & Rasinski, 1988), Tourangeau, Rasinski, and D’Andrade (1991) demonstrated that participants respond more quickly to similar items placed closer together rather than further apart, suggesting that prior responses were more easily accessible in short-term memory. Tourangeau et al.’s (1991) findings also suggest that item intercorrelations may be a function of the consistent ordering of items within a particular instrument. If so, then participants observed data may artificially discriminate on a latent trait. Examining this possibility, Steinberg (1994) found that one item in a 20-item scale became slightly more discriminating when it was presented later in the scale rather than as the first item, which Steinberg attributed to the increase in self-awareness that occurs when individuals are asked to repeatedly reflect on their own tendencies. Additionally, Harrison et al. (1996) found that context effects produced by scale location and scale valence (negative scales presented before positive scales) affected psychometric properties such that scales presented later were less reliable. Further, when negative scales were presented first, estimated inter-scale correlations were much higher, suggesting the presence of scale embeddedness effects.
	To address item priming and item embeddedness effects, researchers have proposed counterbalancing or randomizing the order of item and/or scales within a survey, particular around a filler scale or series of filler scales (Podakoff et al., 2003; Salancik & Pfeffer, 1977). It is assumed that this remedy will reduce the systematic influence of context effects that might introduce common method variance across predictors and criteria, resulting in estimates of scale covariation that are less biased by context. Again, there is no research examining the efficacy of this remedy.
=======
Note: Make sure to consistently use the disctinction between scale and item covariation. 
###Measurement-Specific Causes of Both Common and Uncommon Method Variance
  Advocating a more nuanced view of common method variance, Spector et al. (2016) suggest that certain causes of method variance are specific to the measurement strategy employed by researchers. For instance, when gathering observations on both affective/attitudinal and behavioral factors, Spector et al. encouraged researchers to consider ways to capture the mood state exhibited by their participates, as this would be a likely common cause of method variance that would contaminate the response process underlying both sets of observations, resulting in biased estimates of covariation. Addtionally, there may be sources of method variance that are uncommon (i.e., unique) to particular methods of observation. For instance, affective/attitudinal measures might be more strongly affected by response sets (e.g., consistency motifs) than behavioral measures, which would be affected by impression management strategies. Such effects would attenuate scale covariances unless controlled (see Spector et al., 2016). Addressing these sources of variance requires statistical strategies (e.g., measured cause models; see Williams et al., 2010).
  In line with Spector et al.'s call for controlling both common and uncommon method variance, we built in measures of method variance causes that were shared across our measures and unique to specific mesures. In both of our studies, we used the same measurement model and tested the same underlying theory; namely, that proactive personality relates positively to proactive behaviors (i.e., voice and taking charge) as well as behaviors reflecting task performance (i.e., in-role behavior) and contextual performance (i.e., OCB) (see Figure 1). We decided to study these predictor-criterion relationships for three reasons that are important for both organizational scholars and method variance testing in general. First, these relationships are often described in the literature (e.g., Fuller & Marler, 2009; Grant, Gino, & Hoffman, 2011), suggesting that assuming a relationship is important to a broad audience of organizational scholars. Second, prior research suggests that these relationships may be a function of method variance (e.g., Fuller & Marler, 2009). For instance, a meta-analysis by Fuller and Marler (2009) demonstrated that these relationships are often inflated by common method variance, particularly when observations are gathered using the same source rather than multiple sources (see Table 1). They observed that the relationships between proactive personality and workplace behaviors was substantially inflated by between 129% and 308% when observations were made using a common (rather than independent) source. Though researchers have suggested that such percentages mistate the impact of method variance (e.g., Lance & Simonovsky, 2015; Spector et al., 2016), it seems clear that methodological differences are resulting and diverging estimates. Third, and most importantly, it is not clear what sources of method variance would explain such these differences. To respolve these disparities, we tested the same measurement model in our two studies, examining the effects of various hypothesized common method factors (e.g., mood, affectivity, and consistency motifs) and uncommon method factors (e.g., negative item wording). Figure b illustrates how the method factors were modeled.
=======
[Note: Readers may have noted an apparent disparity between Podsakoff et al. (2003) and Spector et al. (2016) on the role of consistency motifs. Spector et al. (2016) suggest this factor may be more relevant for affective/attitudinal factors than behavioral, whereas Podsakoff et al. (2003) suggests that consistency motifs could be equally relevant for items/scales within a measure).] 
Notes for us to discuss:
The cognitive psychology literature, specifically on heuristics and biases in decision making and judgments, informs method variance research. For instance, consider anchoring, wherein one's judgment is informed by an arbitrary prior thought in one's conciousness, within and their ability to explain item-order (or item priming) effects. In an experiment, college students were asked the questions (a) how happy are you and (b) how often are you dating. When asked in this order, the correlation between the two questions was low (r = .11). However, when asked in reverse, the correlation was 5 times larger (r = .62). Similar findings emerged with married couples about their sex life (see Strack, Martin, & Schwarz, 1988). The design of the questionnaire (in this case, the item order) explains the size of the correlation estimated between measures of life satisfaction and dating or frequency of sex, respectively, with anchoring providing the psychological explanation. 
(A few questions regarding the above: has this been replicated? the effect seems exaggerated, so I'm wondering if the effect size is reliable...What would a Fisher r-to-z test reveal? which effect is closer to the truth, and why?)

References:
Strack, F. L., Martin, L., & Scharz, N. (1988). Priming and communication: The social determinants of information use in judgments of life-satisfaction. European Journal of Social Psychology, pp. 429-442.
###General Purpose
  In two experiments, we examine the efficacy of procedural remedies for different proximal causes of common method variance while also examining the role played by other proximal causes of method variance that are specific to our measures. In both studies, we randomly assigned individuals to one of two conditions where remedies for different proximal causes of CMV have been applied or have not been applied. This allows us to test the causal role of hypothesized mechanisms that give rise method variance and also to test the extent to which these causes produce biased estimates.
#Study 1 - The Efficacy of a One Week Temporal Separation
#Method - Study 1
##Sample, Measures, and Procedure
```{r Load Packages, include=FALSE}
library(foreign)
library(qualtRics)
library(bayesboot)
library(BayesFactor)
library(BayesMed)
library(plyr)
library(psych)
library(cocor)
library(BayesianFirstAid)
library(ggplot2)
library(QRM)
library(lavaan)
library(semPlot)
library(car)
library(semTools)
library(BaylorEdPsych)
library(mi)
library(lordif)
library(mirt)
library(memisc)
library(dplyr)
library(lsr)
set.seed(19)
```
```{r Load Temporal Separation Data, include=FALSE}
library(haven)
data2 <- read_sav("Datasets/CMV.TempSep.sav")

#Delete nuissance factors.
##Marcia's outlier screen, source, and part/full-time status. 
data2$MVOutlier <- NULL
data2$Source <- NULL
data2$PartFullTime <- NULL

#Delete attitudes toward the color blue.
data2$Blue1 <- NULL
data2$Blue2 <- NULL
data2$Blue3 <- NULL
data2$Blue4 <- NULL
data2$BLUE1_T2 <- NULL
data2$BLUE2_T2 <- NULL
data2$BLUE3_T2 <- NULL
data2$BLUE4_T2 <- NULL
data2$BLUE <- NULL

#Delete positive brand label attitudes.
data2$PLBA1 <- NULL
data2$PLBA2 <- NULL
data2$PLBA3 <- NULL
data2$PLBA4 <- NULL
data2$PLBA5 <- NULL
data2$PLBA1_T2 <- NULL
data2$PLBA2_T2 <- NULL
data2$PLBA3_T2 <- NULL
data2$PLBA4_T2 <- NULL
data2$PLBA5_T2 <- NULL
data2$PLBA <- NULL

#Delete survey enjoyment and value
data2$S_ENJ1 <- NULL
data2$S_ENJ2 <- NULL
data2$S_ENJ3 <- NULL
data2$S_ENJ1_T2 <- NULL
data2$S_ENJ2_T2 <- NULL
data2$S_ENJ3_T2 <- NULL
data2$S_VAL1 <- NULL
data2$S_VAL2 <- NULL
data2$S_VAL3 <- NULL
data2$S_VAL1_T2 <- NULL
data2$S_VAL2_T2 <- NULL
data2$S_VAL3_T2 <- NULL

#Delete unnecessary reverse-scored items.
data2$R_IRB6 <- NULL
data2$R_IRB7 <- NULL
data2$R_S_ENJ1 <- NULL
data2$R_S_ENJ1_T2 <- NULL
data2$R_S_VAL2 <- NULL
data2$R_S_VAL2_T2 <- NULL
data2$R_OCBO2 <- NULL
data2$R_OCBO3 <- NULL
data2$R_OCBO4 <- NULL

#Delete scale scores
data2$PP <- NULL
data2$VOICE <- NULL
data2$TC <- NULL
data2$OCBI <- NULL
data2$IRB <- NULL
data2$OCBO <- NULL

#Delete old filter variable
data2$filter <- NULL
data2$filter_. <- NULL
data2$COND2 <- NULL

#Delete old interaction terms
data2$xPP_COND <- NULL
data2$xPP_COND2 <- NULL
data2$xTC_COND <- NULL
data2$xTC_COND2 <- NULL
data2$xV_COND <- NULL
data2$xV_COND2 <- NULL

#Delete formerly considered parcels
data2$PPp1 <- NULL
data2$PPp2 <- NULL
data2$PPp3 <- NULL
data2$OCBIp1 <- NULL
data2$OCBIp2 <- NULL
data2$OCBIp3 <- NULL
data2$OCBOp1 <- NULL
data2$OCBOp2 <- NULL
data2$OCBOp3 <- NULL
data2$IRBp1 <- NULL
data2$IRBp2 <- NULL
data2$IRBp3 <- NULL

#Note: Address the mislabeled OCB variables. 
data2 <- rename(data2, c(OCBI7 = "OCBO7", OCBO7 = "OCBI7"))

#Re-order variables (so affectivity stuff is next to one another).
data2 <- data2[c(1:34,42,36:41,35,43:50,52,54,58,59,61,63,65,66,68,51,53,55,56,57,60,62,64,67,69,70,71,72,73,74)]
```
```{r Missing Data Analysis, include=FALSE}
#Analyses revealed that some cases of data had been imputed. Missing data appears to have been dealt with by simply imputing an average of sorts. TThis could be problematic (see Enders, C. K. (2003). Using the expectation maximization algorithm to estimate coefficient alpha for scales with item-level missing data. Psychological Methods, 8(3), 322-337.; Enders, C. K. (2010). Applied missing data analysis: New York, NY: The Guilford Press. Additionally, and more importantly, the required estimation methods are not maximum likelihood but are ordinal, further requiring the each response category be represented (i.e., averages do not fall into a particular response category).  Reversing this decision:
data2$PA2[data2$PA2==3.23] <- NA
data2$PA5[data2$PA5==3.58] <- NA
data2$PA6[data2$PA6==3.59] <- NA
data2$PA8[data2$PA8==3.77] <- NA
data2$PA9[data2$PA9==3.75] <- NA
data2$PA10[data2$PA10==3.57] <- NA
data2$PA3[data2$PA3==3.44] <- NA
data2$PA4[data2$PA4==3.56] <- NA
data2$PA7[data2$PA7==3.45] <- NA
data2$NA9[data2$NA9==1.87] <- NA
data2$NA2[data2$NA2==3.23] <- NA
data2$NA3[data2$NA3==1.72] <- NA
data2$NA6[data2$NA6==2.17] <- NA
data2$NA7[data2$NA7==1.57] <- NA
data2$NA8[data2$NA8==2.11] <- NA
data2$VOICE2[data2$VOICE2==3.7] <- NA
data2$VOICE4[data2$VOICE4==3.84] <- NA
data2$VOICE5[data2$VOICE5==3.79] <- NA
data2$VOICE6[data2$VOICE6==3.81] <- NA
data2$PP1[data2$PP1==3.49] <- NA
data2$PP3[data2$PP3==3.92] <- NA
data2$PP8[data2$PP8==3.57] <- NA
data2$PP9[data2$PP9==3.53] <- NA
data2$PP10[data2$PP10==3.45] <- NA
data2$TC2[data2$TC2==3.81] <- NA
data2$TC3[data2$TC3==3.81] <- NA
data2$TC4[data2$TC4==3.71] <- NA
data2$TC5[data2$TC5==3.35] <- NA
data2$TC7[data2$TC7==3.77] <- NA
data2$TC8[data2$TC8==3.86] <- NA
data2$TC9[data2$TC9==3.64] <- NA
data2$TC10[data2$TC10==3.54] <- NA
data2$IRB1[data2$IRB1==4.47] <- NA
data2$IRB2[data2$IRB2==4.47] <- NA
data2$IRB4[data2$IRB4==4.49] <- NA
data2$IRB5[data2$IRB5==3.59] <- NA
data2$IRB5[data2$IRB5==3.96] <- NA
data2$IRB6[data2$IRB6==1.88] <- NA
data2$IRB7[data2$IRB7==1.65] <- NA
data2$OCBI3[data2$OCBI3==3.7] <- NA
data2$OCBI5[data2$OCBI5==3.99] <- NA
data2$OCBI7[data2$OCBI7==4.06] <- NA
data2$OCBO1[data2$OCBO1==4.24] <- NA
data2$OCBO3[data2$OCBO3==1.92] <- NA
data2$OCBO4[data2$OCBO4==2.12] <- NA
data2$OCBO5[data2$OCBO5==2.12] <- NA
data2$OCBO6[data2$OCBO6==3.89] <- NA

#Check automated classifications to see if they are correct.
#show(mdf)

#Change classes. 
#mdf <- change(mdf, y = c("MOOD_T1", "MOOD_T2"), what = "type", to = c("ordered-categorical"))
#mdf <- change(mdf, y = c("AGE", "SEX", "RACE"), what = "type", to = c("continuous"))
#show(mdf)

#Summarize data
#summary(mdf)

#Image data
#image(mdf)
#Note: this "hist(mdf)" command might not work unless you print it to the "plots" window. 

#Show missing data patterns.
#show(mdf)
#51 missing data patterns detected, but a visual inspection does not reveal anything that seems strongly at play. 
#Mice uses logistic regression to impute. Research has shown that logistic regression introduces bias (Wu, Jai, & Enders, 2015). However, robust full information maximum likelihood has shown some promise (Jia, 2016), which can be implemented via lavaan. This has been incorporated into the substantive analysis section. Wu, W., Jia, F., & Enders, C. (2015). A Comparison of Imputation Strategies for Ordinal Missing Data on Likert Scale Variables. Multivariate Behavioral Research, 50(5), 484–503. https://doi.org/10.1080/00273171.2015.1022644; Jia, F. (2016). Methods for Handling Missing Non-Normal Data in Structural Equation Modeling. Retrieved from https://kuscholarworks.ku.edu/handle/1808/22401.
```
Participants were recruited using a SurveyMonkey panel and randomly assigned to a condition where they received all measures at the same time (i.e., control) or a condition whereby a temporal separation of one week was used to divide the adminstration of measures. More specifically for temporal remedy condition, proactive personality, voice, taking charge, and momentary mood were administered first. After one week, the role behavior measures were administered along with several method and control measures (e.g., positive and negative affectivity, positive brand lable attitudes, preferences for the color blue, survey enjoyment, and survey value), a demographics questionnaire, and another measure of momentary mood. Momentary mood was assessed with one item: "My mood today can best be described as..." (1 = unpleasant, 7 = pleasant). With the exception of momentary mood, the same measurement model was tested in this study. Respondents received $5 to complete this survey. Participants’ data was used if there were no more than two items on the independent and dependent variables that a respondent did not answer. For the cross-sectional data, 150 of 190 respondents (78.9%) provided usable data. The temporally separated condition resulted in a serious amount of attrition for wave two of the survey. Although 307 respondents completed the first wave of the survey, only 183 completed the second wave and provided usable data (59.6%). 

Mean replacement was used for any missing item, limited to only two per respondent, affecting less than 1% of all items for all respondents across both conditions. In the full sample, respondents’ age ranged from 18 to 83 (M = 45.61). The sample was 57.0% female and 82.9% Caucasian. A large majority (84.1%) of respondents worked full-time, and the other 15.9% worked part-time. 

##Analytical Approach
We used established measured method effect techniques to assess the presence and biasing role of momentary affect (Williams et al., 2010). Similar to study 1, our expectations were that method effects attributable to momentary affect would be stronger in our control condition because, as all measures were administered simultaneously, mood would act as a constant contaminating factor. In other words, we expect to observe significant measurement contamination attributable to momentary mood in the form of significant and positive path coefficients linking momentary affect to the indicators of our measurement model, which are expectations consistent with prior research (Williams & Anderson, 1994). Furthermore, we expected that momentary positive affect would result in biased estimates of correlation. 

Conversely, we expected momentary affect to have virtually no effect in our remedied condition involving a 1-week temporal separation. This pattern would suggest that CMV attributable to affect would be present in a same-source same-time-point survey (hypothesis 1a). Evidence of bias attributable to momentary affect would emerge if the interconstruct correlations varied as a function of momentary affect effects. Thus, supporting hypothesis 1b would provide evidence supporting the viability of a one-week temporal separation for reducing affect effects. Again, we employed Williams et al's (2010) measured method effects technique, which involve a series of nested model comparisons under different assumptions of method variance presence (i.e., CMV is not present, non equal/noncongeneric CMV is present, unequal/congeneric CMV is present) and bias (i.e., no bias or significant bias) while also examiing the non-invariance of our measurement model.

Following study 1, we carried out our analyses using 'lavaan' in R and used robust maximum likelihood estimation (maximum likelihood estimation with robust standard errors) (see Li, 2016: "Confirmatory factor analysis with ordinal data: Comparing robust maximum likelihood and diagnoally weighted least squares") and the same modeling assumptions were applied (i.e., negative method factor, correlated residuals, factor loadings, and latent construct correlations). Satorra-Bentler X2 difference testing was, again, used to compare our models.

#Results - Study 1
```{r Temporal Separation Substantive Analyses, include=FALSE}
initial.cfa  <-  '
#Substantive Factors
  PR =~ PP1 + PP2 + PP3 + PP4 + PP5 + PP6 + PP7 + PP8 + PP9 + PP10
  VOICE =~ VOICE1 + VOICE2 + VOICE3 + VOICE4 + VOICE5 + VOICE6
  TC =~ TC1 + TC2 + TC3 + TC4 + TC5 + TC6 + TC7 + TC8 + TC9 + TC10
  IRB =~ IRB1 + .999*IRB2 + IRB3 + IRB4 + IRB5 + IRB6 + IRB7
  OCBI =~ OCBI1 + OCBI2 + OCBI3 + OCBI4 + OCBI5 + OCBI6 + OCBI7
  OCBO =~ OCBO1 + OCBO2 +OCBO3 + OCBO4 + OCBO5 + OCBO6 + OCBO7

#Method Factors
  PA =~ PA1 + PA2 + PA3 + PA4 + PA5 + PA6 + PA7 + PA8 + PA9 + PA10 
  Na =~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9 + NA10
  AP =~ PA1 + PA2 + PA3 + PA4 + PA5 + PA6 + PA7 + PA8 + PA9 + PA10 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9 + NA10
  Mood =~ MOOD_T1
  NegIW =~ IRB6 + IRB7 + OCBO2 +OCBO3 + OCBO4

#Method causal structure
  Mood ~ PA + NA

#Fix all negative item wording covariances to zero.
  NegIW ~~ 0*PR
  NegIW ~~ 0*VOICE
  NegIW ~~ 0*TC
  NegIW ~~ 0*IRB
  NegIW ~~ 0*OCBI
  NegIW ~~ 0*OCBO
  NegIW ~~ 0*PA
  NegIW ~~ 0*Na
  NegIW ~~ 0*Mood
  NegIW ~~ 0*AP


#set covariances for bifactor to zero
  PA ~~ 0*Na
  PA ~~ 0*AP
  Na ~~ 0*AP

#variances to one
#  PA ~~ 1*PA
#  Na ~~ 1*Na
#  AP ~~ 1*AP
#  Mood ~~ 1*Mood
#  PR ~~ 1*PR
#  VOICE ~~ 1*VOICE
#  TC ~~ 1*TC
#  IRB ~~ 1*IRB
#  OCBI ~~ 1*OCBI
#  OCBO ~~ 1*OCBO
#  NegIW ~~ 1*NegIW

#Fix Heywood case (which appears attributable to the skew in the IRB responses)
# name the parameter label for the variance
#  IRB2 ~~ resvarIRB2*IRB2
# force this parameter value to remain non-negative
#  resvarIRB2 == 0
'

#Note: certain response categories are too few for the estimator to work.  Specifically, the "strongly disagree" and "disagree" response options were simply combined to reflect a "disagree" option. So I combined them:
data2$IRB1[data2$IRB1==1] <- 2
data2$IRB2[data2$IRB2==1] <- 2
data2$IRB4[data2$IRB4==1] <- 2
data2$OCBI7[data2$OCBI7==1] <- 2
data2$OCBO1[data2$OCBO1==1] <- 2
data2$TC2[data2$TC2==1] <- 2
data2$TC1[data2$TC1==1] <- 2
data2$TC7[data2$TC7==1] <- 2
data2$IRB3[data2$IRB3==1] <- 2
data2$PP2[data2$PP2==1] <- 2
data2$PP4[data2$PP4==1] <- 2
data2$PP5[data2$PP5==1] <- 2

#To see the results of an anlysis of the discriminant validity of the measurement model, see the following object. 
dv <- htmt(initial.cfa, data2, missing="ML", ordered = c("PA1", "PA2", "PA3", "PA4", "PA5", "PA6", "PA7", "PA8", "PA9", "PA10", "NA1", "NA2", "NA3", "NA4", "NA5", "NA6", "NA7", "NA8", "NA9", "NA10", "Mood_T1", "PP1", "PP2", "PP3", "PP4", "PP5", "PP6", "PP7", "PP8", "PP9", "PP10", "VOICE1", "VOICE2", "VOICE3", "VOICE4", "VOICE5", "VOICE6", "TC1", "TC2", "TC3", "TC4", "TC5", "TC6", "TC7", "TC8", "TC9", "TC10", "IRB1", "IRB2", "IRB3", "IRB4", "IRB5", "IRB6", "IRB7", "OCBI1", "OCBI2", "OCBI3", "OCBI4", "OCBI5", "OCBI6", "OCBI7", "OCBO1", "OCBO2", "OCBO3", "OCBO4", "OCBO5", "OCBO6", "OCBO7"))
#Derive bootstrap confidence intervals.???
boot <- bsBootMiss(dv)

##Results indicate acceptable discriminant validity according to Hensler et al. (2015) criterion of .85 HTMT criterion. Need to re-examine once I can estimate confidence intervals because VOICE is really close to taking charge. This may not be so much of a problem here because method effects would be expected to inflate certain correlations and attenunate others. 
                  
#Note: Group 1 is the temporally separted condition. Group 2 is the same-time-point condition.
#Configural invariance
config <- cfa(initial.cfa, ordered = c("PA1", "PA2", "PA3", "PA4", "PA5", "PA6", "PA7", "PA8", "PA9", "PA10", "NA1", "NA2", "NA3", "NA4", "NA5", "NA6", "NA7", "NA8", "NA9", "NA10", "Mood_T1", "PP1", "PP2", "PP3", "PP4", "PP5", "PP6", "PP7", "PP8", "PP9", "PP10", "VOICE1", "VOICE2", "VOICE3", "VOICE4", "VOICE5", "VOICE6", "TC1", "TC2", "TC3", "TC4", "TC5", "TC6", "TC7", "TC8", "TC9", "TC10", "IRB1", "IRB2", "IRB3", "IRB4", "IRB5", "IRB6", "IRB7", "OCBI1", "OCBI2", "OCBI3", "OCBI4", "OCBI5", "OCBI6", "OCBI7", "OCBO1", "OCBO2", "OCBO3", "OCBO4", "OCBO5", "OCBO6", "OCBO7"), data = data2, group="COND", missing="ML", estimator = "DWLS", information = "expected")
##Note: initial assessments revealed a negative residual variance. This seems to be attributable to the amount of skew in the IRB scale (which is where the Heywood case is popping up).

mod <- modindices(config)
fitmeasures(config, c("cfi","tli","rmsea","srmr","chisq","df","pvalue"))
pe <- parameterEstimates(config)
summary(config, standardized = TRUE, fit.measures = TRUE)
semPaths(config, "std", curvePivot = TRUE, thresholds = FALSE)

#Examine measurement invariance
mi <- measurementInvarianceCat(initial.cfa, ordered = c("PA1", "PA2", "PA3", "PA4", "PA5", "PA6", "PA7", "PA8", "PA9", "PA10", "NA1", "NA2", "NA3", "NA4", "NA5", "NA6", "NA7", "NA8", "NA9", "NA10", "Mood_T1", "PP1", "PP2", "PP3", "PP4", "PP5", "PP6", "PP7", "PP8", "PP9", "PP10", "VOICE1", "VOICE2", "VOICE3", "VOICE4", "VOICE5", "VOICE6", "TC1", "TC2", "TC3", "TC4", "TC5", "TC6", "TC7", "TC8", "TC9", "TC10", "IRB1", "IRB2", "IRB3", "IRB4", "IRB5", "IRB6", "IRB7", "OCBI1", "OCBI2", "OCBI3", "OCBI4", "OCBI5", "OCBI6", "OCBI7", "OCBO1", "OCBO2", "OCBO3", "OCBO4", "OCBO5", "OCBO6", "OCBO7"), data = data2, group="COND", missing="ML", estimator = "DWLS")

#For testing whether method effects are present and biasing parameter estimates, we followed guidance from Williams et al. (2010), albeit adapted for the case of multiple method factors and a situation involving a test of measurement and structural invariance. Following Williams et al.'s advice, we first estimated a model to identify the the factor loadings and residual variances for the method factors. If method factors do not interact with one another (e.g., the contaminating role of positive and negative affectivity as well as mood are unaffected by the use of a one-week separation), then method effects should pass tests for measurement invariance. Therefore, we tested whether our measured method factors were invariant across our two conditions and then, having established measurement invariance, we constrained the factor loadings and residual variances of our measured method factors to these values for all subsequent tests of both equal and unequal method effects (baseline model). This allowed us to determine if method effects attributable to affective factors varied across condition such that they were either equivalent (congeneric) or varied (non-congeneric). 

##Obtain unstandardized factor loadings and residual residual variances to levels found in the initial CFA, which should be fixed in the baseline models. 
initial.c <- cfa(initial.sem.c, estimator = "MLM", data=control, std.lv = TRUE)
summary(initial.c, standardized = TRUE, fit.measures = TRUE)

#Control Baseline
#Fix factor loadings and residual variances to initial levels.
baseline.sem.c  <-  '
#Substantive Factors
  PR =~ NA*PP1 + PP2 + PP3 + PP4 + PP5 + PP6 + PP7 + PP8 + PP9 + PP10
  VOICE =~ NA*VOICE1 + VOICE2 + VOICE3 + VOICE4 + VOICE5 + VOICE6
  TC =~ NA*TC1 + TC2 + TC3 + TC4 + TC5 + TC6 + TC7 + TC8 + TC9 + TC10
  IRB =~ NA*IRB1 + IRB2 + IRB3 + IRB4 + IRB5 + IRB6 + IRB7
  OCBI =~ NA*OCBI1 + OCBI2 + OCBI3 + OCBI4 + OCBI5 + OCBI6 + OCBI7
  OCBO =~ NA*OCBO1 + OCBO2 +OCBO3 + OCBO4 + OCBO5 + OCBO6 + OCBO7

#Define Method Factors
  PA =~ 4.660*PA1 + 1.898*PA2 + 0.794*PA3 + x*PA4 + x*PA5 + x*PA6 + x*PA7 + x*PA8 + x*PA9 + x*PA10
  Na =~ 1.994*Na1 + 1.509*Na2 + 1.319*Na3 + 0.685*Na4 + 1.388*Na5 + x*NA6 + x*NA7 + x*NA8 + x*NA10
  M =~ 1.332*MOOD_T1
  Neg =~ 0.875*IRBp2 + 1.905*OCBOp3

#set covariances for bifactor to zero
  PA ~~ 0*Na
  PA ~~ 0*AP
  Na ~~ 0*AP

#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 

#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO  

#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO 

#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M

#Method Regressions
  M ~ PA + Na

#Fixed residuals
  PA1 ~~ x*PA1
  PA2 ~~ x*PA2
  PA3 ~~ x*PA3
  PA4 ~~ x*PA4
  PA5 ~~ x*PA5
  PA6 ~~ x*PA6
  PA7 ~~ x*PA7
  PA8 ~~ x*PA8
  PA9 ~~ x*PA9
  PA10 ~~ x*PA10
  NA1 ~~ x*NA1
  NA2 ~~ x*NA2
  NA3 ~~ x*NA3
  NA4 ~~ x*NA4
  NA5 ~~ x*NA5
  NA6 ~~ x*NA6
  NA7 ~~ x*NA7
  NA8 ~~ x*NA8
  NA9 ~~ x*NA9
  NA10 ~~ x*NA10
  MOOD_T1 ~~ 0*MOOD_T1
'
baseline.c <- cfa(baseline.sem.c, estimator = "MLM", data=control, std.lv = TRUE)
summary(baseline.c, standardized = TRUE, fit.measures = TRUE)

#Method C
methodc.sem.c  <-  '
#Substantive Factors
  PR =~ NA*PP1 + PP2 + PP3 + PP4 + PP5 + PP6 + PP7 + PP8 + PP9 + PP10
  VOICE =~ NA*VOICE1 + VOICE2 + VOICE3 + VOICE4 + VOICE5 + VOICE6
  TC =~ NA*TC1 + TC2 + TC3 + TC4 + TC5 + TC6 + TC7 + TC8 + TC9 + TC10
  IRB =~ NA*IRB1 + IRB2 + IRB3 + IRB4 + IRB5 + IRB6 + IRB7
  OCBI =~ NA*OCBI1 + OCBI2 + OCBI3 + OCBI4 + OCBI5 + OCBI6 + OCBI7
  OCBO =~ NA*OCBO1 + OCBO2 +OCBO3 + OCBO4 + OCBO5 + OCBO6 + OCBO7

#Define Method Factors
  PA =~ 4.660*PA1 + 1.898*PA2 + 0.794*PA3 + x*PA4 + x*PA5 + x*PA6 + x*PA7 + x*PA8 + x*PA9 + x*PA10 + x*PP1 + x*PP2 + x*PP3 + x*PP4 + x*PP5 + x*PP6 + x*PP7 + x*PP8 + x*PP9 + x*PP10 + x*VOICE1 + x*VOICE2 +  x*VOICE3 +  x*VOICE4 + x*VOICE5 + x*VOICE6 + x*TC1 + x*TC2 + x*TC3 + x*TC4 + x*TC5 + x*TC6 + x*TC7 + x*TC8 + x*TC9 + x*TC10 + x*IRB1 + x*IRB2 + x*IRB3 + x*IRB4 + x*IRB5 + x*IRB6 + x*IRB7 + x*OCBI1 + x*OCBI2 + x*OCBI3 + x*OCBI4 + x*OCBI5 + x*OCBI6 + x*OCBI7 + x*OCBO1 + x*OCBO2 + x*OCBO3 + x*OCBO4 + x*OCBO5 + x*OCBO6 + x*OCBO7
  Na =~ 4.660*PA1 + 1.898*PA2 + 0.794*PA3 + x*PA4 + x*PA5 + x*PA6 + x*PA7 + x*PA8 + x*PA9 + x*PA10 + x*PP1 + x*PP2 + x*PP3 + x*PP4 + x*PP5 + x*PP6 + x*PP7 + x*PP8 + x*PP9 + x*PP10 + x*VOICE1 + x*VOICE2 +  x*VOICE3 +  x*VOICE4 + x*VOICE5 + x*VOICE6 + x*TC1 + x*TC2 + x*TC3 + x*TC4 + x*TC5 + x*TC6 + x*TC7 + x*TC8 + x*TC9 + x*TC10 + x*IRB1 + x*IRB2 + x*IRB3 + x*IRB4 + x*IRB5 + x*IRB6 + x*IRB7 + x*OCBI1 + x*OCBI2 + x*OCBI3 + x*OCBI4 + x*OCBI5 + x*OCBI6 + x*OCBI7 + x*OCBO1 + x*OCBO2 + x*OCBO3 + x*OCBO4 + x*OCBO5 + x*OCBO6 + x*OCBO7
  M =~ 4.660*PA1 + 1.898*PA2 + 0.794*PA3 + x*PA4 + x*PA5 + x*PA6 + x*PA7 + x*PA8 + x*PA9 + x*PA10 + x*PP1 + x*PP2 + x*PP3 + x*PP4 + x*PP5 + x*PP6 + x*PP7 + x*PP8 + x*PP9 + x*PP10 + x*VOICE1 + x*VOICE2 +  x*VOICE3 +  x*VOICE4 + x*VOICE5 + x*VOICE6 + x*TC1 + x*TC2 + x*TC3 + x*TC4 + x*TC5 + x*TC6 + x*TC7 + x*TC8 + x*TC9 + x*TC10 + x*IRB1 + x*IRB2 + x*IRB3 + x*IRB4 + x*IRB5 + x*IRB6 + x*IRB7 + x*OCBI1 + x*OCBI2 + x*OCBI3 + x*OCBI4 + x*OCBI5 + x*OCBI6 + x*OCBI7 + x*OCBO1 + x*OCBO2 + x*OCBO3 + x*OCBO4 + x*OCBO5 + x*OCBO6 + x*OCBO7
  Neg =~ 0.875*IRBp2 + 1.905*OCBOp3

#set covariances for bifactor to zero
  PA ~~ 0*Na
  PA ~~ 0*AP
  Na ~~ 0*AP 

#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 

#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO  

#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO  

#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M

#Method Regressions
  M ~ PA + Na

#Fixed residuals
  PA1 ~~ x*PA1
  PA2 ~~ x*PA2
  PA3 ~~ x*PA3
  PA4 ~~ x*PA4
  PA5 ~~ x*PA5
  PA6 ~~ x*PA6
  PA7 ~~ x*PA7
  PA8 ~~ x*PA8
  PA9 ~~ x*PA9
  PA10 ~~ x*PA10
  NA1 ~~ x*NA1
  NA2 ~~ x*NA2
  NA3 ~~ x*NA3
  NA4 ~~ x*NA4
  NA5 ~~ x*NA5
  NA6 ~~ x*NA6
  NA7 ~~ x*NA7
  NA8 ~~ x*NA8
  NA9 ~~ x*NA9
  NA10 ~~ x*NA10
  MOOD_T1 ~~ 0*MOOD_T1
'
methodc.sem.c <- cfa(methodc.sem.c, estimator = "MLM", data=control, std.lv = TRUE)
summary(methodc.sem.c, standardized = TRUE, fit.measures = TRUE)
anova(baseline.c,methodc.sem.c)
#Baseline is statistically significant from model c. 

#METHOD U
methodu.sem.c  <-  '
#Substantive Factors
  PR =~ NA*PP1 + PP2 + PP3 + PP4 + PP5 + PP6 + PP7 + PP8 + PP9 + PP10
  VOICE =~ NA*VOICE1 + VOICE2 + VOICE3 + VOICE4 + VOICE5 + VOICE6
  TC =~ NA*TC1 + TC2 + TC3 + TC4 + TC5 + TC6 + TC7 + TC8 + TC9 + TC10
  IRB =~ NA*IRB1 + IRB2 + IRB3 + IRB4 + IRB5 + IRB6 + IRB7
  OCBI =~ NA*OCBI1 + OCBI2 + OCBI3 + OCBI4 + OCBI5 + OCBI6 + OCBI7
  OCBO =~ NA*OCBO1 + OCBO2 +OCBO3 + OCBO4 + OCBO5 + OCBO6 + OCBO7

#Define Method Factors
  PA =~ 4.660*PA1 + 1.898*PA2 + 0.794*PA3 + x*PA4 + x*PA5 + x*PA6 + x*PA7 + x*PA8 + x*PA9 + x*PA10 + x*PP1 + x*PP2 + x*PP3 + x*PP4 + x*PP5 + x*PP6 + x*PP7 + x*PP8 + x*PP9 + x*PP10 + x*VOICE1 + x*VOICE2 +  x*VOICE3 +  x*VOICE4 + x*VOICE5 + x*VOICE6 + x*TC1 + x*TC2 + x*TC3 + x*TC4 + x*TC5 + x*TC6 + x*TC7 + x*TC8 + x*TC9 + x*TC10 + x*IRB1 + x*IRB2 + x*IRB3 + x*IRB4 + x*IRB5 + x*IRB6 + x*IRB7 + x*OCBI1 + x*OCBI2 + x*OCBI3 + x*OCBI4 + x*OCBI5 + x*OCBI6 + x*OCBI7 + x*OCBO1 + x*OCBO2 + x*OCBO3 + x*OCBO4 + x*OCBO5 + x*OCBO6 + x*OCBO7
  Na =~ 4.660*PA1 + 1.898*PA2 + 0.794*PA3 + x*PA4 + x*PA5 + x*PA6 + x*PA7 + x*PA8 + x*PA9 + x*PA10 + x*PP1 + x*PP2 + x*PP3 + x*PP4 + x*PP5 + x*PP6 + x*PP7 + x*PP8 + x*PP9 + x*PP10 + x*VOICE1 + x*VOICE2 +  x*VOICE3 +  x*VOICE4 + x*VOICE5 + x*VOICE6 + x*TC1 + x*TC2 + x*TC3 + x*TC4 + x*TC5 + x*TC6 + x*TC7 + x*TC8 + x*TC9 + x*TC10 + x*IRB1 + x*IRB2 + x*IRB3 + x*IRB4 + x*IRB5 + x*IRB6 + x*IRB7 + x*OCBI1 + x*OCBI2 + x*OCBI3 + x*OCBI4 + x*OCBI5 + x*OCBI6 + x*OCBI7 + x*OCBO1 + x*OCBO2 + x*OCBO3 + x*OCBO4 + x*OCBO5 + x*OCBO6 + x*OCBO7
  M =~ 4.660*PA1 + 1.898*PA2 + 0.794*PA3 + x*PA4 + x*PA5 + x*PA6 + x*PA7 + x*PA8 + x*PA9 + x*PA10 + x*PP1 + x*PP2 + x*PP3 + x*PP4 + x*PP5 + x*PP6 + x*PP7 + x*PP8 + x*PP9 + x*PP10 + x*VOICE1 + x*VOICE2 +  x*VOICE3 +  x*VOICE4 + x*VOICE5 + x*VOICE6 + x*TC1 + x*TC2 + x*TC3 + x*TC4 + x*TC5 + x*TC6 + x*TC7 + x*TC8 + x*TC9 + x*TC10 + x*IRB1 + x*IRB2 + x*IRB3 + x*IRB4 + x*IRB5 + x*IRB6 + x*IRB7 + x*OCBI1 + x*OCBI2 + x*OCBI3 + x*OCBI4 + x*OCBI5 + x*OCBI6 + x*OCBI7 + x*OCBO1 + x*OCBO2 + x*OCBO3 + x*OCBO4 + x*OCBO5 + x*OCBO6 + x*OCBO7
  Neg =~ 0.875*IRBp2 + 1.905*OCBOp3

#set covariances for bifactor to zero
  PA ~~ 0*Na
  PA ~~ 0*AP
  Na ~~ 0*AP

#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 

#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO  

#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO  

#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M

#Method Regressions
  M ~ PA + Na

#Fixed residuals
  PA1 ~~ x*PA1
  PA2 ~~ x*PA2
  PA3 ~~ x*PA3
  PA4 ~~ x*PA4
  PA5 ~~ x*PA5
  PA6 ~~ x*PA6
  PA7 ~~ x*PA7
  PA8 ~~ x*PA8
  PA9 ~~ x*PA9
  PA10 ~~ x*PA10
  NA1 ~~ x*NA1
  NA2 ~~ x*NA2
  NA3 ~~ x*NA3
  NA4 ~~ x*NA4
  NA5 ~~ x*NA5
  NA6 ~~ x*NA6
  NA7 ~~ x*NA7
  NA8 ~~ x*NA8
  NA9 ~~ x*NA9
  NA10 ~~ x*NA10
  MOOD_T1 ~~ 0*MOOD_T1
'

methodu.sem.c <- cfa(methodu.sem.c, estimator = "MLM", data=control, std.lv = TRUE)
summary(methodu.sem.c, standardized = TRUE, fit.measures = TRUE)
anova(methodu.sem.c,methodc.sem.c)
anova(baseline.c, methodu.sem.c)
#Method U is superior to baseline model. Congeneric CMV is present. 

#METHOD R
methodr.sem.c  <-  '
#Substantive Factors
  PR =~ NA*PP1 + PP2 + PP3 + PP4 + PP5 + PP6 + PP7 + PP8 + PP9 + PP10
  VOICE =~ NA*VOICE1 + VOICE2 + VOICE3 + VOICE4 + VOICE5 + VOICE6
  TC =~ NA*TC1 + TC2 + TC3 + TC4 + TC5 + TC6 + TC7 + TC8 + TC9 + TC10
  IRB =~ NA*IRB1 + IRB2 + IRB3 + IRB4 + IRB5 + IRB6 + IRB7
  OCBI =~ NA*OCBI1 + OCBI2 + OCBI3 + OCBI4 + OCBI5 + OCBI6 + OCBI7
  OCBO =~ NA*OCBO1 + OCBO2 +OCBO3 + OCBO4 + OCBO5 + OCBO6 + OCBO7

#Define Method Factors
  PA =~ 4.660*PA1 + 1.898*PA2 + 0.794*PA3 + x*PA4 + x*PA5 + x*PA6 + x*PA7 + x*PA8 + x*PA9 + x*PA10 + x*PP1 + x*PP2 + x*PP3 + x*PP4 + x*PP5 + x*PP6 + x*PP7 + x*PP8 + x*PP9 + x*PP10 + x*VOICE1 + x*VOICE2 +  x*VOICE3 +  x*VOICE4 + x*VOICE5 + x*VOICE6 + x*TC1 + x*TC2 + x*TC3 + x*TC4 + x*TC5 + x*TC6 + x*TC7 + x*TC8 + x*TC9 + x*TC10 + x*IRB1 + x*IRB2 + x*IRB3 + x*IRB4 + x*IRB5 + x*IRB6 + x*IRB7 + x*OCBI1 + x*OCBI2 + x*OCBI3 + x*OCBI4 + x*OCBI5 + x*OCBI6 + x*OCBI7 + x*OCBO1 + x*OCBO2 + x*OCBO3 + x*OCBO4 + x*OCBO5 + x*OCBO6 + x*OCBO7
  Na =~ 4.660*PA1 + 1.898*PA2 + 0.794*PA3 + x*PA4 + x*PA5 + x*PA6 + x*PA7 + x*PA8 + x*PA9 + x*PA10 + x*PP1 + x*PP2 + x*PP3 + x*PP4 + x*PP5 + x*PP6 + x*PP7 + x*PP8 + x*PP9 + x*PP10 + x*VOICE1 + x*VOICE2 +  x*VOICE3 +  x*VOICE4 + x*VOICE5 + x*VOICE6 + x*TC1 + x*TC2 + x*TC3 + x*TC4 + x*TC5 + x*TC6 + x*TC7 + x*TC8 + x*TC9 + x*TC10 + x*IRB1 + x*IRB2 + x*IRB3 + x*IRB4 + x*IRB5 + x*IRB6 + x*IRB7 + x*OCBI1 + x*OCBI2 + x*OCBI3 + x*OCBI4 + x*OCBI5 + x*OCBI6 + x*OCBI7 + x*OCBO1 + x*OCBO2 + x*OCBO3 + x*OCBO4 + x*OCBO5 + x*OCBO6 + x*OCBO7
  M =~ 4.660*PA1 + 1.898*PA2 + 0.794*PA3 + x*PA4 + x*PA5 + x*PA6 + x*PA7 + x*PA8 + x*PA9 + x*PA10 + x*PP1 + x*PP2 + x*PP3 + x*PP4 + x*PP5 + x*PP6 + x*PP7 + x*PP8 + x*PP9 + x*PP10 + x*VOICE1 + x*VOICE2 +  x*VOICE3 +  x*VOICE4 + x*VOICE5 + x*VOICE6 + x*TC1 + x*TC2 + x*TC3 + x*TC4 + x*TC5 + x*TC6 + x*TC7 + x*TC8 + x*TC9 + x*TC10 + x*IRB1 + x*IRB2 + x*IRB3 + x*IRB4 + x*IRB5 + x*IRB6 + x*IRB7 + x*OCBI1 + x*OCBI2 + x*OCBI3 + x*OCBI4 + x*OCBI5 + x*OCBI6 + x*OCBI7 + x*OCBO1 + x*OCBO2 + x*OCBO3 + x*OCBO4 + x*OCBO5 + x*OCBO6 + x*OCBO7
  Neg =~ 0.875*IRBp2 + 1.905*OCBOp3

#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 

#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO  

#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO  

#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M

#Method Regressions
  M ~ PA + Na

#Fixed residuals
  PA1 ~~ x*PA1
  PA2 ~~ x*PA2
  PA3 ~~ x*PA3
  PA4 ~~ x*PA4
  PA5 ~~ x*PA5
  PA6 ~~ x*PA6
  PA7 ~~ x*PA7
  PA8 ~~ x*PA8
  PA9 ~~ x*PA9
  PA10 ~~ x*PA10
  NA1 ~~ x*NA1
  NA2 ~~ x*NA2
  NA3 ~~ x*NA3
  NA4 ~~ x*NA4
  NA5 ~~ x*NA5
  NA6 ~~ x*NA6
  NA7 ~~ x*NA7
  NA8 ~~ x*NA8
  NA9 ~~ x*NA9
  NA10 ~~ x*NA10
  MOOD_T1 ~~ 0*MOOD_T1
'

methodr.sem.c <- cfa(methodr.sem.c, estimator = "MLM", data=control, std.lv = TRUE)
summary(methodr.sem.c, standardized = TRUE, fit.measures = TRUE)
anova(methodr.sem.c,methodu.sem.c)
#NOTE: Method effects attributable to positive affectivity, negative affectivity, mood, and negative item wording were observed. However, a test for method effects attributable to consistency motifs was not observed.
```

#Discussion - Study 1
  Our results speak to a more nuanced view of method variance that has been offered by Spector, Rosen, Richardson, Williams, and Johnson (2016), who urge researchers to focus less on broad solutions to CMV  (e.g., broad set of remedies for proximal method effects and statistical controls that assume unidimensional method effects) and more more on the extraneuous effects that are introduced by the measurement strategy employed in an investigation, which they refer to as either common or unique method variance. To explain by way of example, studies seeking to estimate the effect of attitudinal factors on behaviors should consider sources of variance that are common to all measures in the study (e.g., mood and social desirability) as well as factors unique to specific measures in the study (e.g., response sets might affect self-reports of attitudes while impression management might affect reports of sensitive behaviors, like deviance). Either common or unique sources of variance could dominate a particular study, or they could also cancel each other out. Either way, they introduce variation into the measures utilized in an investigation, which can result in either inflated or attentuated effect size estimates. They also encourage research into the developmentof a knowledge base upon which to rely in deciding how method variance could be addressed in investigations, which will require accumulating knowledge about soruces of methods to consider and control in a study. To this point, while our investigation focused on certain common method variance sources (e.g., consistency motifs, affectivity, mood), we did not exhaustively study the role played by all possible sources (e.g., social desirability). We also neglected to address the role played by uncommon method variance sources (e.g., impression management, which seems relevant for self-reports of IRB, limited information, which seems relevant for self-reports of behavior; and response sets, which seems relevant for proactive personalitiy). Therefore, we encourage future reserach to investigate the role played by these sources of nuissance variance. 
  Additionally, some of our method effects were assumed to be measured without error (e.g., consistency motifs and daily mood). Clearly, this false. [Could we conduct a sensitivity analysis to address this concern? I might need help here.]
  Our item blocking strategy may have also played a role.
##Study 2 - The Efficacy of Proximal Remedies 
  In study 2, we examined the role of consistency motifs, implicit theories, and context effects. More specifically, we tested whether using a cover story and randomzing items and scales around a filler scale worked (i.e., reduced the presence of method variance). Participants were randomly assigned to a condition where observations were obtained with or without these remedies. For our non-remedied (i.e., control) condition, an online self-reported survey was designed such that all items and scales appeared in the same order, but were separated by different webpages. This is a conventional kind of survey design likely in use by many organizations. For our remedied condition, we used a cover story that was designed to both blind participants to the purpose of our study and dispel implicit theories regarding the substantive intention guiding a study such as ours: test for positive relationships linking proactive personality to workplace behaviors. Also, the presentation of items within scales and also the whole scales were randomized around a series of filler scale consisting of measured method effect factors. We provide more detail for these remedies below.
=======
Notes for us to discuss:
The cognitive psychology literature, specifically on heuristics and biases in decision making and judgments, informs method variance research. For instance, consider anchoring, wherein one's judgment is informed by an arbitrary prior thought in one's conciousness, within and their ability to explain item-order (or item priming) effects. In an experiment, college students were asked the questions (a) how happy are you and (b) how often are you dating. When asked in this order, the correlation between the two questions was low (r = .11). However, when asked in reverse, the correlation was 5 times larger (r = .62). Similar findings emerged with married couples about their sex life (see Strack, Martin, & Schwarz, 1988). The design of the questionnaire (in this case, the item order) explains the size of the correlation estimated between measures of life satisfaction and dating or frequency of sex, respectively, with anchoring providing the psychological explanation. 
(A few questions regarding the above: has this been replicated? the effect seems exaggerated, so I'm wondering if the effect size is reliable...What would a Fisher r-to-z test reveal? which effect is closer to the truth, and why?)
References:
Strack, F. L., Martin, L., & Scharz, N. (1988). Priming and communication: The social determinants of information use in judgments of life-satisfaction. European Journal of Social Psychology, pp. 429-442.
#Study 2 - The Efficacy of Proximal Remedies
#Methods
##Sample and Procedure
```{r Load Proximal Separation Data, include=FALSE}
Proximal.spss <- read.spss("Proactivity at Work Survey - Item Counterbalancing - Copy_October 18, 2016_09.25.sav", to.data.frame=TRUE, use.value.labels=FALSE)
data <- as.data.frame(Proximal.spss)

#Six hundred and twenty-one workers from Amazon's Mechanical Turk were paid $1.30 for completing a survey, of which 556 agreed to participate in our study after reading our informed consent form. 
nrow(data)
count(data, Q42) # "1" refers to agreeing to participate in the non-remedied condition. 
count(data, Q44) # "1" refers to agreeing to participate in the remedied condition.
#Look for values of "1": 274 + 282 = 556.

#Data Scrubbing Block.
#Select those who agreed to participate. Q42=1 OR Q44=1. 
Agree <- filter(data, Q42 %in% c("1") & Q47 %in% c("1") | Q44 %in% c("1") & Q79 %in% c("2"))

#Filter in attentive responders.
Inattent <- filter(Agree, Q26_11 == "1" | Q105_11 ==  "1")

#Setup demographics.
#Rename variables.
Inattent$COND <- Inattent$Q42 
#"1" means they have been sent to the "control/non-remedied"" condition, whereas "0" means they have been sent to the "treatment/remedied condition'.
Inattent$AGE <- Inattent$Q33
Inattent$RACE <- Inattent$Q34
Inattent$GENDER <- Inattent$Q35
Inattent$EDUCAT <- Inattent$Q36.0
Inattent$EMPLOYED <- Inattent$Q37
Inattent$JOBTENURE <- Inattent$Q38
Inattent$USREGION <- Inattent$Q40
Inattent$PROFESSION <- Inattent$Q39
Inattent$PROF_SPECIFY <- Inattent$Q39_TEXT

#Delete old dems.
Inattent$Q42 <- NULL
Inattent$Q33 <- NULL
Inattent$Q34 <- NULL
Inattent$Q35 <- NULL
Inattent$Q36.0 <- NULL
Inattent$Q37 <- NULL
Inattent$Q38 <- NULL
Inattent$Q40 <- NULL
Inattent$Q39 <- NULL
Inattent$Q39_TEXT <- NULL

#Then, combine data from separate surveys in order to form the final cleaned and well-structured dataset. Note: there were two surveys setup on Qualtrics, hence why data needs to be combined. You'll see redundant items moving forward.
##Convert NAs to 0 to allow merging. Later, you'll need to reasign NAs to conduct a missing data analysis. 
Inattent[is.na(Inattent)] <- 0

#Proactive personality
Inattent$PP1	<-	Inattent$Q26_1	+Inattent$Q105_1
Inattent$PP2	<-	Inattent$Q26_2	+Inattent$Q105_2
Inattent$PP3	<-	Inattent$Q26_3	+Inattent$Q105_3
Inattent$PP4	<-	Inattent$Q26_4	+Inattent$Q105_4
Inattent$PP5	<-	Inattent$Q26_5	+Inattent$Q105_5
Inattent$PP6	<-	Inattent$Q26_6	+Inattent$Q105_6
Inattent$PP7	<-	Inattent$Q26_7	+Inattent$Q105_7
Inattent$PP8	<-	Inattent$Q26_8	+Inattent$Q105_8
Inattent$PP9	<-	Inattent$Q26_9	+Inattent$Q105_9
Inattent$PP10	<-	Inattent$Q26_10	+Inattent$Q105_10

#Voice
Inattent$VC1	<-	Inattent$Q27_1	+	Inattent$Q106_1
Inattent$VC2	<-	Inattent$Q27_2	+	Inattent$Q106_2
Inattent$VC3	<-	Inattent$Q27_3	+	Inattent$Q106_3
Inattent$VC4	<-	Inattent$Q27_4	+	Inattent$Q106_4
Inattent$VC5	<-	Inattent$Q27_5	+	Inattent$Q106_5
Inattent$VC6	<-	Inattent$Q27_6	+	Inattent$Q106_6

#Taking Charge
Inattent$TC1	<-	Inattent$Q28_1	+	Inattent$Q106_8
Inattent$TC2	<-	Inattent$Q28_2	+	Inattent$Q106_9
Inattent$TC3	<-	Inattent$Q28_3	+	Inattent$Q106_10
Inattent$TC4	<-	Inattent$Q28_4	+	Inattent$Q106_11
Inattent$TC5	<-	Inattent$Q28_5	+	Inattent$Q106_12
Inattent$TC6	<-	Inattent$Q28_6	+	Inattent$Q106_13
Inattent$TC7	<-	Inattent$Q28_7	+	Inattent$Q106_14
Inattent$TC8	<-	Inattent$Q28_8	+	Inattent$Q106_15
Inattent$TC9	<-	Inattent$Q28_9	+	Inattent$Q106_16
Inattent$TC10	<-	Inattent$Q28_10	+	Inattent$Q106_17

#OCBI
Inattent$OCBI1	<-	Inattent$Q29_1	+	Inattent$Q106_19
Inattent$OCBI2	<-	Inattent$Q29_2	+	Inattent$Q106_20
Inattent$OCBI3  <-	Inattent$Q29_3	+	Inattent$Q106_21
Inattent$OCBI4	<-	Inattent$Q29_4	+	Inattent$Q106_22
Inattent$OCBI5	<-	Inattent$Q29_5	+	Inattent$Q106_23
Inattent$OCBI6	<-	Inattent$Q29_6	+	Inattent$Q106_24
Inattent$OCBI7	<-	Inattent$Q29_7	+	Inattent$Q106_25

#OCBO
Inattent$OCBO1	<-	Inattent$Q30_1	+	Inattent$Q106_27
Inattent$OCBO2	<-	Inattent$Q30_2	+	Inattent$Q106_28
Inattent$OCBO3	<-	Inattent$Q30_3	+	Inattent$Q106_29
Inattent$OCBO4	<-	Inattent$Q30_4	+	Inattent$Q106_30
Inattent$OCBO5	<-	Inattent$Q30_5	+	Inattent$Q106_31
Inattent$OCBO6	<-	Inattent$Q30_6	+	Inattent$Q106_32
Inattent$OCBO7	<-	Inattent$Q30_7	+	Inattent$Q106_33

#IRB
Inattent$IRB1	<-	Inattent$Q31_1	+	Inattent$Q106_35
Inattent$IRB2	<-	Inattent$Q31_2	+	Inattent$Q106_36
Inattent$IRB3	<-	Inattent$Q31_3	+	Inattent$Q106_37
Inattent$IRB4	<-	Inattent$Q31_4	+	Inattent$Q106_38
Inattent$IRB5	<-	Inattent$Q31_5	+	Inattent$Q106_39
Inattent$IRB6	<-	Inattent$Q31_6	+	Inattent$Q106_40
Inattent$IRB7	<-	Inattent$Q31_7	+	Inattent$Q106_41

require(car)
#Consistency Motif
#"I am a brave person"
Inattent$CM1a <- Inattent$Q27_7 + Inattent$Q106_7 
#"I am a courageous person"
Inattent$CM1b <- Inattent$Q28_11 + Inattent$Q106_18
Inattent$CM1 <- Inattent$CM1a - Inattent$CM1b
Inattent$CM1 <- abs(Inattent$CM1)
Inattent$CM1a <- NULL
Inattent$CM1b <- NULL
"I am a talkative person"
Inattent$CM2a <- Inattent$Q29_8 + Inattent$Q106_26
#"I am a silent person"
Inattent$Q30_8r <- car::recode(Inattent$Q30_8, "1=5; 2=4; 4=2; 5=1")
Inattent$Q106_34r <- car::recode(Inattent$Q106_34, '1=5; 2=4; 4=2; 5=1')
Inattent$CM2b <- Inattent$Q30_8r + Inattent$Q106_34r
Inattent$CM2 <- Inattent$CM2a - Inattent$CM2b
Inattent$CM2 <- abs(Inattent$CM2)
Inattent$CM2a <- NULL
Inattent$CM2b <- NULL
Inattent$Q30_8r <- NULL
Inattent$Q106_34r <- NULL
#"I am an optimistic person"
Inattent$CM3a <- Inattent$Q33_6 + Inattent$Q112_6
#"I am a pessimistic person"
Inattent$Q41_7r <- car::recode(Inattent$Q41_7, '1=5; 2=4; 4=2; 5=1')
Inattent$Q116_7r <- car::recode(Inattent$Q116_7, '1=5; 2=4; 4=2; 5=1')
Inattent$CM3b <- Inattent$Q41_7r + Inattent$Q116_7r
Inattent$CM3 <- Inattent$CM3a - Inattent$CM3b
Inattent$CM3 <- abs(Inattent$CM3)
Inattent$CM3a <- NULL
Inattent$CM3b <- NULL
Inattent$Q41_7r <- NULL
Inattent$Q116_7r <- NULL
#"I seldom feel blue."
Inattent$CM4a <- Inattent$Q31_8 + Inattent$Q106_42
 #"I often feel blue."
Inattent$Q32_5r <- car::recode(Inattent$Q32_5, '1=5; 2=4; 4=2; 5=1')
Inattent$Q111_5r <- car::recode(Inattent$Q111_5, '1=5; 2=4; 4=2; 5=1')
Inattent$CM4b <- Inattent$Q32_5r + Inattent$Q111_5r
Inattent$CM4 <- Inattent$CM4a - Inattent$CM4b
Inattent$CM4 <- abs(Inattent$CM4)
Inattent$CM4a <- NULL
Inattent$CM4b <- NULL
Inattent$Q32_5r <- NULL
Inattent$Q111_5r <- NULL
Inattent$CM <- Inattent$CM1 + Inattent$CM2 + Inattent$CM3 + Inattent$CM4

#Inattent$Q27_7<-	NULL #"I am a brave person"
#Inattent$Q106_7<- NULL
#Inattent$Q28_11<-	NULL #"I am a courageous person"
#Inattent$Q106_18<-	NULL
#Inattent$Q29_8<-	NULL #"I am a talkative person"
#Inattent$Q106_26<-	NULL
#Inattent$Q30_8<-	NULL #"I am a silent person"
#Inattent$Q106_34<-	NULL
#Inattent$Q33_6<-	NULL #"I am an optimistic person"
#Inattent$Q112_6<-	NULL
#Inattent$Q41_7 <- NULL #"I am a pessimistic person"
#Inattent$Q116_7<-	NULL
#Inattent$Q31_8<-	NULL #"I seldom feel blue."
#Inattent$Q106_42<-	NULL
#Inattent$Q32_5<-	NULL #"I often feel blue."
#Inattent$Q111_5<-	NULL

#Create Daily Mood item.
Inattent$MOOD <- Inattent$Q36 + Inattent$Q115

###Create PANAS items.
##PA
#Interested
Inattent$PA1 <- Inattent$Q34_1 + Inattent$Q113_1
#Excited
Inattent$PA2 <- Inattent$Q34_3 + Inattent$Q113_3
#Strong
Inattent$PA3 <- Inattent$Q34_5 + Inattent$Q113_5
#Enthusiastic
Inattent$PA4 <- Inattent$Q34_9 + Inattent$Q113_9
#Proud
Inattent$PA5 <- Inattent$Q34_10 + Inattent$Q113_10
#Alert
Inattent$PA6 <- Inattent$Q35_2 + Inattent$Q114_2
#Inspired
Inattent$PA7 <- Inattent$Q35_4 + Inattent$Q114_4
#Determined
Inattent$PA8 <- Inattent$Q35_6 + Inattent$Q114_6
#Attentive
Inattent$PA9 <- Inattent$Q35_7 + Inattent$Q114_7
#Active
Inattent$PA10 <- Inattent$Q35_9 + Inattent$Q114_9


##NA
#Distressed
Inattent$NA1 <- Inattent$Q34_2 + Inattent$Q113_2
#Upset
Inattent$NA2 <- Inattent$Q34_4 + Inattent$Q113_4
#Guilty
Inattent$NA3 <- Inattent$Q34_6 + Inattent$Q113_6
#Scared
Inattent$NA4 <- Inattent$Q34_7 + Inattent$Q113_7
#Hostile
Inattent$NA5 <- Inattent$Q34_8 + Inattent$Q113_8
#Irritable
Inattent$NA6 <- Inattent$Q35_1 + Inattent$Q114_1
#Ashamed
Inattent$NA7 <- Inattent$Q35_3 + Inattent$Q114_3
#Nervous
Inattent$NA8 <- Inattent$Q35_5 + Inattent$Q114_5
#Jittery
Inattent$NA9 <- Inattent$Q35_8 + Inattent$Q114_8
#Afraid
Inattent$NA10 <- Inattent$Q35_10 + Inattent$Q114_10

	
#Delete PANAS terms
Inattent$Q34_1<-	NULL
Inattent$Q34_2<-	NULL
Inattent$Q34_3<-	NULL
Inattent$Q34_4<-	NULL
Inattent$Q34_5<-	NULL
Inattent$Q34_6<-	NULL
Inattent$Q34_7<-	NULL
Inattent$Q34_8<-	NULL
Inattent$Q34_9<-	NULL
Inattent$Q34_10<-	NULL
Inattent$Q35_1<-	NULL
Inattent$Q35_2<-	NULL
Inattent$Q35_3<-	NULL
Inattent$Q35_4<-	NULL
Inattent$Q35_5<-	NULL
Inattent$Q35_6<-	NULL
Inattent$Q35_7<-	NULL
Inattent$Q35_8<-	NULL
Inattent$Q35_9<-	NULL
Inattent$Q35_10<-	NULL
Inattent$Q113_1<-	NULL
Inattent$Q113_2<-	NULL
Inattent$Q113_3<-	NULL
Inattent$Q113_4<-	NULL
Inattent$Q113_5<-	NULL
Inattent$Q113_6<-	NULL
Inattent$Q113_7<-	NULL
Inattent$Q113_8<-	NULL
Inattent$Q113_9<-	NULL
Inattent$Q113_10<-	NULL
Inattent$Q114_1<-	NULL
Inattent$Q114_2<-	NULL
Inattent$Q114_3<-	NULL
Inattent$Q114_4<-	NULL
Inattent$Q114_5<-	NULL
Inattent$Q114_6<-	NULL
Inattent$Q114_7<-	NULL
Inattent$Q114_8<-	NULL
Inattent$Q114_9<-	NULL
Inattent$Q114_10<-	NULL

#Delete original variables.
Inattent$Q26_1	<-	NULL
Inattent$Q26_2	<-	NULL
Inattent$Q26_3	<-	NULL
Inattent$Q26_4	<-	NULL
Inattent$Q26_5	<-	NULL
Inattent$Q26_6	<-	NULL
Inattent$Q26_7	<-	NULL
Inattent$Q26_8	<-	NULL
Inattent$Q26_9	<-	NULL
Inattent$Q26_10	<-	NULL
Inattent$Q27_1	<-	NULL
Inattent$Q27_2	<-	NULL
Inattent$Q27_3	<-	NULL
Inattent$Q27_4	<-	NULL
Inattent$Q27_5	<-	NULL
Inattent$Q27_6	<-	NULL
Inattent$Q28_1	<-	NULL
Inattent$Q28_2	<-	NULL
Inattent$Q28_3	<-	NULL
Inattent$Q28_4	<-	NULL
Inattent$Q28_5	<-	NULL
Inattent$Q28_6	<-	NULL
Inattent$Q28_7	<-	NULL
Inattent$Q28_8	<-	NULL
Inattent$Q28_9	<-	NULL
Inattent$Q28_10	<-	NULL
Inattent$Q29_1	<-	NULL
Inattent$Q29_2	<-	NULL
Inattent$Q29_3	<-	NULL
Inattent$Q29_4	<-	NULL
Inattent$Q29_5	<-	NULL
Inattent$Q29_6	<-	NULL
Inattent$Q29_7	<-	NULL
Inattent$Q30_1	<-	NULL
Inattent$Q30_2	<-	NULL
Inattent$Q30_3	<-	NULL
Inattent$Q30_4	<-	NULL
Inattent$Q30_5	<-	NULL
Inattent$Q30_6	<-	NULL
Inattent$Q30_7	<-	NULL
Inattent$Q31_1	<-	NULL
Inattent$Q31_2	<-	NULL
Inattent$Q31_3	<-	NULL
Inattent$Q31_4	<-	NULL
Inattent$Q31_5	<-	NULL
Inattent$Q31_6	<-	NULL
Inattent$Q31_7	<-	NULL
Inattent$Q105_1	<-	NULL
Inattent$Q105_2	<-	NULL
Inattent$Q105_3	<-	NULL
Inattent$Q105_4	<-	NULL
Inattent$Q105_5	<-	NULL
Inattent$Q105_6	<-	NULL
Inattent$Q105_7	<-	NULL
Inattent$Q105_8	<-	NULL
Inattent$Q105_9	<-	NULL
Inattent$Q105_10	<-	NULL
Inattent$Q106_1	<-	NULL
Inattent$Q106_2	<-	NULL
Inattent$Q106_3	<-	NULL
Inattent$Q106_4	<-	NULL
Inattent$Q106_5	<-	NULL
Inattent$Q106_6	<-	NULL
Inattent$Q106_8	<-	NULL
Inattent$Q106_9	<-	NULL
Inattent$Q106_10	<-	NULL
Inattent$Q106_11	<-	NULL
Inattent$Q106_12	<-	NULL
Inattent$Q106_13	<-	NULL
Inattent$Q106_14	<-	NULL
Inattent$Q106_15	<-	NULL
Inattent$Q106_16	<-	NULL
Inattent$Q106_17	<-	NULL
Inattent$Q106_19	<-	NULL
Inattent$Q106_20	<-	NULL
Inattent$Q106_21	<-	NULL
Inattent$Q106_22	<-	NULL
Inattent$Q106_23	<-	NULL
Inattent$Q106_24	<-	NULL
Inattent$Q106_25	<-	NULL
Inattent$Q106_27	<-	NULL
Inattent$Q106_28	<-	NULL
Inattent$Q106_29	<-	NULL
Inattent$Q106_30	<-	NULL
Inattent$Q106_31	<-	NULL
Inattent$Q106_32	<-	NULL
Inattent$Q106_33	<-	NULL
Inattent$Q106_35	<-	NULL
Inattent$Q106_36	<-	NULL
Inattent$Q106_37	<-	NULL
Inattent$Q106_38	<-	NULL
Inattent$Q106_39	<-	NULL
Inattent$Q106_40	<-	NULL
Inattent$Q106_41	<-	NULL

#Other variables in the survey that are deleted:
##Note: There were two 
#1. Redundant condition variables
Inattent$Q47<-	NULL
Inattent$Q44<-	NULL
Inattent$Q79<-	NULL

#2. Attention checks (used and unused; the latter refer to psychometric consistency/synonym/antonym items. I chose not to use these because I believe that these can create artificial consistencies in the data).
Inattent$Q26_11<-	NULL #"Click on the first circle indicating "Strongly Disagree."
Inattent$Q105_11<-	NULL

#2b. These items might be used to capture a consistency motif.
Inattent$Q27_7<-	NULL #"I am a brave person"
Inattent$Q106_7<- NULL
Inattent$Q28_11<-	NULL #"I am a courageous person"
Inattent$Q106_18<-	NULL
Inattent$Q29_8<-	NULL #"I am a talkative person"
Inattent$Q106_26<-	NULL
Inattent$Q30_8<-	NULL #"I am a silent person"
Inattent$Q106_34<-	NULL
Inattent$Q33_6<-	NULL #"I am an optimistic person"
Inattent$Q112_6<-	NULL
Inattent$Q41_7 <- NULL #"I am a pessimistic person"
Inattent$Q116_7<-	NULL
Inattent$Q31_8<-	NULL #"I seldom feel blue."
Inattent$Q106_42<-	NULL

#3. Remove common scaling technique measured method effect factor (preference for the color blue).
Inattent$Q32_1<-	NULL #"I prefer blue to other colors."
Inattent$Q32_2<-	NULL #"I like the color blue."
Inattent$Q32_3<-	NULL #"I like blue colors."
Inattent$Q32_4<-	NULL #"I hope my next car is blue."
Inattent$Q32_5<-	NULL #"I often feel blue."
Inattent$Q111_1<-	NULL
Inattent$Q111_2<-	NULL
Inattent$Q111_3<-	NULL
Inattent$Q111_4<-	NULL
Inattent$Q111_5<-	NULL

#4. Remove common scaling technique measured method effect factor (private brand label attitudes).
Inattent$Q33_1<-	NULL #"Buying private label brands makes me feel good."
Inattent$Q33_2<-	NULL #"I love it when private label brands are..."
Inattent$Q33_3<-	NULL #"For most product categories..."
Inattent$Q33_4<-	NULL #"Considering the value for the money, I prefer..."
Inattent$Q33_5<-	NULL #"When I buy a private lable brand, I always..."
Inattent$Q112_1<-	NULL
Inattent$Q112_2<-	NULL
Inattent$Q112_3<-	NULL
Inattent$Q112_4<-	NULL
Inattent$Q112_5<-	NULL

#5. Remove PANAS items.
Inattent$Q34_1<-	NULL
Inattent$Q34_2<-	NULL
Inattent$Q34_3<-	NULL
Inattent$Q34_4<-	NULL
Inattent$Q34_5<-	NULL
Inattent$Q34_6<-	NULL
Inattent$Q34_7<-	NULL
Inattent$Q34_8<-	NULL
Inattent$Q34_9<-	NULL
Inattent$Q34_10<-	NULL
Inattent$Q35_1<-	NULL
Inattent$Q35_2<-	NULL
Inattent$Q35_3<-	NULL
Inattent$Q35_4<-	NULL
Inattent$Q35_5<-	NULL
Inattent$Q35_6<-	NULL
Inattent$Q35_7<-	NULL
Inattent$Q35_8<-	NULL
Inattent$Q35_9<-	NULL
Inattent$Q35_10<-	NULL
Inattent$Q113_1<-	NULL
Inattent$Q113_2<-	NULL
Inattent$Q113_3<-	NULL
Inattent$Q113_4<-	NULL
Inattent$Q113_5<-	NULL
Inattent$Q113_6<-	NULL
Inattent$Q113_7<-	NULL
Inattent$Q113_8<-	NULL
Inattent$Q113_9<-	NULL
Inattent$Q113_10<-	NULL
Inattent$Q114_1<-	NULL
Inattent$Q114_2<-	NULL
Inattent$Q114_3<-	NULL
Inattent$Q114_4<-	NULL
Inattent$Q114_5<-	NULL
Inattent$Q114_6<-	NULL
Inattent$Q114_7<-	NULL
Inattent$Q114_8<-	NULL
Inattent$Q114_9<-	NULL
Inattent$Q114_10<-	NULL

#6. Remove daily hedonic tone.
Inattent$Q36<-	NULL
Inattent$Q115<-	NULL

#7. Remove survey enjoyment and value measure.
Inattent$Q41_1 <-	NULL
Inattent$Q41_2<-	NULL
Inattent$Q41_3<-	NULL
Inattent$Q41_4<-	NULL
Inattent$Q41_5<-	NULL
Inattent$Q41_6<-	NULL
Inattent$Q116_1<-	NULL
Inattent$Q116_2<-	NULL
Inattent$Q116_3<-	NULL
Inattent$Q116_4<-	NULL
Inattent$Q116_5<-	NULL
Inattent$Q116_6<-	NULL

#8. Delete location data.
Inattent$LocationLatitude <- NULL
Inattent$LocationLongitude <- NULL
Inattent$LocationAccuracy <- NULL

#9. Delete confirmation code.
#Inattent$confirmation_code <- NULL

#Rename dataset
data <- Inattent

#Recode 0 values to missing.
data$PP1[data$PP1==0] <- NA
data$PP2[data$PP2==0] <- NA
data$PP3[data$PP3==0] <- NA
data$PP4[data$PP4==0] <- NA
data$PP5[data$PP5==0] <- NA
data$PP6[data$PP6==0] <- NA
data$PP7[data$PP7==0] <- NA
data$PP8[data$PP8==0] <- NA
data$PP9[data$PP9==0] <- NA
data$PP10[data$PP10==0] <- NA
data$VC1[data$VC1==0] <- NA
data$VC2[data$VC2==0] <- NA
data$VC3[data$VC3==0] <- NA
data$VC4[data$VC4==0] <- NA
data$VC5[data$VC5==0] <- NA
data$VC6[data$VC6==0] <- NA
data$TC1[data$TC1==0] <- NA
data$TC2[data$TC2==0] <- NA
data$TC3[data$TC3==0] <- NA
data$TC4[data$TC4==0] <- NA
data$TC5[data$TC5==0] <- NA
data$TC6[data$TC6==0] <- NA
data$TC7[data$TC7==0] <- NA
data$TC8[data$TC8==0] <- NA
data$TC9[data$TC9==0] <- NA
data$TC10[data$TC10==0] <- NA
data$OCBI1[data$OCBI1==0] <- NA
data$OCBI2[data$OCBI2==0] <- NA
data$OCBI3[data$OCBI3==0] <- NA
data$OCBI4[data$OCBI4==0] <- NA
data$OCBI5[data$OCBI5==0] <- NA
data$OCBI6[data$OCBI6==0] <- NA
data$OCBI7[data$OCBI7==0] <- NA
data$OCBO1[data$OCBO1==0] <- NA
data$OCBO2[data$OCBO2==0] <- NA
data$OCBO3[data$OCBO3==0] <- NA
data$OCBO4[data$OCBO4==0] <- NA
data$OCBO5[data$OCBO5==0] <- NA
data$OCBO6[data$OCBO6==0] <- NA
data$OCBO7[data$OCBO7==0] <- NA
data$IRB1[data$IRB1==0] <- NA
data$IRB2[data$IRB2==0] <- NA
data$IRB3[data$IRB3==0] <- NA
data$IRB4[data$IRB4==0] <- NA
data$IRB5[data$IRB5==0] <- NA
data$IRB6[data$IRB6==0] <- NA
data$IRB7[data$IRB7==0] <- NA

#Delete unnecessary variables
data <- data[c(-1,-2,-3,-4,-5,-7, -8,-9,-10,-11,-12,-13,-14,-15,-16,-17,-18,-19, -20, -21, -22,-23,-24,-25,-26,-27,-28,-29,-30,-31,-32,-33,-34,-35,-36)]

##Derive counts. 
#Control
count(Agree, Q47) #"a" is "1" for this figure.
#Treatment
count(Agree, Q79) #"b" is "2" for this figure.

#Screen inattentive responders
count(Agree, Q26_11)
count(Agree, Q105_11)
##Sum all frequencies that aren't 1 and you'll obtain a total 7.

#Omit missing data. 
data <- na.omit(data) 

#Grab counts across conditions
count(data, COND) #Check to make sure that individuals who did not report thier demographics, who abandoned the survey, or did not complete an entire measure were deleted from the analysis (n = 6).
```
Six hundred and twenty-one workers from Amazon's Mechanical Turk were paid $1.30 for completing a survey, of which 556 agreed to participate in our study after reading our informed consent form. These 556 participants were randomly assigned to one of two conditions that began with the content in their informed consent form. In the control condition (n = 274), participants were given a message making the purpose of the study transparent:
The purpose of this study is to test for relationships between proactive personality and workplace behaviors (including taking charge at work, having a voice in the workplace, organizational citizenship behavior at work, and job performance). 
These participants were also given a survey with all items and scales presented in the same order (demographics were presented last). 
In the experimental condition where the proximal separation of measurement remedy was used (n = 282), participants were given a cover story designed to disguise the purpose of the study:
In this study, you will be asked to respond to statements about yourself and how you behave at work. Separate researchers built the content of this survey for separate purposes, and so the questions may or may not relate to one another. As such, there are no right or wrong answers, so please provide honest responses.
Following this manipulation, we asked our participants to respond to the following item indicating whether they understood the purpose of our study: "Before you take our survey, please tell us which of the following correctly describes the purpose of this study." Three response options were given: (a) "The purpose of this study is to test for relationships between personality and workplace behaviors. As such, there is a clear purpose to the study." (b) "Separate researchers built the content of this survey for separate purposes, and so the questions may or may not relate to one another. As such, there is no single clear purpose to this study." and (c) "The purpose of this study is to measure emotional intelligence and workplace behaviors." Individuals in the control condition who selected "a" were allowed to participate whereas individuals in the treatment condition who select "b" were allowed to participate. 
  In addition to the use of a cover story, two blocks of scales were created, one for the proactive personality and another for the remaining criteria scales. We decided to separate proactive personality from the criteria scales to minimize the shared impact of context effects on these measures. These two blocks were then randomized around a set of filler scales that included (1) positive attitudes toward the color blue (4 items), (2) positive brand label attitudes (5 items), (3) the positive and negative affective schedule (20 items), (4) momentary mood (1 item), a measure of survey enjoyment and value (6 items), and two psychometric synonyms and six psychometric antonyms. Items within all scales were also randomized. All participants, regardless of their assigned condition, completed the demographic questionnaire. 
  Interestingly, while our remedy for context effects was designed to reduce the impact of serial order effects across both the predictor and criteria measures, by combining all criteria into the same survey block or webpage, we introduced uncommon method variance attributable to items being presented on the same page/in the same block (hence, the "common block factor" in Figure 1b) (see also Weijters, Beuckelaer, & Baumgartner, 2014). Research suggests that such uncommon method variance would attenuate estimates of predictor-criterion covariances unless controlled (Spector et al., 2016). Therefore, we modeled this source of method variance using the unmeasured latent method construct technique (ULMC; see Williams, Cote, & Buckley, 1989). 
##Measures
###Consistency motif
  To capture the presence of a consistency motif, we employed psychometric synonyms and antonyms that have been used previously (Goldberg & Kilkowski, 1988). Specifically we asked individuals to respond to the following items with a 5-point agreement scale: [synonyms] (1a) "I am a brave person", (1b) "I am a courageous person"; [antonyms] (2a) "I am a talkative person", (2b) "I am a silent person", (3a) " I am an optimistic person" (3b) "I am a pessimistic person", (4a) "I am seldom blue", and (4b) "I am often blue". These items were interspersed throughout the questionnaire, allowing us to capture a consistency motif across the survey. Four difference scores were calculated to capture the extent to which scores for a pair of items diverged. The overall index was scaled such that higher scores on this correspond to higher levels of inconsistent responding.
###Proactive personality
  Bateman and Crant's (1993) 10-item proactive personality scale was used to capture proactive personality. Example items include: "I am constantly on the lookout for new ways to improve," "If I see something I don't like, I fix it," and "I excel at identifying opportunities." This, and all scales included in this study, utilized a five-point agreement Likert rating scale (1 = strongly disagree, 5 = strongly agree). 

###Voice
  Van Dyne and LePine's (1998) 6-item scale, which are based on a modification of Van Dyne, Graham, and Dienesch's (1994) scale, were used. Example items include "I develop and make recommendations concerning issues that affect my work group" and "I speak up and ecnourage others in this group to get involved in issues that affect this group."

###Taking charge
  Morrison and Phelp's (1999) 10-item measure of taking charge was used for this study. Example items include "I often try to bring about improved procedures for my work unit or department" and "I often try to adopt improved procedures for doing my job."

###In-role and organizational citizenship behavior 
  Williams and Anderson's (1991) In-Role Performance Behavior (IRB) and Organizational Citizenship Behavior (OCB) scales, the latter of which includes OCBI (OCB directed at the individual) and OCBO (OCB directed at the organization, were used as dependent variables. Example items include: "I perform tasks that are expected of me" (IRB), "I take a personal interest in other employees"" (OCBI), and "I conserve and protect organizational property" (OCBO). Cronbach alphas were .82, .83, and .71 for IRB, OCBI, and OCBO, respectively.

###Data Quality
  To help ensure data quality, we used an inattentive responding check (Mead & Craig, 2012), specifically we asked individuals to "Click on the first circle indicating "Strongly Disagree?""

##Analytical Approach
  To examine the presence of method variance, we used latent variable modleing strategies proposed by method variance researchers (e.g., Williams et al., 2010; McGonagle & Williams, 2015). More specifically, we aplied the same latent variable model to both remedied and non-remedied observations seperately, following the approached used by a similar investigation (e.g., Johnson et al., 2011). A series of diifferent models were nested in the data, varying in their assumptions regarding the role played by method factors. For instance, method variance attributable to a specific cause could present equivalent effects on all measures or vary in its extent and impact across all or only a few measures. We followed guidance offered by McGonagle and Williams (2015) to conduct the series of model comparisons needed to indicate the impact of CMV and to also test for biased parameter estimates. Following convention (Williams et al., 2010; McGonagle & Williams, 2015), we used chi-square difference testing to compare our models and an alpha level of .05 was chosen for declaring models as statistically different. We carried out our analyses using 'lavaan' in R.
  Other key modeling details require explanation. Given the presence of negatively-keyed items in our measurement model, we created a negatively-keyed method factor that explained variance in the negatively-keyed items only (see Zhang & Savalei, 2015; Dalal & Carter, 2015). This negative method factor was theoretically independent from the other latent factors given the assumption that methods be uncorrelated with traits (see Conway & Lance, 2010). Also, we modeled an ULMC in our remedied condition to control for method variance shared by our criteria measures that was attributable to those items being on the same webage (see Weijters et al., 2014). 

##Data Screening Process 
Approximately 75% (n = 204) percent of participants randomly assigned to the control condition and approximately 50% (n = 143) in the experimental condition correctly responded to the manipulation check and were allowed to participate in our study. Seven more individuals were screened out for incorrectly responding to attentive responding item. Thirty five people were omitted for failing to respond to the inattentive responding check quesiton. Lastly, individuals who did not report thier demographics, who abandoned the survey, or did not complete an entire measure were deleted from the analysis (n = 6). This filtering process resulted in a final sample of 299 individuals (182 in the control and 117 in the experimental condition).

#Results - Study 2
##Manipulation check for consistency motifs 
To determine if our treatment weakened the consistency motif, we conducted an independent samples t-test to test for a statistically significant differences in consistency motif scores. The results were significant (t = 2.015, df = 297, p = .045) and revealed a small to moderate effect (d = 0.24, 95%CI:[.008, .72]) linking our condition to consistency index scores. The pattern of the results suggested that scores in the control condition were more consistent than scores in the treatment condition (control: M = 1.62, SD = 1.46; treatment: M = 1.98, SD = 1.60). Though scores on the consistency motif index were low in both conditions and suggested that participants were generally consistent, our manipulation check suggests that we successfully weakened reliance on consistency motifs. 

Having demonstrated a succesful manipulation, we subsetted our data into separate groups, specified a causal model as outlined by figure 1b, examined the method effects present in each group. The process was applied for both control and remedied conditions separately as these conditions would vary in their respective method effects, rendering the results of a multi-group analysis suspect. Specifically for the remedied condition, an unmeasured latent method construct factor was included in the model as representing a common blocking factor that exerted equal contaminating effects on all criterion measurement variables. This unmeasured latent method factor was modeled as causing an equal amount of method variance that would be common to the criterion variables because these measures were in the same block of items. Because there were nested model comparisons, we have chosen to only report the best-fitting models for each respective dataset along with predictor-criterion standardized regression estimates (see Table x). Note: for the best-fitting method effects model applied to the remedied condition, we observed a noncongeneric (equal) method effect factor loading of .21 (p = .003) for all criterion variables, which we believed captured a common item blocking factor.
```{r Proximal Separation Substantive Analyses, include=FALSE}
#Consistency moftif scores
independentSamplesTTest(CM ~ COND, data = data, var.equal = TRUE, conf.level = .95)
boxplot(data$CM ~ data$COND)
#Consider showing the boxplots.

#Factor analyses
initial.cfa  <-  '
#Substantive Factors
  PR =~ NA*PP1 + PP2 + PP3 + PP4 + PP5 + PP6 + PP7 + PP8 + PP9 + PP10
  VOICE =~ NA*VOICE1 + VOICE2 + VOICE3 + VOICE4 + VOICE5 + VOICE6
  TC =~ NA*TC1 + TC2 + TC3 + TC4 + TC5 + TC6 + TC7 + TC8 + TC9 + TC10
  IRB =~ NA*IRB1 + IRB2 + IRB3 + IRB4 + IRB5 + IRB6 + IRB7
  OCBI =~ NA*OCBI1 + OCBI2 + OCBI3 + OCBI4 + OCBI5 + OCBI6 + OCBI7
  OCBO =~ NA*OCBO1 + OCBO2 +OCBO3 + OCBO4 + OCBO5 + OCBO6 + OCBO7

#Method Factors
  PA =~ NA*PA1 + PA2 + PA3 + PA4 + PA5 + PA6 + PA7 + PA8 + PA9 + PA10 
  Na =~ NA*NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9 + NA10
  AP =~ NA*PA1 + PA2 + PA3 + PA4 + PA5 + PA6 + PA7 + PA8 + PA9 + PA10 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9 + NA10
  Mood =~ MOOD_T1
  NegIW =~ NA*IRB6 + IRB7 + OCBO2 +OCBO3 + OCBO4

#Method causal structure
  Mood ~ PA + NA

#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M

#set covariances for bifactor to zero
  PA ~~ 0*Na
  PA ~~ 0*AP
  Na ~~ 0*AP

#variances to one
  PA ~~ 1*PA
  Na ~~ 1*Na
  AP ~~ 1*AP
  Mood ~~ 1*Mood
  PR ~~ 1*PR
  VOICE ~~ 1*VOICE
  TC ~~ 1*TC
  IRB ~~ 1*IRB
  OCBI ~~ 1*OCBI
  OCBO ~~ 1*OCBO
  NegIW ~~ 1*NegIW
'

#To see the results of an anlysis of the discriminant validity of the measurement model, see the following object. 
dv <- htmt(initial.cfa, data2, missing="ML", ordered = c("PA1", "PA2", "PA3", "PA4", "PA5", "PA6", "PA7", "PA8", "PA9", "PA10", "NA1", "NA2", "NA3", "NA4", "NA5", "NA6", "NA7", "NA8", "NA9", "NA10", "PP1", "PP2", "PP3", "PP4", "PP5", "PP6", "PP7", "PP8", "PP9", "PP10", "VOICE1", "VOICE2", "VOICE3", "VOICE4", "VOICE5", "VOICE6", "TC1", "TC2", "TC3", "TC4", "TC5", "TC6", "TC7", "TC8", "TC9", "TC10", "IRB1", "IRB2", "IRB3", "IRB4", "IRB5", "IRB6", "IRB7", "OCBI1", "OCBI2", "OCBI3", "OCBI4", "OCBI5", "OCBI6", "OCBI7", "OCBO1", "OCBO2", "OCBO3", "OCBO4", "OCBO5", "OCBO6", "OCBO7"))
##Results indicate acceptable discriminant validity according to Hensler et al. (2015) criterion of .85 HTMT criterion. Need to re-examine once I can estimate confidence intervals because VOICE is really close to taking charge. This may not be so much of a problem here because method effects would be expected to inflate certain correlations and attenunate others. 
                  
#Note: Group 1 is the temporally separted condition. Group 2 is the same-time-point condition.
#Configural invariance
config <- cfa(initial.cfa, ordered = c("PA1", "PA2", "PA3", "PA4", "PA5", "PA6", "PA7", "PA8", "PA9", "PA10", "NA1", "NA2", "NA3", "NA4", "NA5", "NA6", "NA7", "NA8", "NA9", "NA10", "Mood_T1", "PP1", "PP2", "PP3", "PP4", "PP5", "PP6", "PP7", "PP8", "PP9", "PP10", "VOICE1", "VOICE2", "VOICE3", "VOICE4", "VOICE5", "VOICE6", "TC1", "TC2", "TC3", "TC4", "TC5", "TC6", "TC7", "TC8", "TC9", "TC10", "IRB1", "IRB2", "IRB3", "IRB4", "IRB5", "IRB6", "IRB7", "OCBI1", "OCBI2", "OCBI3", "OCBI4", "OCBI5", "OCBI6", "OCBI7", "OCBO1", "OCBO2", "OCBO3", "OCBO4", "OCBO5", "OCBO6", "OCBO7"), data = data2, group="COND", missing="ML", estimator = "DWLS")
##Note: initial assessments revealed a negative residual variance. This seems to be attributable to the amount of skew in the IRB scale (which is where the Heywood case is popping up).

mod <- modindices(config)
fitmeasures(config, c("cfi","tli","rmsea","srmr","chisq","df","pvalue"))
pe <- parameterEstimates(config)
summary(config, standardized = TRUE, fit.measures = TRUE)
semPaths(config, "std", curvePivot = TRUE, thresholds = FALSE)

#Examine measurement invariance
mi <- measurementInvarianceCat(initial.cfa, ordered = c("PA1", "PA2", "PA3", "PA4", "PA5", "PA6", "PA7", "PA8", "PA9", "PA10", "NA1", "NA2", "NA3", "NA4", "NA5", "NA6", "NA7", "NA8", "NA9", "NA10", "Mood_T1", "PP1", "PP2", "PP3", "PP4", "PP5", "PP6", "PP7", "PP8", "PP9", "PP10", "VOICE1", "VOICE2", "VOICE3", "VOICE4", "VOICE5", "VOICE6", "TC1", "TC2", "TC3", "TC4", "TC5", "TC6", "TC7", "TC8", "TC9", "TC10", "IRB1", "IRB2", "IRB3", "IRB4", "IRB5", "IRB6", "IRB7", "OCBI1", "OCBI2", "OCBI3", "OCBI4", "OCBI5", "OCBI6", "OCBI7", "OCBO1", "OCBO2", "OCBO3", "OCBO4", "OCBO5", "OCBO6", "OCBO7"), data = data2, group="COND", missing="ML", estimator = "DWLS")

##Obtain unstandardized factor loadings and residual residual variances to levels found in the initial CFA, which should be fixed in the baseline models. 
initial.c <- cfa(initial.sem.c, estimator = "MLM", data=control, std.lv = TRUE)
summary(initial.c, standardized = TRUE, fit.measures = TRUE)

#Control Baseline
#Fix factor loadings and residual variances to initial levels.
baseline.sem.c  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ PR
  TC ~ PR
  IRB ~ PR
  OCBI ~ PR
  OCBO ~ PR
#Define Method Factors
  PA =~ 3.450*PAp1 + 0.686*PAp2 + 0.745*PAp3 + 2.255*PAp4 + 0.861*PAp5
  Na =~ 0.954*Nap1 + 1.363*Nap2 + 0.943*Nap3 + 1.101*Nap4 + 0.835*Nap5
  M =~ 1.224*MOOD
  CMi =~ 1.458*CM
  Neg =~ 0.562*IRBp2 + 1.363*OCBOp3
#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 
#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO  
#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO  
#Fix all consistency motif covariances to zero.
  CMi ~~ 0*PR
  CMi ~~ 0*VC
  CMi ~~ 0*TC
  CMi ~~ 0*IRB
  CMi ~~ 0*OCBI
  CMi ~~ 0*OCBO  
  CMi ~~ 0*PA
  CMi ~~ 0*Na
  CMi ~~ 0*M
#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M
  Neg ~~ 0*CMi
#Method Regressions
  M ~ PA + Na
#Fixed residuals
  PAp1 ~~ 4.074*PAp1
  PAp2 ~~ 0.380*PAp2
  PAp3 ~~ 0.519*PAp3
  PAp4 ~~ 1.163*PAp4
  PAp5 ~~ 0.579*PAp5
  Nap1 ~~ 0.649*Nap1
  Nap2 ~~ 0.498*Nap2
  Nap3 ~~ 0.845*Nap3
  Nap4 ~~ 1.063*Nap4
  Nap5 ~~ 0.459*Nap5
  IRBp2 ~~ 0.670*IRBp2
  OCBOp3 ~~ 1.510*OCBOp3
  MOOD ~~ 0*MOOD
  CM   ~~ 0*CM
'
baseline.c <- cfa(baseline.sem.c, estimator = "MLM", data=control, std.lv = TRUE)
summary(baseline.c, standardized = TRUE, fit.measures = TRUE)

#Method C
methodc.sem.c  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ PR
  TC ~ PR
  IRB ~ PR
  OCBI ~ PR
  OCBO ~ PR
#Define Method Factors
  PA =~ 3.450*PAp1 + 0.686*PAp2 + 0.745*PAp3 + 2.255*PAp4 + 0.861*PAp5 + PPp1 + PPp2 + PPp3 + PPp4 + VCp1 + VCp2 + VCp3 + TCp1 + TCp2 + TCp3 + IRBp1 + IRBp2 + OCBIp1 + OCBIp2 + OCBIp3 + OCBOp1 + OCBOp2 + OCBOp3
  Na =~ 0.954*Nap1 + 1.363*Nap2 + 0.943*Nap3 + 1.101*Nap4 + 0.835*Nap5 + PPp2 + PPp4 + IRBp2 + OCBIp1
  M =~ 1.224*MOOD + TCp3 + IRBp1 + OCBIp1 + OCBOp1 + OCBOp2
  CMi =~ 1.458*CM + v4*PPp1 + v4*PPp2 + v4*PPp3 + v4*PPp4 + v4*VCp1 + v4*VCp2 + v4*VCp3 + v4*TCp1 + v4*TCp2 + v4*TCp3 + v4*IRBp1 + v4*IRBp2 + v4*IRBp3 + v4*OCBIp1 + v4*OCBIp2 + v4*OCBIp3 + v4*OCBOp1 + v4*OCBOp2 + v4*OCBOp3
  Neg =~ 0.562*IRBp2 + 1.363*OCBOp3
#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 
#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO  
#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO  
#Fix all consistency motif covariances to zero.
  CMi ~~ 0*PR
  CMi ~~ 0*VC
  CMi ~~ 0*TC
  CMi ~~ 0*IRB
  CMi ~~ 0*OCBI
  CMi ~~ 0*OCBO  
  CMi ~~ 0*PA
  CMi ~~ 0*Na
  CMi ~~ 0*M
#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M
  Neg ~~ 0*CMi
#Method Regressions
  M ~ PA + Na
#Fixed residuals
  PAp1 ~~ 4.074*PAp1
  PAp2 ~~ 0.380*PAp2
  PAp3 ~~ 0.519*PAp3
  PAp4 ~~ 1.163*PAp4
  PAp5 ~~ 0.579*PAp5
  Nap1 ~~ 0.649*Nap1
  Nap2 ~~ 0.498*Nap2
  Nap3 ~~ 0.845*Nap3
  Nap4 ~~ 1.063*Nap4
  Nap5 ~~ 0.459*Nap5
  IRBp2 ~~ 0.670*IRBp2
  OCBOp3 ~~ 1.510*OCBOp3
  MOOD ~~ 0*MOOD
  CM   ~~ 0*CM
'
methodc.sem.c <- cfa(methodc.sem.c, estimator = "MLM", data=control, std.lv = TRUE)
summary(methodc.sem.c, standardized = TRUE, fit.measures = TRUE)
anova(baseline.c,methodc.sem.c)
#Baseline is statistically significant from model c. 

#METHOD U
methodu.sem.c  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ PR
  TC ~ PR
  IRB ~ PR
  OCBI ~ PR
  OCBO ~ PR
#Define Method Factors
  PA =~ 3.450*PAp1 + 0.686*PAp2 + 0.745*PAp3 + 2.255*PAp4 + 0.861*PAp5 + PPp1 + PPp2 + PPp3 + PPp4 + VCp1 + VCp2 + VCp3 + TCp1 + TCp2 + TCp3 + IRBp1 + IRBp2 + OCBIp1 + OCBIp2 + OCBIp3 + OCBOp1 + OCBOp2 + OCBOp3
  Na =~ 0.954*Nap1 + 1.363*Nap2 + 0.943*Nap3 + 1.101*Nap4 + 0.835*Nap5
  M =~ 1.224*MOOD + TCp3 + IRBp1 + OCBIp1 + OCBOp1 + OCBOp2
  CMi =~ 1.458*CM + OCBIp3 + OCBOp2
  Neg =~ 0.562*IRBp2 + 1.363*OCBOp3
#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 
#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO  
#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO  
#Fix all consistency motif covariances to zero.
  CMi ~~ 0*PR
  CMi ~~ 0*VC
  CMi ~~ 0*TC
  CMi ~~ 0*IRB
  CMi ~~ 0*OCBI
  CMi ~~ 0*OCBO  
  CMi ~~ 0*PA
  CMi ~~ 0*Na
  CMi ~~ 0*M
#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M
  Neg ~~ 0*CMi
#Method Regressions
  M ~ PA + Na
#Fixed residuals
  PAp1 ~~ 4.074*PAp1
  PAp2 ~~ 0.380*PAp2
  PAp3 ~~ 0.519*PAp3
  PAp4 ~~ 1.163*PAp4
  PAp5 ~~ 0.579*PAp5
  Nap1 ~~ 0.649*Nap1
  Nap2 ~~ 0.498*Nap2
  Nap3 ~~ 0.845*Nap3
  Nap4 ~~ 1.063*Nap4
  Nap5 ~~ 0.459*Nap5
  IRBp2 ~~ 0.670*IRBp2
  OCBOp3 ~~ 1.510*OCBOp3
  MOOD ~~ 0*MOOD
  CM   ~~ 0*CM
'
methodu.sem.c <- cfa(methodu.sem.c, estimator = "MLM", data=control, std.lv = TRUE)
summary(methodu.sem.c, standardized = TRUE, fit.measures = TRUE)
anova(methodu.sem.c,methodc.sem.c)
anova(baseline.c, methodu.sem.c)
#Method U is superior to baseline model. Congeneric CMV is present. 

#METHOD R
methodr.sem.c  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ 1.221*PR
  TC ~ 1.199*PR
  IRB ~ 0.410*PR
  OCBI ~ 0.763*PR
  OCBO ~ 0.444*PR
#Define Method Factors
  PA =~ 3.450*PAp1 + 0.686*PAp2 + 0.745*PAp3 + 2.255*PAp4 + 0.861*PAp5 + PPp1 + PPp2 + PPp3 + PPp4 + VCp1 + VCp2 + VCp3 + TCp1 + TCp2 + TCp3 + IRBp1 + IRBp2 + OCBIp1 + OCBIp2 + OCBIp3 + OCBOp1 + OCBOp2 + OCBOp3
  Na =~ 0.954*Nap1 + 1.363*Nap2 + 0.943*Nap3 + 1.101*Nap4 + 0.835*Nap5
  M =~ 1.224*MOOD + TCp3 + IRBp1 + OCBIp1 + OCBOp1 + OCBOp2
  CMi =~ 1.458*CM + OCBIp3 + OCBOp2
  Neg =~ 0.562*IRBp2 + 1.363*OCBOp3
#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 
#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO  
#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO  
#Fix all consistency motif covariances to zero.
  CMi ~~ 0*PR
  CMi ~~ 0*VC
  CMi ~~ 0*TC
  CMi ~~ 0*IRB
  CMi ~~ 0*OCBI
  CMi ~~ 0*OCBO  
  CMi ~~ 0*PA
  CMi ~~ 0*Na
  CMi ~~ 0*M
#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M
  Neg ~~ 0*CMi
#Method Regressions
  M ~ PA + Na
#Fixed residuals
  PAp1 ~~ 4.074*PAp1
  PAp2 ~~ 0.380*PAp2
  PAp3 ~~ 0.519*PAp3
  PAp4 ~~ 1.163*PAp4
  PAp5 ~~ 0.579*PAp5
  Nap1 ~~ 0.649*Nap1
  Nap2 ~~ 0.498*Nap2
  Nap3 ~~ 0.845*Nap3
  Nap4 ~~ 1.063*Nap4
  Nap5 ~~ 0.459*Nap5
  IRBp2 ~~ 0.670*IRBp2
  OCBOp3 ~~ 1.510*OCBOp3
  MOOD ~~ 0*MOOD
  CM   ~~ 0*CM
'
methodr.sem.c <- cfa(methodr.sem.c, estimator = "MLM", data=control, std.lv = TRUE)
summary(methodr.sem.c, standardized = TRUE, fit.measures = TRUE)
anova(methodr.sem.c,methodu.sem.c)
#NOTE: Method effects attributable to positive affectivity, negative affectivity, mood, and negative item wording were observed. However, a test for method effects attributable to consistency motifs was not observed.
```
```{r}
#TREATMENT
initial.t  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ PR
  TC ~ PR
  IRB ~ PR
  OCBI ~ PR
  OCBO ~ PR
#Method Factors
  PA =~ PAp1 + PAp2 + PAp3 + PAp4 + PAp5
  Na =~ Nap1 + Nap2 + Nap3 + Nap4 + Nap5
  M =~ MOOD
  CMi =~ CM
  Neg =~ IRBp2 + OCBOp3
  Block =~ a*VCp1 + a*VCp2 + a*VCp3 + a*TCp1 + a*TCp2 + a*TCp3 + a*TCp1 + a*TCp2 + a*TCp3 + a*IRBp1 + a*IRBp2 + a*IRBp3 + a*OCBIp1 + a*OCBIp2 + a*OCBIp3 + a*OCBOp1 + a*OCBOp2 + a*OCBOp3
#Fix all Block-Factor Covariances to zero.
  Block ~~ 0*PR
  Block ~~ 0*VC
  Block ~~ 0*TC
  Block ~~ 0*IRB
  Block ~~ 0*OCBI
  Block ~~ 0*OCBO
  Block ~~ 0*PA
  Block ~~ 0*Na
  Block ~~ 0*M
  Block ~~ 0*CMi
  Block ~~ 0*Neg
#Fix residual variances for Heywood cases.
  IRBp1 ~~ 0*IRBp1
  OCBOp3 ~~ 0*OCBOp3
'
#Note: Two Heywood cases emerged, but they were non-significant (IRBp1 and OCBOp3). So they were constrained to zero. 
##Obtain unstandardized factor loadings and residual residual variances to levels found in the initial CFA, which should be fixed in the baseline models. 
initial.sem.t <- cfa(initial.t, estimator = "MLM", data=treatment, std.lv = TRUE)
summary(initial.sem.t, standardized = TRUE, fit.measures = TRUE)

baseline.t  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ PR
  TC ~ PR
  IRB ~ PR
  OCBI ~ PR
  OCBO ~ PR
#Method Factors
  PA =~ 2.964*PAp1 + 0.653*PAp2 + 0.614*PAp3 + 2.358*PAp4 + 0.700*PAp5
  Na =~ 1.243*Nap1 + 1.285*Nap2 + 1.267*Nap3 + 1.273*Nap4 + 0.989*Nap5
  M =~ 1.495*MOOD
  CMi =~ 1.590*CM
  Neg =~ 0.675*IRBp2 + 1.903*OCBOp3
  Block =~ c1*VCp1 + c1*VCp2 + c1*VCp3 + c1*TCp1 + c1*TCp2 + c1*TCp3 + c1*IRBp1 + c1*IRBp2 + c1*IRBp3 + c1*OCBIp1 + c1*OCBIp2 + c1*OCBIp3 + c1*OCBOp1 + c1*OCBOp2 + c1*OCBOp3
#Fix all Block-Factor Covariances to zero.
  Block ~~ 0*PR
  Block ~~ 0*VC
  Block ~~ 0*TC
  Block ~~ 0*IRB
  Block ~~ 0*OCBI
  Block ~~ 0*OCBO
  Block ~~ 0*PA
  Block ~~ 0*Na
  Block ~~ 0*M
  Block ~~ 0*CMi
  Block ~~ 0*Neg
#Fix residual variances for Heywood cases.
  IRBp1 ~~ 0*IRBp1
  OCBOp3 ~~ 0*OCBOp3
#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 
#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO
#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO 
#Fix all consistency motif covariances to zero.
  CMi ~~ 0*PR
  CMi ~~ 0*VC
  CMi ~~ 0*TC
  CMi ~~ 0*IRB
  CMi ~~ 0*OCBI
  CMi ~~ 0*OCBO  
  CMi ~~ 0*PA
  CMi ~~ 0*Na
  CMi ~~ 0*M
#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M
  Neg ~~ 0*CMi
#Method Regressions
  M ~ PA + Na
#Fixed residuals
  PAp1 ~~ 4.502*PAp1
  PAp2 ~~ 0.576*PAp2
  PAp3 ~~ 0.539*PAp3
  PAp4 ~~ 1.363*PAp4
  PAp5 ~~ 0.553*PAp5
  Nap1 ~~ 0.363*Nap1
  Nap2 ~~ 0.658*Nap2
  Nap3 ~~ 0.410*Nap3
  Nap4 ~~ 1.126*Nap4
  Nap5 ~~ 0.786*Nap5
  IRBp2 ~~ 1.191*IRBp2
  MOOD ~~ 0*MOOD
  CM   ~~ 0*CM
'
baseline.sem.t <- cfa(baseline.t, estimator = "MLM", data=treatment, std.lv = TRUE)
summary(baseline.sem.t, standardized = TRUE, fit.measures = TRUE)
anova(baseline.sem.t, initial.sem.t)

#Method C
methodc.t  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ PR
  TC ~ PR
  IRB ~ PR
  OCBI ~ PR
  OCBO ~ PR
#Method Factors
  PA =~ 2.964*PAp1 + 0.653*PAp2 + 0.614*PAp3 + 2.358*PAp4 + 0.700*PAp5 + v1*PPp1 + v1*PPp2 + v1*PPp3 + v1*PPp4 + v1*VCp1 + v1*VCp2 + v1*VCp3 + v1*TCp1 + v1*TCp2 + v1*TCp3 + v1*IRBp1 + v1*IRBp2 + v1*IRBp3 + v1*OCBIp1 + v1*OCBIp2 + v1*OCBIp3 + v1*OCBOp1 + v1*OCBOp2 + v1*OCBOp3
  Na =~ 1.243*Nap1 + 1.285*Nap2 + 1.267*Nap3 + 1.273*Nap4 + 0.989*Nap5 + v2*PPp1 + v2*PPp2 + v2*PPp3 + v2*PPp4 + v2*VCp1 + v2*VCp2 + v2*VCp3 + v2*TCp1 + v2*TCp2 + v2*TCp3 + v2*IRBp1 + v2*IRBp2 + v2*IRBp3 + v2*OCBIp1 + v2*OCBIp2 + v2*OCBIp3 + v2*OCBOp1 + v2*OCBOp2 + v2*OCBOp3
  M =~ 1.495*MOOD + PPp1 + PPp2 + PPp3 + PPp4 + VCp1 + VCp2 + VCp3 + TCp1 + TCp2 + TCp3 + IRBp1 + IRBp2 + IRBp3 + OCBIp1 + OCBIp2 + OCBIp3 + OCBOp1 + OCBOp2 + OCBOp3
  CMi =~ 1.590*CM + v4*PPp1 + v4*PPp2 + v4*PPp3 + v4*PPp4 + v4*VCp1 + v4*VCp2 + v4*VCp3 + v4*TCp1 + v4*TCp2 + v4*TCp3 + v4*IRBp1 + v4*IRBp2 + v4*IRBp3 + v4*OCBIp1 + v4*OCBIp2 + v4*OCBIp3 + v4*OCBOp1 + v4*OCBOp2 + v4*OCBOp3
  Neg =~ 0.675*IRBp2 + 1.903*OCBOp3
  Block =~ c1*VCp1 + c1*VCp2 + c1*VCp3 + c1*TCp1 + c1*TCp2 + c1*TCp3 + c1*IRBp1 + c1*IRBp2 + c1*IRBp3 + c1*OCBIp1 + c1*OCBIp2 + c1*OCBIp3 + c1*OCBOp1 + c1*OCBOp2 + c1*OCBOp3
#Fix all Block-Factor Covariances to zero.
  Block ~~ 0*PR
  Block ~~ 0*VC
  Block ~~ 0*TC
  Block ~~ 0*IRB
  Block ~~ 0*OCBI
  Block ~~ 0*OCBO
  Block ~~ 0*PA
  Block ~~ 0*Na
  Block ~~ 0*M
  Block ~~ 0*CMi
  Block ~~ 0*Neg
#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 
#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO
#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO 
#Fix all consistency motif covariances to zero.
  CMi ~~ 0*PR
  CMi ~~ 0*VC
  CMi ~~ 0*TC
  CMi ~~ 0*IRB
  CMi ~~ 0*OCBI
  CMi ~~ 0*OCBO  
  CMi ~~ 0*PA
  CMi ~~ 0*Na
  CMi ~~ 0*M
#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M
  Neg ~~ 0*CMi
#Method Regressions
  M ~ PA + Na
#Fixed residuals
  PAp1 ~~ 4.502*PAp1
  PAp2 ~~ 0.576*PAp2
  PAp3 ~~ 0.539*PAp3
  PAp4 ~~ 1.363*PAp4
  PAp5 ~~ 0.553*PAp5
  Nap1 ~~ 0.363*Nap1
  Nap2 ~~ 0.658*Nap2
  Nap3 ~~ 0.410*Nap3
  Nap4 ~~ 1.126*Nap4
  Nap5 ~~ 0.786*Nap5
  IRBp2 ~~ 1.191*IRBp2
  MOOD ~~ 0*MOOD
  CM   ~~ 0*CM
#Fix Heywood cases
  IRBp1 ~~ 0*IRBp1
  OCBOp3 ~~ 0*OCBOp3
'
methodc.sem.t <- cfa(methodc.t, estimator = "MLM", data=treatment, std.lv = TRUE)
summary(methodc.sem.t, standardized = TRUE, fit.measures = TRUE)
anova(methodc.sem.t,baseline.sem.t)
#Heywood cases emerged (2; IRBp1, OCBOp3). They were non-significant.

#Method U
methodu.t  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ PR
  TC ~ PR
  IRB ~ PR
  OCBI ~ PR
  OCBO ~ PR
#Method Factors
  PA =~ 2.964*PAp1 + 0.653*PAp2 + 0.614*PAp3 + 2.358*PAp4 + 0.700*PAp5 + PPp1 + PPp2 + PPp3 + PPp4 + VCp1 + VCp3 + TCp1 + TCp2 + TCp3 + OCBIp1 + OCBIp2 + OCBIp3 + OCBOp1 + OCBOp2
  Na =~ 1.243*Nap1 + 1.285*Nap2 + 1.267*Nap3 + 1.273*Nap4 + 0.989*Nap5 + VCp3
  M =~ 1.495*MOOD + IRBp1
  CMi =~ 1.590*CM
  Neg =~ 0.675*IRBp2 + 1.903*OCBOp3
  Block =~ c1*VCp1 + c1*VCp2 + c1*VCp3 + c1*TCp1 + c1*TCp2 + c1*TCp3 + c1*IRBp1 + c1*IRBp2 + c1*IRBp3 + c1*OCBIp1 + c1*OCBIp2 + c1*OCBIp3 + c1*OCBOp1 + c1*OCBOp2 + c1*OCBOp3
#Fix all Block-Factor Covariances to zero.
  Block ~~ 0*PR
  Block ~~ 0*VC
  Block ~~ 0*TC
  Block ~~ 0*IRB
  Block ~~ 0*OCBI
  Block ~~ 0*OCBO
  Block ~~ 0*PA
  Block ~~ 0*Na
  Block ~~ 0*M
  Block ~~ 0*CMi
  Block ~~ 0*Neg
#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 
#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO
#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO 
#Fix all consistency motif covariances to zero.
  CMi ~~ 0*PR
  CMi ~~ 0*VC
  CMi ~~ 0*TC
  CMi ~~ 0*IRB
  CMi ~~ 0*OCBI
  CMi ~~ 0*OCBO  
  CMi ~~ 0*PA
  CMi ~~ 0*Na
  CMi ~~ 0*M
#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M
  Neg ~~ 0*CMi
#Method Regressions
  M ~ PA + Na
#Fixed residuals
  PAp1 ~~ 4.502*PAp1
  PAp2 ~~ 0.576*PAp2
  PAp3 ~~ 0.539*PAp3
  PAp4 ~~ 1.363*PAp4
  PAp5 ~~ 0.553*PAp5
  Nap1 ~~ 0.363*Nap1
  Nap2 ~~ 0.658*Nap2
  Nap3 ~~ 0.410*Nap3
  Nap4 ~~ 1.126*Nap4
  Nap5 ~~ 0.786*Nap5
  IRBp2 ~~ 1.191*IRBp2
  MOOD ~~ 0*MOOD
  CM   ~~ 0*CM
#Fix Heywood cases
  IRBp1 ~~ 0*IRBp1
  OCBOp3 ~~ 0*OCBOp3
'
methodu.sem.t <- cfa(methodu.t, estimator = "MLM", data=treatment, std.lv = TRUE)
summary(methodu.sem.t, standardized = TRUE, fit.measures = TRUE)
anova(methodu.sem.t,methodc.sem.t)

#METHOD R
methodr.t  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ 1.375*PR
  TC ~ 1.104*PR
  IRB ~ 0.234*PR
  OCBI ~ 0.507*PR
  OCBO ~ 0.501*PR
#Method Factors
  PA =~ 2.964*PAp1 + 0.653*PAp2 + 0.614*PAp3 + 2.358*PAp4 + 0.700*PAp5 + PPp1 + PPp2 + PPp3 + PPp4 + VCp1 + VCp3 + TCp1 + TCp2 + TCp3 + OCBIp1 + OCBIp2 + OCBIp3 + OCBOp1 + OCBOp2
  Na =~ 1.243*Nap1 + 1.285*Nap2 + 1.267*Nap3 + 1.273*Nap4 + 0.989*Nap5 + VCp3
  M =~ 1.495*MOOD + IRBp1
  CMi =~ 1.590*CM
  Neg =~ 0.675*IRBp2 + 1.903*OCBOp3
  Block =~ c1*VCp1 + c1*VCp2 + c1*VCp3 + c1*TCp1 + c1*TCp2 + c1*TCp3 + c1*IRBp1 + c1*IRBp2 + c1*IRBp3 + c1*OCBIp1 + c1*OCBIp2 + c1*OCBIp3 + c1*OCBOp1 + c1*OCBOp2 + c1*OCBOp3
#Fix all Block-Factor Covariances to zero.
  Block ~~ 0*PR
  Block ~~ 0*VC
  Block ~~ 0*TC
  Block ~~ 0*IRB
  Block ~~ 0*OCBI
  Block ~~ 0*OCBO
  Block ~~ 0*PA
  Block ~~ 0*Na
  Block ~~ 0*M
  Block ~~ 0*CMi
  Block ~~ 0*Neg
#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 
#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO  
#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO  
#Fix all consistency motif covariances to zero.
  CMi ~~ 0*PR
  CMi ~~ 0*VC
  CMi ~~ 0*TC
  CMi ~~ 0*IRB
  CMi ~~ 0*OCBI
  CMi ~~ 0*OCBO  
  CMi ~~ 0*PA
  CMi ~~ 0*Na
  CMi ~~ 0*M
#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M
  Neg ~~ 0*CMi
#Method Regressions
  M ~ PA + Na
#Fixed residuals
  PAp1 ~~ 4.502*PAp1
  PAp2 ~~ 0.576*PAp2
  PAp3 ~~ 0.539*PAp3
  PAp4 ~~ 1.363*PAp4
  PAp5 ~~ 0.553*PAp5
  Nap1 ~~ 0.363*Nap1
  Nap2 ~~ 0.658*Nap2
  Nap3 ~~ 0.410*Nap3
  Nap4 ~~ 1.126*Nap4
  Nap5 ~~ 0.786*Nap5
  IRBp2 ~~ 1.191*IRBp2
  MOOD ~~ 0*MOOD
  CM   ~~ 0*CM
#Fix Heywood cases
  IRBp1 ~~ 0*IRBp1
  OCBOp3 ~~ 0*OCBOp3
'
methodr.sem.t <- cfa(methodr.t, estimator = "MLM", data=control, std.lv = TRUE)
summary(methodr.sem.t, standardized = TRUE, fit.measures = TRUE)
anova(methodu.sem.t,methodr.sem.t)
```
The results of our tests and model comparisons are available in table x. The model data fit for our baseline models (i.e., models without method effects included) were generally unacceptable, which is common when method effects have not been modeled (see Williams et al., 2010). As can be seen, modeling method effects improved the fit of the model to the data relative to baseline levels. The best-fitting method effect models were the most flexible ones (i.e., congeneric) with some consistency to both substantive and method effects in terms of direction and magnitude. Across all models, the predictor-criterion relationships were positive. The series of model comparisons consistently (as in across conditions) revealed congeneric (unequal) method effects attributable to positive affectivity, negative affectivity, mood, and negative item wording. However, consistency motif effects were observed in the non-remedied condition for two of the OCBO parcels. While this does suggest that consistency motifs played a role in our non-remedied condition, and were absent in the remedied condition, the magnitude of these effects seem negligible. In fact, overall tests for method bias in the control condition failed to support method bias attributable to all method variance sources. Additionally, as expected, a blocking factor emerged in the remedied condition and exerted a constant effect on the measured variables for our criteria. Further testing revealed that all method effects in the remedied condition were enough to bias the observed correlations and that once these method effects were modeled, the proactive personality-role behavior relationships became non-significant. All other substantive effects were reduced in magnitude. The magnitude and direction of all method effects can be observed in table x. 

Table z displays the decomposed reliabilities of each scale as a function of substantive and method variance and also lists the percentage of overall method variance attributable to these different sources. As can be seen, there were multiple sources of method variance evident in our measurement model. Consistently, positive affectivity exerted an influence over responses to measures of proactive personality, voice, taking charge, in-role behavior, and OCBI. For OCBO, which was primarily method variance (63% in the control vs. 74% in the remedied condition), multiple method sources were present. 

#Discussion Study 2
With our first study, we sought to test the role played by multiple sources of method variance (i.e., consistency motif, measurement context, affectivity, mood, negative item wording, and measurement context). More specifically, we experimentally manipulated both consistency motifs and measurement context in a practical manner, allowing us to test the causal role of these method variance sources in a realistic research setting. Additionally, we controlled for a variety of other sources of method variance to ensure that our causal effects would be reliably estimated, allowing us to draw strong conclusions about the magnitude of consistency motifs and measurement context effects.  

Generally speaking, our results cast doubt onto the strength of consistency motifs and provide mixed support for measurement context effects. For consistency motifs, we observered that they played stronger role in our non-remedied condition compared to our remedied condition, but lacked the potency for causing method bias in this condition. Consistency motifs were absent in our remedied condition. This suggests that our treatment (e.g., a cover story) weakens consistency motifs. Specifically, we found only two significant effects (p < .05) linking consistency motif scores to our parcels. Though this was in the expected direction (i.e., increasing consistency) and in the expected condition (non-remedied), their magnitude was small and ultimately negligible (factor loadings = .13 and .14). These effects were not observed in our remedied condition. Therefore, our findings call into question the potency of consistency motifs as a cause of method bias.

Our results suggest that method variance is far more nuanced than suggested by the early literature. Firstly, our results support the view that method variance is best viewed multidimensionally and approached in an multipronged manner. For instance, we found several method effects linked to positive affect, mood, negative item wording, and (presumably) being in a common block. These forces were also strong enough to cause biased estimates of latent construct correlation in our remedied condition, some of which became non-significant once method effects were controlled. However, certain causes of method variance seem insufficient for causing bias. 

#Study 3 - The Efficacy of Individual Proximal Remedies for MV
With this study, we examined the efficacy of individual remeides for method variance. In other words, individuals were randomly assigned to receive only (a) a cover story, (b) randomized items, (c) randomized scales, (d) a set of filler scales, or (e) were assigned to a conrol condition whereby no remeides were applied. This allowed us to isolate the effects of specific remedies. 
```{r Source Data message = FALSE, warning = FALSE}
#library("papaja")
library("readr")
library("qualtRics")
library("datapack")

#Optional generic preliminaries.
graphics.off() # This closes all of R's graphics windows.
rm(list=ls())  # Careful! This clears all of R's memory!

######  BUILD DATASETS  ######

#Link to Qualtrics
registerOptions(api_token="U8ut239nN4WeqDo0V1cDniUGMBG1jRtICUgZw0hp", root_url="rutgers.qualtrics.com")
surveys <- getSurveys()
#Get survey data
mysurvey <- getSurvey(surveys$id[3], force_request = TRUE)
##Prune the stuff we don't need.#Note: Make sure to keep the "Finished" variable as some folks do not turn in the survey even though they complete it. 
mysurvey <- mysurvey[c(-1,-2,-3,-4,-5,-6,-7,-8,-9,-11,-13)]
data <- mysurvey
##Collapse like data across columns.
###Conditions; build independent data sets and then rename the condition variable.
###1 Control
data$Demand.ConsitMotif1[is.na(data$Demand.ConsitMotif1)] <- 0
data$Demand.ConsitMotif1 <- as.numeric(data$Demand.ConsitMotif1)
###2 Cover Story Manipulation
data$CoverStoryManip <- ifelse(data$CoverStoryManip == 1, c("2"))
data$CoverStoryManip[is.na(data$CoverStoryManip)] <- 0
data$CoverStoryManip <- as.numeric(data$CoverStoryManip)
###3 Randomized Items
data$Demand.ConsitMotif3 <- ifelse(data$Demand.ConsitMotif3 == 1, c("3"))
data$Demand.ConsitMotif3[is.na(data$Demand.ConsitMotif3)] <- 0
data$Demand.ConsitMotif3 <- as.numeric(data$Demand.ConsitMotif3)
###4 Filler Scales
data$Demand.ConsitMotif4 <- ifelse(data$Demand.ConsitMotif4 == 1, c("4"))
data$Demand.ConsitMotif4[is.na(data$Demand.ConsitMotif4)] <- 0
data$Demand.ConsitMotif4 <- as.numeric(data$Demand.ConsitMotif4)
###5 Randomized Scales
data$Demand.ConsitMotif2 <- ifelse(data$Demand.ConsitMotif2 == 1, c("5"))
data$Demand.ConsitMotif2[is.na(data$Demand.ConsitMotif2)] <- 0
data$Demand.ConsitMotif2 <- as.numeric(data$Demand.ConsitMotif2)
#Combine condition data into single variable column
data$Condition <- data$Demand.ConsitMotif1 + data$CoverStoryManip + data$Demand.ConsitMotif3 + data$Demand.ConsitMotif4 + data$Demand.ConsitMotif2
####Delete old condition data.
data <- subset(data, select=-c(Demand.ConsitMotif1, CoverStoryManip, Demand.ConsitMotif3, Demand.ConsitMotif4, Demand.ConsitMotif2))
#Apply value lables
data$Condition <- factor(data$Condition, levels = c(1,2,3,4,5), labels = c("Control", "CSManipulation","ItemRand","FillerScales","ScaleRand"))
###Combine all open-ended responses to the manipulation check for study purpose
data$PurpCk1 <- paste(data$PurpChk1a, data$PurpChk2a,data$PurpChk3a, data$PurpChk4a, data$PurpChk5a)
data$PurpCk2 <- paste(data$PurpChk1b, data$PurpChk2b,data$PurpChk3b, data$PurpChk4b, data$PurpChk5b) 
####Delete old purpose chk data. 
data <- subset(data, select=-c(PurpChk1a, PurpChk1b, PurpChk2a, PurpChk2b, PurpChk3a, PurpChk3b, PurpChk4a, PurpChk4b, PurpChk5a, PurpChk5b))
####Delte meaningless condition data. IC = Informed Consent. FOR = Frame of Reference.
data <- subset(data, select=-c(IC1, IC2, IC3, Q136, FOR1, FOR2, FOR4, FOR5, FORx))
#Move Finished to the end of the file.
moveMe <- function(data, tomove, where = "last", ba = NULL) {
  temp <- setdiff(names(data), tomove)
  x <- switch(
    where,
    first = data[c(tomove, temp)],
    last = data[c(temp, tomove)],
    before = {
      if (is.null(ba)) stop("must specify ba column")
      if (length(ba) > 1) stop("ba must be a single character string")
      data[append(temp, values = tomove, after = (match(ba, temp)-1))]
    },
    after = {
      if (is.null(ba)) stop("must specify ba column")
      if (length(ba) > 1) stop("ba must be a single character string")
      data[append(temp, values = tomove, after = (match(ba, temp)))]
    })
  x
}
data <- moveMe(data, c("Finished"),"after", "PurpCk2")
#Delete Prep and Text variables.
data <- data[c(-2,-3,-69,-135,-201,-268)]
#Consolidate item response measures
##Form Likert datasets, re-lable values, convert to numeric data, consolidate measures, and delete data.
agree <- data[c(2:41,67:106,132:171,198:209,235:302,329:335)]
agree <- ifelse(agree == "Strongly Disagree", 1, ifelse(agree == "Disagree", 2, ifelse(agree == "Neither Agree nor Disagree", 3, ifelse(agree == "Agree", 4, ifelse(agree == "Strongly Agree", 5,0)))))
write.csv(agree, file = "agree.csv")
agree <- read_csv("agree.csv")
#####Proactive Personality 
agree$PP1	<-	agree$PP1_1	+	agree$PP2_1	+	agree$PP5_1	+	agree$PP6_1	+	agree$PPx_1
agree$PP2	<-	agree$PP1_2	+	agree$PP2_2	+	agree$PP5_2	+	agree$PP6_2	+	agree$PPx_2
agree$PP3	<-	agree$PP1_3	+	agree$PP2_3	+	agree$PP5_3	+	agree$PP6_3	+	agree$PPx_3
agree$PP4	<-	agree$PP1_4	+	agree$PP2_4	+	agree$PP5_4	+	agree$PP6_4	+	agree$PPx_4
agree$PP5	<-	agree$PP1_5	+	agree$PP2_5	+	agree$PP5_5	+	agree$PP6_5	+	agree$PPx_5
agree$PP6	<-	agree$PP1_6	+	agree$PP2_6	+	agree$PP5_6	+	agree$PP6_6	+	agree$PPx_6
agree$PP7	<-	agree$PP1_7	+	agree$PP2_7	+	agree$PP5_7	+	agree$PP6_7	+	agree$PPx_7
agree$PP8	<-	agree$PP1_8	+	agree$PP2_8	+	agree$PP5_8	+	agree$PP6_8	+	agree$PPx_8
agree$PP9	<-	agree$PP1_9	+	agree$PP2_9	+	agree$PP5_9	+	agree$PP6_9	+	agree$PPx_9
agree$PP10	<-	agree$PP1_10	+	agree$PP2_10	+	agree$PP5_10	+	agree$PP6_10	+	agree$PPx_10
#####OCBI
agree$OCBI1	<-	agree$OCBI1_1	+	agree$OCBI2_1	+	agree$OCBI5_1	+	agree$OCBI6_1	+	agree$OCBIx_1
agree$OCBI2	<-	agree$OCBI1_3	+	agree$OCBI2_3	+	agree$OCBI5_3	+	agree$OCBI6_3	+	agree$OCBIx_3
agree$OCBI3	<-	agree$OCBI1_4	+	agree$OCBI2_4	+	agree$OCBI5_4	+	agree$OCBI6_4	+	agree$OCBIx_4
agree$OCBI4	<-	agree$OCBI1_5	+	agree$OCBI2_5	+	agree$OCBI5_5	+	agree$OCBI6_5	+	agree$OCBIx_5
agree$OCBI5	<-	agree$OCBI1_6	+	agree$OCBI2_6	+	agree$OCBI5_6	+	agree$OCBI6_6	+	agree$OCBIx_6
agree$OCBI6	<-	agree$OCBI1_7	+	agree$OCBI2_7	+	agree$OCBI5_7	+	agree$OCBI6_7	+	agree$OCBIx_7
agree$OCBI7	<-	agree$OCBI1_8	+	agree$OCBI2_8	+	agree$OCBI5_8	+	agree$OCBI6_8	+	agree$OCBIx_8
#####OCBO
agree$OCBO1	<-	agree$OCBO1_1	+	agree$OCBO2_1	+	agree$OCBO5_1	+	agree$OCBO6_1	+	agree$OCBOx_1
agree$OCBO2	<-	agree$OCBO1_2	+	agree$OCBO2_2	+	agree$OCBO5_2	+	agree$OCBO6_2	+	agree$OCBOx_2
agree$OCBO3	<-	agree$OCBO1_3	+	agree$OCBO2_3	+	agree$OCBO5_3	+	agree$OCBO6_3	+	agree$OCBOx_3
agree$OCBO4	<-	agree$OCBO1_5	+	agree$OCBO2_5	+	agree$OCBO5_5	+	agree$OCBO6_5	+	agree$OCBOx_5
agree$OCBO5	<-	agree$OCBO1_6	+	agree$OCBO2_6	+	agree$OCBO5_6	+	agree$OCBO6_6	+	agree$OCBOx_6
agree$OCBO6	<-	agree$OCBO1_7	+	agree$OCBO2_7	+	agree$OCBO5_7	+	agree$OCBO6_7	+	agree$OCBOx_7
agree$OCBO7	<-	agree$OCBO1_8	+	agree$OCBO2_8	+	agree$OCBO5_8	+	agree$OCBO6_8	+	agree$OCBOx_8
#####IRB
agree$IRB1	<-	agree$IRB1_1	+	agree$IRB2_1	+	agree$IRB5_1	+	agree$IRB6_1	+	agree$IRBx_1
agree$IRB2	<-	agree$IRB1_3	+	agree$IRB2_3	+	agree$IRB5_3	+	agree$IRB6_3	+	agree$IRBx_3
agree$IRB3	<-	agree$IRB1_4	+	agree$IRB2_4	+	agree$IRB5_4	+	agree$IRB6_4	+	agree$IRBx_4
agree$IRB4	<-	agree$IRB1_5	+	agree$IRB2_5	+	agree$IRB5_5	+	agree$IRB6_5	+	agree$IRBx_5
agree$IRB5	<-	agree$IRB1_6	+	agree$IRB2_6	+	agree$IRB5_6	+	agree$IRB6_6	+	agree$IRBx_6
agree$IRB6	<-	agree$IRB1_8	+	agree$IRB2_8	+	agree$IRB5_8	+	agree$IRB6_8	+	agree$IRBx_8
agree$IRB7	<-	agree$IRB1_9	+	agree$IRB2_9	+	agree$IRB5_9	+	agree$IRB6_9	+	agree$IRBx_9
#####Inattentive Responding
agree$IR	<-	agree$PP1_11	+	agree$PP2_11	+	agree$PP5_11	+	agree$PP6_11	+	agree$PPx_11
#####Consistency Motif
#Brave
agree$CM1	<-	agree$PP1_12	+	agree$PP2_12	+	agree$PP5_12	+	agree$PP6_12	+	agree$PPx_12
#Often feel blue.
agree$CM2	<-	agree$OCBI1_2	+	agree$OCBI2_2	+	agree$OCBI5_2	+	agree$OCBI6_2	+	agree$OCBIx_2
#Talkative
agree$CM3	<-	agree$OCBI1_9	+	agree$OCBI2_9	+	agree$OCBI5_9	+	agree$OCBI6_9	+	agree$OCBIx_9
#Pessimistic Person
agree$CM4	<-	agree$OCBO1_4	+	agree$OCBO2_4	+	agree$OCBO5_4	+	agree$OCBO6_4	+	agree$OCBOx_4
#Silent Person
agree$CM5	<-	agree$OCBO1_9	+	agree$OCBO2_9	+	agree$OCBO5_9	+	agree$OCBO6_9	+	agree$OCBOx_9
#Courageous
agree$CM6	<-	agree$IRB1_2	+	agree$IRB2_2	+	agree$IRB5_2	+	agree$IRB6_2	+	agree$IRBx_2
#Optimistic 
agree$CM7	<-	agree$IRB1_7	+	agree$IRB2_7	+	agree$IRB5_7	+	agree$IRB6_7	+	agree$IRBx_7
#Seldom feel blue.
agree$CM8	<-	agree$IRB1_10	+	agree$IRB2_10	+	agree$IRB5_10	+	agree$IRB6_10	+	agree$IRBx_10
#Consolidate "agree" dataset.
agree <- agree[c(202:248)]
#HALO
halo <- data[c(42:45,107:110,172:175,210:213,303:306)]
halo <- ifelse(halo == "Very bad", 1, ifelse(halo == "Bad", 2, ifelse(halo == "Somewhat bad", 3, ifelse(halo == "Neither good nor bad", 4, ifelse(halo == "Somewhat good", 5, ifelse(halo == "Good", 6, ifelse(halo == "Very good", 7, 0)))))))
write.csv(halo, file = "halo.csv")
halo <- read_csv("halo.csv")
halo$HALO1	<-	halo$HALO1_1	+	halo$HALO2_1	+	halo$HALO5_1	+	halo$HALO6_1 +	halo$HALOx_1
halo$HALO2	<-	halo$HALO1_2	+	halo$HALO2_2	+	halo$HALO5_2	+	halo$HALO6_2 +	halo$HALOx_2
halo$HALO3	<-	halo$HALO1_3	+	halo$HALO2_3	+	halo$HALO5_3	+	halo$HALO6_3 +	halo$HALOx_3
halo$HALO4	<-	halo$HALO1_4	+	halo$HALO2_4	+	halo$HALO5_4	+	halo$HALO6_4 +	halo$HALOx_4
#Consolidate "halo" dataset.
halo <- halo[c(22:25)]
#PANAS
PANAS <- data[c(46:65,111:130,176:195,214:233,307:326)]
PANAS <- ifelse(PANAS == "Very slightly or not at all", 1, ifelse(PANAS == "A little", 2, ifelse(PANAS == "Moderately", 3, ifelse(PANAS == "Quite a bit", 4, ifelse(PANAS == "Extremely", 5, 0)))))
write.csv(PANAS, file = "PANAS.csv")
PANAS <- read_csv("PANAS.csv")
PANAS$PAff1	<-	PANAS$PAff1_1	+	PANAS$PAff2_1	+	PANAS$PAff5_1	+	PANAS$PAff6_1	+	PANAS$PAffx_1
PANAS$PAff2	<-	PANAS$PAff1_2	+	PANAS$PAff2_2	+	PANAS$PAff5_2	+	PANAS$PAff6_2	+	PANAS$PAffx_2
PANAS$PAff3	<-	PANAS$PAff1_3	+	PANAS$PAff2_3	+	PANAS$PAff5_3	+	PANAS$PAff6_3	+	PANAS$PAffx_3
PANAS$PAff4	<-	PANAS$PAff1_4	+	PANAS$PAff2_4	+	PANAS$PAff5_4	+	PANAS$PAff6_4	+	PANAS$PAffx_4
PANAS$PAff5	<-	PANAS$PAff1_5	+	PANAS$PAff2_5	+	PANAS$PAff5_5	+	PANAS$PAff6_5	+	PANAS$PAffx_5
PANAS$PAff6	<-	PANAS$PAff1_6	+	PANAS$PAff2_6	+	PANAS$PAff5_6	+	PANAS$PAff6_6	+	PANAS$PAffx_6
PANAS$PAff7	<-	PANAS$PAff1_7	+	PANAS$PAff2_7	+	PANAS$PAff5_7	+	PANAS$PAff6_7	+	PANAS$PAffx_7
PANAS$PAff8	<-	PANAS$PAff1_8	+	PANAS$PAff2_8	+	PANAS$PAff5_8	+	PANAS$PAff6_8	+	PANAS$PAffx_8
PANAS$PAff9	<-	PANAS$PAff1_9	+	PANAS$PAff2_9	+	PANAS$PAff5_9	+	PANAS$PAff6_9	+	PANAS$PAffx_9
PANAS$PAff10	<-	PANAS$PAff1_10	+	PANAS$PAff2_10	+	PANAS$PAff5_10	+	PANAS$PAff6_10	+	PANAS$PAffx_10
PANAS$NAff1	<-	PANAS$NAff1_1	+	PANAS$NAff2_1	+	PANAS$NAff5_1	+	PANAS$NAff6_1	+	PANAS$NAffx_1
PANAS$NAff2	<-	PANAS$NAff1_2	+	PANAS$NAff2_2	+	PANAS$NAff5_2	+	PANAS$NAff6_2	+	PANAS$NAffx_2
PANAS$NAff3	<-	PANAS$NAff1_3	+	PANAS$NAff2_3	+	PANAS$NAff5_3	+	PANAS$NAff6_3	+	PANAS$NAffx_3
PANAS$NAff4	<-	PANAS$NAff1_4	+	PANAS$NAff2_4	+	PANAS$NAff5_4	+	PANAS$NAff6_4	+	PANAS$NAffx_4
PANAS$NAff5	<-	PANAS$NAff1_5	+	PANAS$NAff2_5	+	PANAS$NAff5_5	+	PANAS$NAff6_5	+	PANAS$NAffx_5
PANAS$NAff6	<-	PANAS$NAff1_6	+	PANAS$NAff2_6	+	PANAS$NAff5_6	+	PANAS$NAff6_6	+	PANAS$NAffx_6
PANAS$NAff7	<-	PANAS$NAff1_7	+	PANAS$NAff2_7	+	PANAS$NAff5_7	+	PANAS$NAff6_7	+	PANAS$NAffx_7
PANAS$NAff8	<-	PANAS$NAff1_8	+	PANAS$NAff2_8	+	PANAS$NAff5_8	+	PANAS$NAff6_8	+	PANAS$NAffx_8
PANAS$NAff9	<-	PANAS$NAff1_9	+	PANAS$NAff2_9	+	PANAS$NAff5_9	+	PANAS$NAff6_9	+	PANAS$NAffx_9
PANAS$NAff10	<-	PANAS$NAff1_10	+	PANAS$NAff2_10	+	PANAS$NAff5_10	+	PANAS$NAff6_10	+	PANAS$NAffx_10
#Consolidate "PANAS" dataset.
PANAS <- PANAS[c(102:121)]
#Consolidate manipulation checks. 
##Deleting likert/psychological data.
data <- subset(data,select=-c(2:327,329:335))
##Deleting other nuissance variables.
data <- subset(data,select=-c(2,13,23:36))
##Reorganize data.
#data <- data[,c(1,6:14, 15,16, 2, 3, 4, 5)]
#write.csv(data, file = "proximalCMV2.csv")
#Bind all datasets.
data <- cbind(data,agree,halo,PANAS)

#MTurk IGNORE THIS SEGMENT as MTurkR is now compromised.
#require("MTurkR")
#Link to MTurk (Work on MTurkR)
#Sys.setenv("AWS_ACCESS_KEY_ID" = "")
#Sys.setenv("AWS_SECRET_ACCESS_KEY" = "")
#Test connection to live server
#AccountBalance()
#SearchHITS
#hits <- SearchHITs()
#Buy more HITS. CAREFUL. 
#ExtendHIT(hit.type = "3SVZJ4A2P5CW4G2BZY7UE78GFL1IEV",add.assignments = 60, add.seconds = seconds(days=7))
#GetHITs and merge into MTurk dataframe. Use "SearchHITS()" to find the correct HIT. Make sure you have the right HITId. The followering are seprated out by batches.
#Go to MTurk account and manually download batch results. Place the batch in the working directory. 
a <- read_csv("Batch_2846969_batch_results.csv")
#Manually edit confirmation codes.
#a$Answer.surveycode[a$Answer.surveycode=="A11FRLH5KWRLBV"] <- "96090306"
#How long did the average participant take to complete the survey?
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
getmode(a$WorkTimeInSeconds)/60

#Create seperate datasets to identify possible those who did not complete the survey. Convert confirmation_code from character to numeric.
data.mt <- a[c(15,16,17,24,28)]
#Relabel variables to allow merging.
names(data.mt) <- c("AssignmentID","WorkerID","AssignmentStatus","Seconds","confirmation_code")
data.mt <- transform(data.mt, confirmation_code = as.numeric(confirmation_code))
#Ensure that paid participants' data are analyzed
data1 <- merge(data.mt, data, ID="confirmation_code",  all.x = TRUE)
#Acquire data from individuals who completed the survey in earnest. 
data1.1 <- merge(data.mt, data, ID="confirmation_code",  all.y = TRUE)
data1.1 <- data1.1[is.na(data1.1$WorkerID),]
rownames(data1.1) <- seq(length=nrow(data1.1)) 
#Subset in individuals providing a confirmation code that is not in our database. 
false.code <- data1[is.na(data1$Finished),]
false.code <- subset(false.code, AssignmentStatus=='Submitted')
#Subset in individuals with complete data and get rid of the workerIDs.
data2 <- merge(data.mt, data, ID="confirmation_code")
data2 <- rbind(data2,data1.1)
data <- data2[c(-2)]
#Subset into submitted, accepted, and rejected.
submitted <- data.mt[ which(data.mt$AssignmentStatus=='Submitted'), ]
approved <- data.mt[ which(data.mt$AssignmentStatus=='Approved'), ]
rejected <- data.mt[ which(data.mt$AssignmentStatus=='Rejected'), ]
#Build dataset
a <- a[c(28,24)]
names(a) <- c("confirmation_code", "WorkTimeInSeconds")
df <- merge(a,data,by="confirmation_code")
#Get RID of the worker IDS.
data <- data[c(-2)]

###### ENGAGE MACHINE LEARNING TO AUTOMATE MANIPULATION CHECK ######
#Engage supervised machine learning to analyze purpose check data, which will tell us if participants actually watched the video and got the purpose of the study. 
library(caret)
library(tm)
library(SnowballC)
library(stringr)
#Training data. NOTE: You'll need to filter in the 30 cases that force participants to distinguish purpose 1 from purpose 2. This will improve the accuracy of the algorithms.
data.t <- data[1:60]
data.t <- data.t[c(23,24,25,26)]
data.t[] <- lapply(data.t, str_trim)
is.na(data.t) <- data.t==''
data.t <- na.omit(data.t)
cond.code <- data.t[c(1)]
data.t <- data.t[c(2)]
corpus <- VCorpus(VectorSource(data.t$PurpCk1))
##Create a text document matrix.
tdm <- DocumentTermMatrix(corpus, list(removePunctuation = TRUE, stripWhitespace = TRUE, content_transformer(tolower), stopwords = TRUE, stemming = FALSE, removeNumbers = FALSE))
##Convert to a data.frame for training and assign a classification (factor) to each document.
train <- as.matrix(tdm)
#Create condition code that differentiates individuals from conditions.
cond.code <- as.data.frame(ifelse(cond.code$Condition== "CSManipulation", 1,0))
train <- cbind(train, cond.code)
colnames(train)[ncol(train)] <- 'y'
train <- as.data.frame(train)
train$y <- as.factor(train$y)
##Train.
require(foreach)
registerDoSEQ()
#Training control prevents model over-fitting. 
tc <- trainControl(method = "cv", number = 7, verboseIter=FALSE , preProcOptions="pca", allowParallel=TRUE)
bayesglm <- train(y ~ ., data = train, method = 'bayesglm', trControl=tc)
rf <- train(y ~ ., data = train, method = 'rf', trControl=tc)
NN <- train(y ~ ., data = train, method = 'nnet', trControl=tc, verbose=FALSE)
svml <- train(y ~ ., data = train, method = 'svmLinear', trControl=tc)
logitboost <- train(y ~ ., data = train, method = 'LogitBoost', trControl=tc)

#This is used to complare the models against one anoher.
model <- c("Bayes GLM", "Neural Net", "SVM (linear)", "LogitBoost")
Accuracy <- c(max(bayesglm$results$Accuracy),
              max(NN$results$Accuracy),
              max(svml$results$Accuracy),
              max(logitboost$results$Accuracy))
Kappa <- c(max(bayesglm$results$Kappa),
           max(NN$results$Kappa),
           max(svml$results$Kappa),
           max(logitboost$results$Kappa))
performance <- cbind(model,Accuracy,Kappa)
knitr::kable(performance)
#Create new dataset and fit models to new data.
data.n <- data[1:60]
data.n <- data.n[c(24,25,26)]

#ERROR HERE <- number of columns of arguments do not match
#data.n <- rbind(data.t,data.n)
data.n[] <- lapply(data.n, str_trim)
is.na(data.n) <- data.n==''
data.n <- na.omit(data.n)
data.n <- unique(data.n)
data.n <- data.n$PurpCk1
data.n <- as.data.frame(data.n)
#Fit model to new data
corpus <- VCorpus(VectorSource(data.n$data.n))
##Create a text document matrix.
tdm <- DocumentTermMatrix(corpus, list(removePunctuation = TRUE, stripWhitespace = TRUE, content_transformer(tolower), stopwords = TRUE, stemming = FALSE, removeNumbers = FALSE))
test <- as.matrix(tdm)
##Convert to a data.frame for training and assign a classification (factor) to each document.
z <- as.data.frame(predict(bayesglm,test))
###IT WORKS!!!! Send to team. 
share <- subset(data, Condition=="CSManipulation")
share <- share[c(24,25)]

#Assuming that some people correctly guessed the hypothesis, then engage supervised machine learning to analyze hypothesis guessing data. 
#data <- data[1:30,]
#data <- df$HypoGuessing2

######  DEVELOP SCALE SCORES AND EVALUATE USING CLASSICAL TEST THEORY ###### 
require(psych)
#see  the example including the bfi data set
keys.list <- list(PP=c("PP1","PP2","PP3","PP4","PP5","PP6","PP7","PP8","PP9","PP10"),OCBI=c("OCBI1","OCBI2","OCBI3","OCBI4","OCBI5","OCBI6","OCBI7"), OCBO=c("OCBO1","OCBO2","-OCBO3","-OCBO4","-OCBO5","OCBO6","OCBO7"), IRB=c("IRB1","IRB2","IRB3","IRB4","IRB5","-IRB6","-IRB7")) 
keys <- make.keys(df,keys.list)  #no longer necessary
scores <- scoreItems(keys,df,min=1,max=5)  #using a keys matrix 
#Create scale scores
scores <- as.data.frame(scores$scores)
#Bind scales with condition variables. 
final <- as.data.frame(df$Condition)
names(final) <- c("Condition")
final <- cbind(final,scores)

###Evaluate Scale Reliability. Code examining scale reliability is needed. Wayne, I can teach you how to do this. 
library(CTT)

######  TEST HYPOTHESES ###### 
require(BayesFactor)
plot(PP ~ Condition, data = final)
plot(OCBI ~ Condition, data = final)
plot(OCBO ~ Condition, data = final)
plot(IRB ~ Condition, data = final)

#Check the BayesFactor package to make sure that you are using the correct formula. 

#Proactive Personality 
describeBy(final$PP, final$Condition )
summary(aov(PP ~ Condition, data = final))
anovaBF(PP ~ Condition, data = final, progress=FALSE)
###Seems to favor null so far...

#OCBI
describeBy(final$OCBI, final$Condition )
summary(aov(OCBI ~ Condition, data = final))
anovaBF(OCBI ~ Condition, data = final, progress=FALSE)
###Seems to favor null so far...

#OCBO
describeBy(final$OCBO, final$Condition )
summary(aov(OCBO ~ Condition, data = final))
anovaBF(OCBO ~ Condition, data = final, progress=FALSE)

#IRB
describeBy(final$IRB, final$Condition )
summary(aov(IRB ~ Condition, data = final))
anovaBF(IRB ~ Condition, data = final, progress=FALSE)

######  Bayesian Measurement Invariance ######  
#The goal of the analysis is to examine the invariance (or lackthereof) of various invariant or non-invariant models. In other words, test for stricter and stricter forms of invariance. If survey design features are causing differences, then a non-invariant model will provide a better fit to the data than a model assuming invariance.
#require(blavaan)
#SubstantiveModel <- '
##Substantive Factors
##  PR =~ PP1+PP2+PP3+PP4+PP5+PP6+PP7+PP8+PP9+PP10
##  IRB =~ IRB1+IRB2+IRB3+IRB4+IRB5+IRB6+IRB7
##  OCBI =~ OCBI1+OCBI2+OCBI3+OCBI4+OCBI5+OCBI6+OCBI7
#  OCBO =~ OCBO1+OCBO2+OCBO3+OCBO4+OCBO5+OCBO6+OCBO7
#'
#bfit1 <- bcfa(SubstantiveModel, data = df, burnin = 4000, sample = 10000, jagcontrol = list(method = "rjparallel"))
#summary(bfit1, standardized = TRUE)
##Model is poor fit to the data.
#Equal loadings
#bfit2 <- bcfa(HS.model, data = HolzingerSwineford1939, group = "school", burnin = 1000, sample = 1000, group.equal = "loadings", jagcontrol = list(method = "rjparallel"))
#blavCompare(bfit1, bfit2)
##Conclusion: Configural invariance is 19.836 times more probable than configural non-invariance (though this model asssuming configural invariance is also, still, a bad model). 
#bfit3 <- bcfa(HS.model, data = HolzingerSwineford1939, group = "school", burnin = 1000, sample = 1000, group.equal = c("loadings","intercepts"), jagcontrol = list(method = "rjparallel"))
#blavCompare(bfit2, bfit3)
#fitmeasures(bfit3)
#summary(bfit1, standardized = TRUE)
#blavInspect(bfit1,"hpd")
##Plots:
#plot(bfit1, plot.type="trace")
#plot(bfit1, plot.type="autocorr")
#plot(bfit1, "histogram")

#write CSV file
write.csv(final, file = "data.csv")
```
```{r Isolated Proximal Manipulations}
#Link to Qualtrics
registerOptions(api_token="U8ut239nN4WeqDo0V1cDniUGMBG1jRtICUgZw0hp", root_url="rutgers.qualtrics.com")
surveys <- getSurveys()
#Get survey data
mysurvey <- getSurvey(surveys$id[3], force_request = TRUE)
##Prune the stuff we don't need.#Note: Make sure to keep the "Finished" variable as some folks do not turn in the survey even though they complete it. 
mysurvey <- mysurvey[c(-1,-2,-3,-4,-5,-6,-7,-8,-9,-11,-13)]
data <- mysurvey
##Collapse like data across columns.
###Conditions; build independent data sets and then rename the condition variable.
###1 Control
data$Demand.ConsitMotif1[is.na(data$Demand.ConsitMotif1)] <- 0
data$Demand.ConsitMotif1 <- as.numeric(data$Demand.ConsitMotif1)
###2 Cover Story Manipulation
data$CoverStoryManip <- ifelse(data$CoverStoryManip == 1, c("2"))
data$CoverStoryManip[is.na(data$CoverStoryManip)] <- 0
data$CoverStoryManip <- as.numeric(data$CoverStoryManip)
###3 Randomized Items
data$Demand.ConsitMotif3 <- ifelse(data$Demand.ConsitMotif3 == 1, c("3"))
data$Demand.ConsitMotif3[is.na(data$Demand.ConsitMotif3)] <- 0
data$Demand.ConsitMotif3 <- as.numeric(data$Demand.ConsitMotif3)
###4 Filler Scales
data$Demand.ConsitMotif4 <- ifelse(data$Demand.ConsitMotif4 == 1, c("4"))
data$Demand.ConsitMotif4[is.na(data$Demand.ConsitMotif4)] <- 0
data$Demand.ConsitMotif4 <- as.numeric(data$Demand.ConsitMotif4)
###5 Randomized Scales
data$Demand.ConsitMotif2 <- ifelse(data$Demand.ConsitMotif2 == 1, c("5"))
data$Demand.ConsitMotif2[is.na(data$Demand.ConsitMotif2)] <- 0
data$Demand.ConsitMotif2 <- as.numeric(data$Demand.ConsitMotif2)
#Combine condition data into single variable column
data$Condition <- data$Demand.ConsitMotif1 + data$CoverStoryManip + data$Demand.ConsitMotif3 + data$Demand.ConsitMotif4 + data$Demand.ConsitMotif2
####Delete old condition data.
data <- subset(data, select=-c(Demand.ConsitMotif1, CoverStoryManip, Demand.ConsitMotif3, Demand.ConsitMotif4, Demand.ConsitMotif2))
#Apply value lables
data$Condition <- factor(data$Condition, levels = c(1,2,3,4,5), labels = c("Control", "CSManipulation","ItemRand","FillerScales","ScaleRand"))
###Combine all open-ended responses to the manipulation check for study purpose
data$PurpCk1 <- paste(data$PurpChk1a, data$PurpChk2a,data$PurpChk3a, data$PurpChk4a, data$PurpChk5a)
data$PurpCk2 <- paste(data$PurpChk1b, data$PurpChk2b,data$PurpChk3b, data$PurpChk4b, data$PurpChk5b) 
####Delete old purpose chk data. 
data <- subset(data, select=-c(PurpChk1a, PurpChk1b, PurpChk2a, PurpChk2b, PurpChk3a, PurpChk3b, PurpChk4a, PurpChk4b, PurpChk5a, PurpChk5b))
####Delte meaningless condition data. IC = Informed Consent. FOR = Frame of Reference.
data <- subset(data, select=-c(IC1, IC2, IC3, Q136, FOR1, FOR2, FOR4, FOR5, FORx))
#Move Finished to the end of the file.
moveMe <- function(data, tomove, where = "last", ba = NULL) {
  temp <- setdiff(names(data), tomove)
  x <- switch(
    where,
    first = data[c(tomove, temp)],
    last = data[c(temp, tomove)],
    before = {
      if (is.null(ba)) stop("must specify ba column")
      if (length(ba) > 1) stop("ba must be a single character string")
      data[append(temp, values = tomove, after = (match(ba, temp)-1))]
    },
    after = {
      if (is.null(ba)) stop("must specify ba column")
      if (length(ba) > 1) stop("ba must be a single character string")
      data[append(temp, values = tomove, after = (match(ba, temp)))]
    })
  x
}
data <- moveMe(data, c("Finished"),"after", "PurpCk2")
#Delete Prep and Text variables.
data <- data[c(-2,-3,-69,-135,-201,-268)]
#Consolidate item response measures
##Form Likert datasets, re-lable values, convert to numeric data, consolidate measures, and delete data.
agree <- data[c(2:41,67:106,132:171,198:209,235:302,329:335)]
agree <- ifelse(agree == "Strongly Disagree", 1, ifelse(agree == "Disagree", 2, ifelse(agree == "Neither Agree nor Disagree", 3, ifelse(agree == "Agree", 4, ifelse(agree == "Strongly Agree", 5,0)))))
write.csv(agree, file = "agree.csv")
agree <- read_csv("agree.csv")
#####Proactive Personality 
agree$PP1	<-	agree$PP1_1	+	agree$PP2_1	+	agree$PP5_1	+	agree$PP6_1	+	agree$PPx_1
agree$PP2	<-	agree$PP1_2	+	agree$PP2_2	+	agree$PP5_2	+	agree$PP6_2	+	agree$PPx_2
agree$PP3	<-	agree$PP1_3	+	agree$PP2_3	+	agree$PP5_3	+	agree$PP6_3	+	agree$PPx_3
agree$PP4	<-	agree$PP1_4	+	agree$PP2_4	+	agree$PP5_4	+	agree$PP6_4	+	agree$PPx_4
agree$PP5	<-	agree$PP1_5	+	agree$PP2_5	+	agree$PP5_5	+	agree$PP6_5	+	agree$PPx_5
agree$PP6	<-	agree$PP1_6	+	agree$PP2_6	+	agree$PP5_6	+	agree$PP6_6	+	agree$PPx_6
agree$PP7	<-	agree$PP1_7	+	agree$PP2_7	+	agree$PP5_7	+	agree$PP6_7	+	agree$PPx_7
agree$PP8	<-	agree$PP1_8	+	agree$PP2_8	+	agree$PP5_8	+	agree$PP6_8	+	agree$PPx_8
agree$PP9	<-	agree$PP1_9	+	agree$PP2_9	+	agree$PP5_9	+	agree$PP6_9	+	agree$PPx_9
agree$PP10	<-	agree$PP1_10	+	agree$PP2_10	+	agree$PP5_10	+	agree$PP6_10	+	agree$PPx_10
#####OCBI
agree$OCBI1	<-	agree$OCBI1_1	+	agree$OCBI2_1	+	agree$OCBI5_1	+	agree$OCBI6_1	+	agree$OCBIx_1
agree$OCBI2	<-	agree$OCBI1_3	+	agree$OCBI2_3	+	agree$OCBI5_3	+	agree$OCBI6_3	+	agree$OCBIx_3
agree$OCBI3	<-	agree$OCBI1_4	+	agree$OCBI2_4	+	agree$OCBI5_4	+	agree$OCBI6_4	+	agree$OCBIx_4
agree$OCBI4	<-	agree$OCBI1_5	+	agree$OCBI2_5	+	agree$OCBI5_5	+	agree$OCBI6_5	+	agree$OCBIx_5
agree$OCBI5	<-	agree$OCBI1_6	+	agree$OCBI2_6	+	agree$OCBI5_6	+	agree$OCBI6_6	+	agree$OCBIx_6
agree$OCBI6	<-	agree$OCBI1_7	+	agree$OCBI2_7	+	agree$OCBI5_7	+	agree$OCBI6_7	+	agree$OCBIx_7
agree$OCBI7	<-	agree$OCBI1_8	+	agree$OCBI2_8	+	agree$OCBI5_8	+	agree$OCBI6_8	+	agree$OCBIx_8
#####OCBO
agree$OCBO1	<-	agree$OCBO1_1	+	agree$OCBO2_1	+	agree$OCBO5_1	+	agree$OCBO6_1	+	agree$OCBOx_1
agree$OCBO2	<-	agree$OCBO1_2	+	agree$OCBO2_2	+	agree$OCBO5_2	+	agree$OCBO6_2	+	agree$OCBOx_2
agree$OCBO3	<-	agree$OCBO1_3	+	agree$OCBO2_3	+	agree$OCBO5_3	+	agree$OCBO6_3	+	agree$OCBOx_3
agree$OCBO4	<-	agree$OCBO1_5	+	agree$OCBO2_5	+	agree$OCBO5_5	+	agree$OCBO6_5	+	agree$OCBOx_5
agree$OCBO5	<-	agree$OCBO1_6	+	agree$OCBO2_6	+	agree$OCBO5_6	+	agree$OCBO6_6	+	agree$OCBOx_6
agree$OCBO6	<-	agree$OCBO1_7	+	agree$OCBO2_7	+	agree$OCBO5_7	+	agree$OCBO6_7	+	agree$OCBOx_7
agree$OCBO7	<-	agree$OCBO1_8	+	agree$OCBO2_8	+	agree$OCBO5_8	+	agree$OCBO6_8	+	agree$OCBOx_8
#####IRB
agree$IRB1	<-	agree$IRB1_1	+	agree$IRB2_1	+	agree$IRB5_1	+	agree$IRB6_1	+	agree$IRBx_1
agree$IRB2	<-	agree$IRB1_3	+	agree$IRB2_3	+	agree$IRB5_3	+	agree$IRB6_3	+	agree$IRBx_3
agree$IRB3	<-	agree$IRB1_4	+	agree$IRB2_4	+	agree$IRB5_4	+	agree$IRB6_4	+	agree$IRBx_4
agree$IRB4	<-	agree$IRB1_5	+	agree$IRB2_5	+	agree$IRB5_5	+	agree$IRB6_5	+	agree$IRBx_5
agree$IRB5	<-	agree$IRB1_6	+	agree$IRB2_6	+	agree$IRB5_6	+	agree$IRB6_6	+	agree$IRBx_6
agree$IRB6	<-	agree$IRB1_8	+	agree$IRB2_8	+	agree$IRB5_8	+	agree$IRB6_8	+	agree$IRBx_8
agree$IRB7	<-	agree$IRB1_9	+	agree$IRB2_9	+	agree$IRB5_9	+	agree$IRB6_9	+	agree$IRBx_9
#####Inattentive Responding
agree$IR	<-	agree$PP1_11	+	agree$PP2_11	+	agree$PP5_11	+	agree$PP6_11	+	agree$PPx_11
#####Consistency Motif
#Brave
agree$CM1	<-	agree$PP1_12	+	agree$PP2_12	+	agree$PP5_12	+	agree$PP6_12	+	agree$PPx_12
#Often feel blue.
agree$CM2	<-	agree$OCBI1_2	+	agree$OCBI2_2	+	agree$OCBI5_2	+	agree$OCBI6_2	+	agree$OCBIx_2
#Talkative
agree$CM3	<-	agree$OCBI1_9	+	agree$OCBI2_9	+	agree$OCBI5_9	+	agree$OCBI6_9	+	agree$OCBIx_9
#Pessimistic Person
agree$CM4	<-	agree$OCBO1_4	+	agree$OCBO2_4	+	agree$OCBO5_4	+	agree$OCBO6_4	+	agree$OCBOx_4
#Silent Person
agree$CM5	<-	agree$OCBO1_9	+	agree$OCBO2_9	+	agree$OCBO5_9	+	agree$OCBO6_9	+	agree$OCBOx_9
#Courageous
agree$CM6	<-	agree$IRB1_2	+	agree$IRB2_2	+	agree$IRB5_2	+	agree$IRB6_2	+	agree$IRBx_2
#Optimistic 
agree$CM7	<-	agree$IRB1_7	+	agree$IRB2_7	+	agree$IRB5_7	+	agree$IRB6_7	+	agree$IRBx_7
#Seldom feel blue.
agree$CM8	<-	agree$IRB1_10	+	agree$IRB2_10	+	agree$IRB5_10	+	agree$IRB6_10	+	agree$IRBx_10
#Consolidate "agree" dataset.
agree <- agree[c(202:248)]
#HALO
halo <- data[c(42:45,107:110,172:175,210:213,303:306)]
halo <- ifelse(halo == "Very bad", 1, ifelse(halo == "Bad", 2, ifelse(halo == "Somewhat bad", 3, ifelse(halo == "Neither good nor bad", 4, ifelse(halo == "Somewhat good", 5, ifelse(halo == "Good", 6, ifelse(halo == "Very good", 7, 0)))))))
write.csv(halo, file = "halo.csv")
halo <- read_csv("halo.csv")
halo$HALO1	<-	halo$HALO1_1	+	halo$HALO2_1	+	halo$HALO5_1	+	halo$HALO6_1 +	halo$HALOx_1
halo$HALO2	<-	halo$HALO1_2	+	halo$HALO2_2	+	halo$HALO5_2	+	halo$HALO6_2 +	halo$HALOx_2
halo$HALO3	<-	halo$HALO1_3	+	halo$HALO2_3	+	halo$HALO5_3	+	halo$HALO6_3 +	halo$HALOx_3
halo$HALO4	<-	halo$HALO1_4	+	halo$HALO2_4	+	halo$HALO5_4	+	halo$HALO6_4 +	halo$HALOx_4
#Consolidate "halo" dataset.
halo <- halo[c(22:25)]
#PANAS
PANAS <- data[c(46:65,111:130,176:195,214:233,307:326)]
PANAS <- ifelse(PANAS == "Very slightly or not at all", 1, ifelse(PANAS == "A little", 2, ifelse(PANAS == "Moderately", 3, ifelse(PANAS == "Quite a bit", 4, ifelse(PANAS == "Extremely", 5, 0)))))
write.csv(PANAS, file = "PANAS.csv")
PANAS <- read_csv("PANAS.csv")
PANAS$PAff1	<-	PANAS$PAff1_1	+	PANAS$PAff2_1	+	PANAS$PAff5_1	+	PANAS$PAff6_1	+	PANAS$PAffx_1
PANAS$PAff2	<-	PANAS$PAff1_2	+	PANAS$PAff2_2	+	PANAS$PAff5_2	+	PANAS$PAff6_2	+	PANAS$PAffx_2
PANAS$PAff3	<-	PANAS$PAff1_3	+	PANAS$PAff2_3	+	PANAS$PAff5_3	+	PANAS$PAff6_3	+	PANAS$PAffx_3
PANAS$PAff4	<-	PANAS$PAff1_4	+	PANAS$PAff2_4	+	PANAS$PAff5_4	+	PANAS$PAff6_4	+	PANAS$PAffx_4
PANAS$PAff5	<-	PANAS$PAff1_5	+	PANAS$PAff2_5	+	PANAS$PAff5_5	+	PANAS$PAff6_5	+	PANAS$PAffx_5
PANAS$PAff6	<-	PANAS$PAff1_6	+	PANAS$PAff2_6	+	PANAS$PAff5_6	+	PANAS$PAff6_6	+	PANAS$PAffx_6
PANAS$PAff7	<-	PANAS$PAff1_7	+	PANAS$PAff2_7	+	PANAS$PAff5_7	+	PANAS$PAff6_7	+	PANAS$PAffx_7
PANAS$PAff8	<-	PANAS$PAff1_8	+	PANAS$PAff2_8	+	PANAS$PAff5_8	+	PANAS$PAff6_8	+	PANAS$PAffx_8
PANAS$PAff9	<-	PANAS$PAff1_9	+	PANAS$PAff2_9	+	PANAS$PAff5_9	+	PANAS$PAff6_9	+	PANAS$PAffx_9
PANAS$PAff10	<-	PANAS$PAff1_10	+	PANAS$PAff2_10	+	PANAS$PAff5_10	+	PANAS$PAff6_10	+	PANAS$PAffx_10
PANAS$NAff1	<-	PANAS$NAff1_1	+	PANAS$NAff2_1	+	PANAS$NAff5_1	+	PANAS$NAff6_1	+	PANAS$NAffx_1
PANAS$NAff2	<-	PANAS$NAff1_2	+	PANAS$NAff2_2	+	PANAS$NAff5_2	+	PANAS$NAff6_2	+	PANAS$NAffx_2
PANAS$NAff3	<-	PANAS$NAff1_3	+	PANAS$NAff2_3	+	PANAS$NAff5_3	+	PANAS$NAff6_3	+	PANAS$NAffx_3
PANAS$NAff4	<-	PANAS$NAff1_4	+	PANAS$NAff2_4	+	PANAS$NAff5_4	+	PANAS$NAff6_4	+	PANAS$NAffx_4
PANAS$NAff5	<-	PANAS$NAff1_5	+	PANAS$NAff2_5	+	PANAS$NAff5_5	+	PANAS$NAff6_5	+	PANAS$NAffx_5
PANAS$NAff6	<-	PANAS$NAff1_6	+	PANAS$NAff2_6	+	PANAS$NAff5_6	+	PANAS$NAff6_6	+	PANAS$NAffx_6
PANAS$NAff7	<-	PANAS$NAff1_7	+	PANAS$NAff2_7	+	PANAS$NAff5_7	+	PANAS$NAff6_7	+	PANAS$NAffx_7
PANAS$NAff8	<-	PANAS$NAff1_8	+	PANAS$NAff2_8	+	PANAS$NAff5_8	+	PANAS$NAff6_8	+	PANAS$NAffx_8
PANAS$NAff9	<-	PANAS$NAff1_9	+	PANAS$NAff2_9	+	PANAS$NAff5_9	+	PANAS$NAff6_9	+	PANAS$NAffx_9
PANAS$NAff10	<-	PANAS$NAff1_10	+	PANAS$NAff2_10	+	PANAS$NAff5_10	+	PANAS$NAff6_10	+	PANAS$NAffx_10
#Consolidate "PANAS" dataset.
PANAS <- PANAS[c(102:121)]
#Consolidate manipulation checks. 
##Deleting likert/psychological data.
data <- subset(data,select=-c(2:327,329:335))
##Deleting other nuissance variables.
data <- subset(data,select=-c(2,13,23:36))
##Reorganize data.
#data <- data[,c(1,6:14, 15,16, 2, 3, 4, 5)]
#write.csv(data, file = "proximalCMV2.csv")
#Bind all datasets.
data <- cbind(data,agree,halo,PANAS)

#MTurk IGNORE THIS SEGMENT as MTurkR is now compromised.
#require("MTurkR")
#Link to MTurk (Work on MTurkR)
#Sys.setenv("AWS_ACCESS_KEY_ID" = "")
#Sys.setenv("AWS_SECRET_ACCESS_KEY" = "")
#Test connection to live server
#AccountBalance()
#SearchHITS
#hits <- SearchHITs()
#Buy more HITS. CAREFUL. 
#ExtendHIT(hit.type = "3SVZJ4A2P5CW4G2BZY7UE78GFL1IEV",add.assignments = 60, add.seconds = seconds(days=7))
#GetHITs and merge into MTurk dataframe. Use "SearchHITS()" to find the correct HIT. Make sure you have the right HITId. The followering are seprated out by batches.
#Go to MTurk account and manually download batch results. Place the batch in the working directory. 
a <- read_csv("Batch_2846969_batch_results.csv")
#Manually edit confirmation codes.
#a$Answer.surveycode[a$Answer.surveycode=="A11FRLH5KWRLBV"] <- "96090306"
#How long did the average participant take to complete the survey?
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
getmode(a$WorkTimeInSeconds)/60

#Create seperate datasets to identify possible those who did not complete the survey. Convert confirmation_code from character to numeric.
data.mt <- a[c(15,16,17,24,28)]
#Relabel variables to allow merging.
names(data.mt) <- c("AssignmentID","WorkerID","AssignmentStatus","Seconds","confirmation_code")
data.mt <- transform(data.mt, confirmation_code = as.numeric(confirmation_code))
#Ensure that paid participants' data are analyzed
data1 <- merge(data.mt, data, ID="confirmation_code",  all.x = TRUE)
#Acquire data from individuals who completed the survey in earnest. 
data1.1 <- merge(data.mt, data, ID="confirmation_code",  all.y = TRUE)
data1.1 <- data1.1[is.na(data1.1$WorkerID),]
rownames(data1.1) <- seq(length=nrow(data1.1)) 
#Subset in individuals providing a confirmation code that is not in our database. 
false.code <- data1[is.na(data1$Finished),]
false.code <- subset(false.code, AssignmentStatus=='Submitted')
#Subset in individuals with complete data and get rid of the workerIDs.
data2 <- merge(data.mt, data, ID="confirmation_code")
data2 <- rbind(data2,data1.1)
data <- data2[c(-2)]
#Subset into submitted, accepted, and rejected.
submitted <- data.mt[ which(data.mt$AssignmentStatus=='Submitted'), ]
approved <- data.mt[ which(data.mt$AssignmentStatus=='Approved'), ]
rejected <- data.mt[ which(data.mt$AssignmentStatus=='Rejected'), ]
#Build dataset
a <- a[c(28,24)]
names(a) <- c("confirmation_code", "WorkTimeInSeconds")
df <- merge(a,data,by="confirmation_code")
#Get RID of the worker IDS.
data <- data[c(-2)]

###### ENGAGE MACHINE LEARNING TO AUTOMATE MANIPULATION CHECK ######
#Engage supervised machine learning to analyze purpose check data, which will tell us if participants actually watched the video and got the purpose of the study. 
#library(caret)
#library(tm)
#library(SnowballC)
#library(stringr)
#Training data.NOTE: You'll need to filter in the 30 cases that force participants to distinguish purpose 1 from purpose 2. This will improve the accuracy of the algorithms.
#data.t <- data[1:60]
#data.t <- data.t[c(23,24,25,26)]
#data.t[] <- lapply(data.t, str_trim)
#is.na(data.t) <- data.t==''
#data.t <- na.omit(data.t)
#cond.code <- data.t[c(1)]
#data.t <- data.t[c(2)]
#corpus <- VCorpus(VectorSource(data.t$PurpCk1))
##Create a text document matrix.
#tdm <- DocumentTermMatrix(corpus, list(removePunctuation = TRUE, stripWhitespace = TRUE, content_transformer(tolower), stopwords = TRUE, stemming = FALSE, removeNumbers = FALSE))
##Convert to a data.frame for training and assign a classification (factor) to each document.
#train <- as.matrix(tdm)
#Create condition code that differentiates individuals from conditions.
#cond.code <- as.data.frame(ifelse(cond.code$Condition== "CSManipulation", 1,0))
#train <- cbind(train, cond.code)
#colnames(train)[ncol(train)] <- 'y'
#train <- as.data.frame(train)
#train$y <- as.factor(train$y)
##Train.
#require(foreach)
#registerDoSEQ()
#Training control prevents model over-fitting. 
#tc <- trainControl(method = "cv", number = 7, verboseIter=FALSE , #preProcOptions="pca", allowParallel=TRUE)
#bayesglm <- train(y ~ ., data = train, method = 'bayesglm', trControl=tc)
#rf <- train(y ~ ., data = train, method = 'rf', trControl=tc)
#NN <- train(y ~ ., data = train, method = 'nnet', trControl=tc, verbose=FALSE)
#svml <- train(y ~ ., data = train, method = 'svmLinear', trControl=tc)
#logitboost <- train(y ~ ., data = train, method = 'LogitBoost', trControl=tc)

#This is used to complare the models against one anoher.
#model <- c("Bayes GLM", "Neural Net", "SVM (linear)", "LogitBoost")
#Accuracy <- c(max(bayesglm$results$Accuracy),
#              max(NN$results$Accuracy),
#              max(svml$results$Accuracy),
#              max(logitboost$results$Accuracy))
#Kappa <- c(max(bayesglm$results$Kappa),
#           max(NN$results$Kappa),
#           max(svml$results$Kappa),
#           max(logitboost$results$Kappa))
#performance <- cbind(model,Accuracy,Kappa)
#knitr::kable(performance)
#Create new dataset and fit models to new data.
#data.n <- data[1:60]
#data.n <- data.n[c(24,25,26)]
#data.n <- rbind(data.t,data.n)
#data.n[] <- lapply(data.n, str_trim)
#is.na(data.n) <- data.n==''
#data.n <- na.omit(data.n)
#data.n <- unique(data.n)
#data.n <- data.n$PurpCk1
#data.n <- as.data.frame(data.n)
#Fit model to new data
#corpus <- VCorpus(VectorSource(data.n$data.n))
##Create a text document matrix.
#tdm <- DocumentTermMatrix(corpus, list(removePunctuation = TRUE, stripWhitespace = TRUE, content_transformer(tolower), stopwords = TRUE, stemming = FALSE, removeNumbers = FALSE))
#test <- as.matrix(tdm)
##Convert to a data.frame for training and assign a classification (factor) to each document.
#predict(svml,test)
###IT WORKS!!!! Send to team. 
#share <- subset(data, Condition=="CSManipulation")
#share <- share[c(24,25)]

#Assuming that some people correctly guessed the hypothesis, then engage supervised machine learning to analyze hypothesis guessing data. 
#data <- data[1:30,]
#data <- df$HypoGuessing2

######  DEVELOP SCALE SCORES AND EVALUATE USING CLASSICAL TEST THEORY ###### 
require(psych)
#see  the example including the bfi data set
keys.list <- list(PP=c("PP1","PP2","PP3","PP4","PP5","PP6","PP7","PP8","PP9","PP10"),OCBI=c("OCBI1","OCBI2","OCBI3","OCBI4","OCBI5","OCBI6","OCBI7"), OCBO=c("OCBO1","OCBO2","-OCBO3","-OCBO4","-OCBO5","OCBO6","OCBO7"), IRB=c("IRB1","IRB2","IRB3","IRB4","IRB5","-IRB6","-IRB7")) 
keys <- make.keys(df,keys.list)  #no longer necessary
scores <- scoreItems(keys,df,min=1,max=5)  #using a keys matrix 
#Create scale scores
scores <- as.data.frame(scores$scores)
#Bind scales with condition variables. 
final <- as.data.frame(df$Condition)
names(final) <- c("Condition")
final <- cbind(final,scores)

###Evaluate Scale Reliability. Code examining scale reliability is needed. Wayne, I can teach you how to do this. 
library(CTT)

######  TEST HYPOTHESES ###### 
require(BayesFactor)
plot(PP ~ Condition, data = final)
plot(OCBI ~ Condition, data = final)
plot(OCBO ~ Condition, data = final)
plot(IRB ~ Condition, data = final)

#Check the BayesFactor package to make sure that you are using the correct formula. 

#Proactive Personality 
describeBy(final$PP, final$Condition )
summary(aov(PP ~ Condition, data = final))
anovaBF(PP ~ Condition, data = final, progress=FALSE)
###Seems to favor null so far...

#OCBI
describeBy(final$OCBI, final$Condition )
summary(aov(OCBI ~ Condition, data = final))
anovaBF(OCBI ~ Condition, data = final, progress=FALSE)
###Seems to favor null so far...

#OCBO
describeBy(final$OCBO, final$Condition )
summary(aov(OCBO ~ Condition, data = final))
anovaBF(OCBO ~ Condition, data = final, progress=FALSE)

#IRB
describeBy(final$IRB, final$Condition )
summary(aov(IRB ~ Condition, data = final))
anovaBF(IRB ~ Condition, data = final, progress=FALSE)

######  Bayesian Measurement Invariance ######  
#The goal of the analysis is to examine the invariance (or lackthereof) of various invariant or non-invariant models. In other words, test for stricter and stricter forms of invariance. If survey design features are causing differences, then a non-invariant model will provide a better fit to the data than a model assuming invariance.
#require(blavaan)
#SubstantiveModel <- '
##Substantive Factors
##  PR =~ PP1+PP2+PP3+PP4+PP5+PP6+PP7+PP8+PP9+PP10
##  IRB =~ IRB1+IRB2+IRB3+IRB4+IRB5+IRB6+IRB7
##  OCBI =~ OCBI1+OCBI2+OCBI3+OCBI4+OCBI5+OCBI6+OCBI7
#  OCBO =~ OCBO1+OCBO2+OCBO3+OCBO4+OCBO5+OCBO6+OCBO7
#'
#bfit1 <- bcfa(SubstantiveModel, data = df, burnin = 4000, sample = 10000, jagcontrol = list(method = "rjparallel"))
#summary(bfit1, standardized = TRUE)
##Model is poor fit to the data.
#Equal loadings
#bfit2 <- bcfa(HS.model, data = HolzingerSwineford1939, group = "school", burnin = 1000, sample = 1000, group.equal = "loadings", jagcontrol = list(method = "rjparallel"))
#blavCompare(bfit1, bfit2)
##Conclusion: Configural invariance is 19.836 times more probable than configural non-invariance (though this model asssuming configural invariance is also, still, a bad model). 
#bfit3 <- bcfa(HS.model, data = HolzingerSwineford1939, group = "school", burnin = 1000, sample = 1000, group.equal = c("loadings","intercepts"), jagcontrol = list(method = "rjparallel"))
#blavCompare(bfit2, bfit3)
#fitmeasures(bfit3)
#summary(bfit1, standardized = TRUE)
#blavInspect(bfit1,"hpd")
##Plots:
#plot(bfit1, plot.type="trace")
#plot(bfit1, plot.type="autocorr")
#plot(bfit1, "histogram")

#write CSV file
write.csv(final, file = "data.csv")
```
```{r Bayesian Data Analysis}
#Note that the problem with this analysis is that it takes correlations at face value. You need to have a full causal model built out that explains why a correlation is to be taken at face value. 

#Notes below are the Bayesian test of the invariance of scale intercorrelations. 

#Frequentist estimate of correlation between PP and OCBI for across conditions.
f.Controlr1 <- cor.test( ~ PP + OCBI, data = final[final$Condition == "Control", ])
f.ItemRandr1 <- cor.test( ~ PP + OCBI, data = final[final$Condition == "ItemRand", ])
f.CSManpipr1 <- cor.test( ~ PP + OCBI, data = final[final$Condition == "CSManipulation", ])
f.FScalesr1 <- cor.test( ~ PP + OCBI, data = final[final$Condition == "FillerScales", ])
f.ScaleRandr1 <- cor.test( ~ PP + OCBI, data = final[final$Condition == "ScaleRand", ])

f.Controlr2 <- cor.test( ~ PP + OCBO, data = final[final$Condition == "Control", ])
f.ItemRandr2 <- cor.test( ~ PP + OCBO, data = final[final$Condition == "ItemRand", ])
f.CSManpipr2 <- cor.test( ~ PP + OCBO, data = final[final$Condition == "CSManipulation", ])
f.FScalesr2 <- cor.test( ~ PP + OCBO, data = final[final$Condition == "FillerScales", ])
f.ScaleRandr2 <- cor.test( ~ PP + OCBO, data = final[final$Condition == "ScaleRand", ])

f.Controlr3 <- cor.test( ~ PP + IRB, data = final[final$Condition == "Control", ])
f.ItemRandr3 <- cor.test( ~ PP + IRB, data = final[final$Condition == "ItemRand", ])
f.CSManpipr3 <- cor.test( ~ PP + IRB, data = final[final$Condition == "CSManipulation", ])
f.FScalesr3 <- cor.test( ~ PP + IRB, data = final[final$Condition == "FillerScales", ])
f.ScaleRandr3 <- cor.test( ~ PP + IRB, data = final[final$Condition == "ScaleRand", ])

#Examining correlations between variables across conditions.
#Extract sample sizes
c <- describe.by(final, group="Condition")
Controln <- c$Control$n[c(1)]
CSManipn <- c$CSManipulation$n[c(1)]
ItemRandn <- c$ItemRand$n[c(1)]
FScalesn <- c$FillerScales$n[c(1)]
ScaleRandn <- c$ScaleRand$n[c(1)]

#Extract correlations and compare
func1 <- function(xx1)
{
return(data.frame(COR = cor(xx1$a, xx1$b)))
}
##PP and OCBI
xx1 <- data.frame(group = final$Condition, a = final$PP, b = final$OCBI)
a <- ddply(xx1, .(group), func)

func <- function(xx2)
{
return(data.frame(COR = cor(xx2$a, xx2$b)))
}
##PP and OCBO
xx2 <- data.frame(group = final$Condition, a = final$PP, b = final$OCBO)
b <- ddply(xx2, .(group), func) 

func <- function(xx3)
{
return(data.frame(COR = cor(xx3$a, xx3$b)))
}
##PP and OCBI
xx3 <- data.frame(group = final$Condition, a = final$PP, b = final$IRB)
c<- ddply(xx3, .(group), func)

#Evaluate the difference in correlations using frequentist method: Fisher r to z transformation and test of significant differences. Note: these tests are conducted singly as in the test compares only two conditions at a time. It is possible to run an omnibus test that examines whether a correlation varies across all groups. However, the code for this test would need to be written. No big deal, but an easy alternative would be to correct for the number of tests that we run, which may be too conservative. See http://davidakenny.net/doc/statbook/chapter_16.pdf for more information. 

##1. PP and OCBI: Control vs. CSManipulation
Control.PP.OCBI.r <- a[1,]
Control.PP.OCBI.r <- Control.PP.OCBI.r[2]
CSManip.PP.OCBI.r <- a[2,]
CSManip.PP.OCBI.r <- CSManip.PP.OCBI.r[2]
cocor.indep.groups(as.numeric(Control.PP.OCBI.r), as.numeric(CSManip.PP.OCBI.r), Controln, CSManipn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis retained using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results.

##2. PP and OCBI: Control vs. ItemRand
ItemRand.PP.OCBI.r <- a[3,]
ItemRand.PP.OCBI.r <- ItemRand.PP.OCBI.r[2]
cocor.indep.groups(as.numeric(Control.PP.OCBI.r), as.numeric(ItemRand.PP.OCBI.r), Controln, ItemRandn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis retained using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results.

##3. PP and OCBI: Control vs. FScales
FScales.PP.OCBI.r <- a[4,]
FScales.PP.OCBI.r <- FScales.PP.OCBI.r[2]
cocor.indep.groups(as.numeric(Control.PP.OCBI.r), as.numeric(FScales.PP.OCBI.r), Controln, FScalesn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis rejected using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results.

##4. PP and OCBI: Control vs. ScaleRand
ScaleRand.PP.OCBI.r <- a[5,]
ScaleRand.PP.OCBI.r <- ScaleRand.PP.OCBI.r[2]
cocor.indep.groups(as.numeric(Control.PP.OCBI.r), as.numeric(ScaleRand.PP.OCBI.r), Controln, ScaleRandn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis retained using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results.

##1. PP and OCBO: Control vs. CSManipulation
Control.PP.OCBO.r <- a[1,]
Control.PP.OCBO.r <- Control.PP.OCBO.r[2]
CSManip.PP.OCBO.r <- a[2,]
CSManip.PP.OCBO.r <- CSManip.PP.OCBO.r[2]
cocor.indep.groups(as.numeric(Control.PP.OCBO.r), as.numeric(CSManip.PP.OCBO.r), Controln, CSManipn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis retained using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results. (update)

##2. PP and OCBO: Control vs. ItemRand
ItemRand.PP.OCBO.r <- a[3,]
ItemRand.PP.OCBO.r <- ItemRand.PP.OCBO.r[2]
cocor.indep.groups(as.numeric(Control.PP.OCBO.r), as.numeric(ItemRand.PP.OCBO.r), Controln, ItemRandn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis retained using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results. (update)

##3. PP and OCBO: Control vs. FScales
FScales.PP.OCBO.r <- a[4,]
FScales.PP.OCBO.r <- FScales.PP.OCBO.r[2]
cocor.indep.groups(as.numeric(Control.PP.OCBO.r), as.numeric(FScales.PP.OCBO.r), Controln, FScalesn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis rejected using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results. (update)

##4. PP and OCBO: Control vs. ScaleRand
ScaleRand.PP.OCBO.r <- a[5,]
ScaleRand.PP.OCBO.r <- ScaleRand.PP.OCBO.r[2]
cocor.indep.groups(as.numeric(Control.PP.OCBO.r), as.numeric(ScaleRand.PP.OCBO.r), Controln, ScaleRandn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis retained using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results. (update)

##1. PP and IRB: Control vs. CSManipulation
Control.PP.IRB.r <- a[1,]
Control.PP.IRB.r <- Control.PP.IRB.r[2]
CSManip.PP.IRB.r <- a[2,]
CSManip.PP.IRB.r <- CSManip.PP.IRB.r[2]
cocor.indep.groups(as.numeric(Control.PP.IRB.r), as.numeric(CSManip.PP.IRB.r), Controln, CSManipn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis retained using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results. (update)

##2. PP and IRB: Control vs. ItemRand
ItemRand.PP.IRB.r <- a[3,]
ItemRand.PP.IRB.r <- ItemRand.PP.IRB.r[2]
cocor.indep.groups(as.numeric(Control.PP.IRB.r), as.numeric(ItemRand.PP.IRB.r), Controln, ItemRandn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis retained using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results. (update)

##3. PP and IRB: Control vs. FScales
FScales.PP.IRB.r <- a[4,]
FScales.PP.IRB.r <- FScales.PP.IRB.r[2]
cocor.indep.groups(as.numeric(Control.PP.IRB.r), as.numeric(FScales.PP.IRB.r), Controln, FScalesn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis rejected using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results. (update)

##4. PP and IRB: Control vs. ScaleRand
ScaleRand.PP.IRB.r <- a[5,]
ScaleRand.PP.IRB.r <- ScaleRand.PP.IRB.r[2]
cocor.indep.groups(as.numeric(Control.PP.IRB.r), as.numeric(ScaleRand.PP.IRB.r), Controln, ScaleRandn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis retained using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results. (update)


####Note for Wayne: Text above should be replicated for the other intercorrelations other than PP-OCBI (e.g., PP-OCBO, PP-IRB).

##Praoctive Personality and OCBO
xx <- data.frame(group = final$Condition, a = final$PP, b = final$OCBO)
a <- ddply(xx, .(group), func)

##Praoctive Personality and IRB
xx <- data.frame(group = final$Condition, a = final$PP, b = final$IRB)
a <- ddply(xx, .(group), func)

##OCBI and OCBO
xx <- data.frame(group = final$Condition, a = final$OCBI, b = final$OCBO)
a <- ddply(xx, .(group), func)

##OCBI and IRB
xx <- data.frame(group = final$Condition, a = final$OCBI, b = final$IRB)
a <- ddply(xx, .(group), func)

##OCBO and IRB
xx <- data.frame(group = final$Condition, a = final$OCBO, b = final$IRB)
a <- ddply(xx, .(group), func)

#Bayesian estimate of correlation between PP and OCBI for across conditions.
b.Controlr1 <- bayes.cor.test( ~ PP + OCBI, data = final[final$Condition == "Control", ])
b.ItemRandr1 <- bayes.cor.test( ~ PP + OCBI, data = final[final$Condition == "ItemRand", ])
b.CSManpipr1 <- bayes.cor.test( ~ PP + OCBI, data = final[final$Condition == "CSManipulation", ])
b.FScalesr1 <- bayes.cor.test( ~ PP + OCBI, data = final[final$Condition == "FillerScales", ])
b.ScaleRandr1 <- bayes.cor.test( ~ PP + OCBI, data = final[final$Condition == "ScaleRand", ])

b.Controlr2 <- bayes.cor.test( ~ PP + OCBO, data = final[final$Condition == "Control", ])
b.ItemRandr2 <- bayes.cor.test( ~ PP + OCBO, data = final[final$Condition == "ItemRand", ])
b.CSManpipr2 <- bayes.cor.test( ~ PP + OCBO, data = final[final$Condition == "CSManipulation", ])
b.FScalesr2 <- bayes.cor.test( ~ PP + OCBO, data = final[final$Condition == "FillerScales", ])
b.ScaleRandr2 <- bayes.cor.test( ~ PP + OCBO, data = final[final$Condition == "ScaleRand", ])

b.Controlr3 <- bayes.cor.test( ~ PP + IRB, data = final[final$Condition == "Control", ])
b.ItemRandr3 <- bayes.cor.test( ~ PP + IRB, data = final[final$Condition == "ItemRand", ])
b.CSManpipr3 <- bayes.cor.test( ~ PP + IRB, data = final[final$Condition == "CSManipulation", ])
b.FScalesr3 <- bayes.cor.test( ~ PP + IRB, data = final[final$Condition == "FillerScales", ])
b.ScaleRandr3 <- bayes.cor.test( ~ PP + IRB, data = final[final$Condition == "ScaleRand", ])

#Examine fit of the models
plot(b.Controlr1)
plot(b.ItemRandr1)
plot(b.CSManpipr1)
plot(b.FScalesr1)
plot(b.ScaleRandr1)

plot(b.Controlr2)
plot(b.ItemRandr2)
plot(b.CSManpipr2)
plot(b.FScalesr2)
plot(b.ScaleRandr2)

plot(b.Controlr3)
plot(b.ItemRandr3)
plot(b.CSManpipr3)
plot(b.FScalesr3)
plot(b.ScaleRandr3)

#Plot differences for PP-OCBI relationship and estimate proportion of effects that are consistent with a null effect, which here is defined as r = [-.10, .10].
#Item Randomization
control_mcmc <- as.data.frame(b.Controlr1)
ItemRandr1_mcmc <- as.data.frame(b.ItemRandr1)
hist(ItemRandr1_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-OCBI r Between Control and Item-Randomized Scales", yaxt = "n")
1-(mean(ItemRandr1_mcmc$rho - control_mcmc$rho > .1 )+mean(ItemRandr1_mcmc$rho - control_mcmc$rho < -.1 ))
#48% chance that filler scales have no effect defined by r = [-.10, .10]. 

control_mcmc <- as.data.frame(b.Controlr2)
ItemRandr2_mcmc <- as.data.frame(b.ItemRandr2)
hist(ItemRandr2_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-OCBO r Between Control and Item-Randomized Scales", yaxt = "n")
1-(mean(ItemRandr2_mcmc$rho - control_mcmc$rho > .1 )+mean(ItemRandr2_mcmc$rho - control_mcmc$rho < -.1 ))
#48% chance that filler scales have no effect defined by r = [-.10, .10]. (update)

control_mcmc <- as.data.frame(b.Controlr3)
ItemRandr3_mcmc <- as.data.frame(b.ItemRandr3)
hist(ItemRandr3_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-IRB r Between Control and Item-Randomized Scales", yaxt = "n")
1-(mean(ItemRandr3_mcmc$rho - control_mcmc$rho > .1 )+mean(ItemRandr3_mcmc$rho - control_mcmc$rho < -.1 ))
#48% chance that filler scales have no effect defined by r = [-.10, .10]. (update)

#Cover Story Manipulation
control_mcmc <- as.data.frame(b.Controlr1)
CSManpipr1_mcmc <- as.data.frame(b.CSManpipr1)
hist(CSManpipr1_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-OCBI r Between Control and Coverstory", yaxt = "n")
1-(mean(CSManpipr1_mcmc$rho - control_mcmc$rho > .1 )+mean(CSManpipr1_mcmc$rho - control_mcmc$rho < -.1 ))
#51% chance that filler scales have no effect defined by r = [-.10, .10]. 

control_mcmc <- as.data.frame(b.Controlr2)
CSManpipr2_mcmc <- as.data.frame(b.CSManpipr2)
hist(CSManpipr2_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-OCBO r Between Control and Coverstory", yaxt = "n")
1-(mean(CSManpipr2_mcmc$rho - control_mcmc$rho > .1 )+mean(CSManpipr2_mcmc$rho - control_mcmc$rho < -.1 ))
#51% chance that filler scales have no effect defined by r = [-.10, .10]. (update)

control_mcmc <- as.data.frame(b.Controlr3)
CSManpipr3_mcmc <- as.data.frame(b.CSManpipr3)
hist(CSManpipr3_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-IRB r Between Control and Coverstory", yaxt = "n")
1-(mean(CSManpipr3_mcmc$rho - control_mcmc$rho > .1 )+mean(CSManpipr3_mcmc$rho - control_mcmc$rho < -.1 ))
#51% chance that filler scales have no effect defined by r = [-.10, .10]. (update)

#Large difference between control and filler scales. Examine the probability that filler scales weaken correlation.
control_mcmc <- as.data.frame(b.Controlr1)
Fscales1_mcmc <- as.data.frame(b.FScalesr1)
head(control_mcmc)
hist(Fscales1_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-OCBI r Between Control and Filler Scales", yaxt = "n")
1-(mean(Fscales1_mcmc$rho - control_mcmc$rho > .1 )+mean(Fscales1_mcmc$rho - control_mcmc$rho < -.1 ))
#12% chance that filler scales have no effect defined by r = [-.10, .10]. 

control_mcmc <- as.data.frame(b.Controlr2)
Fscales2_mcmc <- as.data.frame(b.FScalesr2)
head(control_mcmc)
hist(Fscales2_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-OCBO r Between Control and Filler Scales", yaxt = "n")
1-(mean(Fscales2_mcmc$rho - control_mcmc$rho > .1 )+mean(Fscales2_mcmc$rho - control_mcmc$rho < -.1 ))
#12% chance that filler scales have no effect defined by r = [-.10, .10]. (update)

control_mcmc <- as.data.frame(b.Controlr3)
Fscales3_mcmc <- as.data.frame(b.FScalesr3)
head(control_mcmc)
hist(Fscales3_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-IRB r Between Control and Filler Scales", yaxt = "n")
1-(mean(Fscales3_mcmc$rho - control_mcmc$rho > .1 )+mean(Fscales3_mcmc$rho - control_mcmc$rho < -.1 ))
#12% chance that filler scales have no effect defined by r = [-.10, .10]. (update)

#Scale Randomization
control_mcmc <- as.data.frame(b.Controlr1)
ScaleRandr1_mcmc <- as.data.frame(b.ScaleRandr1)
hist(ScaleRandr1_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-OCBI r Between Control and Scale-Randomized Scales", yaxt = "n")
1-(mean(ScaleRandr1_mcmc$rho - control_mcmc$rho > .1 )+mean(ScaleRandr1_mcmc$rho - control_mcmc$rho < -.1 ))
#40% chance that filler scales have no effect defined by r = [-.10, .10]. 

control_mcmc <- as.data.frame(b.Controlr2)
ScaleRandr2_mcmc <- as.data.frame(b.ScaleRandr2)
hist(ScaleRandr2_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-OCBO r Between Control and Scale-Randomized Scales", yaxt = "n")
1-(mean(ScaleRandr2_mcmc$rho - control_mcmc$rho > .1 )+mean(ScaleRandr2_mcmc$rho - control_mcmc$rho < -.1 ))
#40% chance that filler scales have no effect defined by r = [-.10, .10]. (update)

control_mcmc <- as.data.frame(b.Controlr3)
ScaleRandr3_mcmc <- as.data.frame(b.ScaleRandr3)
hist(ScaleRandr3_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-IRB r Between Control and Scale-Randomized Scales", yaxt = "n")
1-(mean(ScaleRandr3_mcmc$rho - control_mcmc$rho > .1 )+mean(ScaleRandr3_mcmc$rho - control_mcmc$rho < -.1 ))
#40% chance that filler scales have no effect defined by r = [-.10, .10]. (update)

##Note for Wayne: Text above should be replicated for the other intercorrelations other than PP-OCBI (e.g., PP-OCBO, PP-IRB, etc.).
```

Notes below are the Bayesian tests of scale intercorrelations. 
```{r}
library(bayesboot)
library(BayesFactor)
library(BayesMed)
library(plyr)
library(psych)
library(cocor)
library(BayesianFirstAid)
library(ggplot2)
library(QRM)

#Frequentist estimate of correlation between PP and OCBI for across conditions.
f.Controlr1 <- cor.test( ~ PP + OCBI, data = final[final$Condition == "Control", ])
f.ItemRandr1 <- cor.test( ~ PP + OCBI, data = final[final$Condition == "ItemRand", ])
f.CSManpipr1 <- cor.test( ~ PP + OCBI, data = final[final$Condition == "CSManipulation", ])
f.FScalesr1 <- cor.test( ~ PP + OCBI, data = final[final$Condition == "FillerScales", ])
f.ScaleRandr1 <- cor.test( ~ PP + OCBI, data = final[final$Condition == "ScaleRand", ])

f.Controlr2 <- cor.test( ~ PP + OCBO, data = final[final$Condition == "Control", ])
f.ItemRandr2 <- cor.test( ~ PP + OCBO, data = final[final$Condition == "ItemRand", ])
f.CSManpipr2 <- cor.test( ~ PP + OCBO, data = final[final$Condition == "CSManipulation", ])
f.FScalesr2 <- cor.test( ~ PP + OCBO, data = final[final$Condition == "FillerScales", ])
f.ScaleRandr2 <- cor.test( ~ PP + OCBO, data = final[final$Condition == "ScaleRand", ])

f.Controlr3 <- cor.test( ~ PP + IRB, data = final[final$Condition == "Control", ])
f.ItemRandr3 <- cor.test( ~ PP + IRB, data = final[final$Condition == "ItemRand", ])
f.CSManpipr3 <- cor.test( ~ PP + IRB, data = final[final$Condition == "CSManipulation", ])
f.FScalesr3 <- cor.test( ~ PP + IRB, data = final[final$Condition == "FillerScales", ])
f.ScaleRandr3 <- cor.test( ~ PP + IRB, data = final[final$Condition == "ScaleRand", ])

#Examining correlations between variables across conditions.
#Extract sample sizes
c <- describe.by(final, group="Condition")
Controln <- c$Control$n[c(1)]
CSManipn <- c$CSManipulation$n[c(1)]
ItemRandn <- c$ItemRand$n[c(1)]
FScalesn <- c$FillerScales$n[c(1)]
ScaleRandn <- c$ScaleRand$n[c(1)]

#Extract correlations and compare
func <- function(xx1)
{
return(data.frame(COR = cor(xx1$a, xx1$b)))
}
##PP and OCBI
xx1 <- data.frame(group = final$Condition, a = final$PP, b = final$OCBI)
a <- ddply(xx1, .(group), func)

func <- function(xx2)
{
return(data.frame(COR = cor(xx2$a, xx2$b)))
}
##PP and OCBO
xx2 <- data.frame(group = final$Condition, a = final$PP, b = final$OCBO)
b <- ddply(xx2, .(group), func) 

func <- function(xx3)
{
return(data.frame(COR = cor(xx3$a, xx3$b)))
}
##PP and OCBI
xx3 <- data.frame(group = final$Condition, a = final$PP, b = final$IRB)
c<- ddply(xx3, .(group), func)

#Evaluate the difference in correlations using frequentist method: Fisher r to z transformation and test of significant differences. Note: these tests are conducted singly as in the test compares only two conditions at a time. It is possible to run an omnibus test that examines whether a correlation varies across all groups. However, the code for this test would need to be written. No big deal, but an easy alternative would be to correct for the number of tests that we run, which may be too conservative. See http://davidakenny.net/doc/statbook/chapter_16.pdf for more information. 

##1. PP and OCBI: Control vs. CSManipulation
Control.PP.OCBI.r <- a[1,]
Control.PP.OCBI.r <- Control.PP.OCBI.r[2]
CSManip.PP.OCBI.r <- a[2,]
CSManip.PP.OCBI.r <- CSManip.PP.OCBI.r[2]
cocor.indep.groups(as.numeric(Control.PP.OCBI.r), as.numeric(CSManip.PP.OCBI.r), Controln, CSManipn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis retained using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results.

##2. PP and OCBI: Control vs. ItemRand
ItemRand.PP.OCBI.r <- a[3,]
ItemRand.PP.OCBI.r <- ItemRand.PP.OCBI.r[2]
cocor.indep.groups(as.numeric(Control.PP.OCBI.r), as.numeric(ItemRand.PP.OCBI.r), Controln, ItemRandn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis retained using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results.

##3. PP and OCBI: Control vs. FScales
FScales.PP.OCBI.r <- a[4,]
FScales.PP.OCBI.r <- FScales.PP.OCBI.r[2]
cocor.indep.groups(as.numeric(Control.PP.OCBI.r), as.numeric(FScales.PP.OCBI.r), Controln, FScalesn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis rejected using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results.

##4. PP and OCBI: Control vs. ScaleRand
ScaleRand.PP.OCBI.r <- a[5,]
ScaleRand.PP.OCBI.r <- ScaleRand.PP.OCBI.r[2]
cocor.indep.groups(as.numeric(Control.PP.OCBI.r), as.numeric(ScaleRand.PP.OCBI.r), Controln, ScaleRandn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis retained using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results.

##1. PP and OCBO: Control vs. CSManipulation
Control.PP.OCBO.r <- a[1,]
Control.PP.OCBO.r <- Control.PP.OCBO.r[2]
CSManip.PP.OCBO.r <- a[2,]
CSManip.PP.OCBO.r <- CSManip.PP.OCBO.r[2]
cocor.indep.groups(as.numeric(Control.PP.OCBO.r), as.numeric(CSManip.PP.OCBO.r), Controln, CSManipn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis retained using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results. (update)

##2. PP and OCBO: Control vs. ItemRand
ItemRand.PP.OCBO.r <- a[3,]
ItemRand.PP.OCBO.r <- ItemRand.PP.OCBO.r[2]
cocor.indep.groups(as.numeric(Control.PP.OCBO.r), as.numeric(ItemRand.PP.OCBO.r), Controln, ItemRandn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis retained using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results. (update)

##3. PP and OCBO: Control vs. FScales
FScales.PP.OCBO.r <- a[4,]
FScales.PP.OCBO.r <- FScales.PP.OCBO.r[2]
cocor.indep.groups(as.numeric(Control.PP.OCBO.r), as.numeric(FScales.PP.OCBO.r), Controln, FScalesn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis rejected using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results. (update)

##4. PP and OCBO: Control vs. ScaleRand
ScaleRand.PP.OCBO.r <- a[5,]
ScaleRand.PP.OCBO.r <- ScaleRand.PP.OCBO.r[2]
cocor.indep.groups(as.numeric(Control.PP.OCBO.r), as.numeric(ScaleRand.PP.OCBO.r), Controln, ScaleRandn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis retained using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results. (update)

##1. PP and IRB: Control vs. CSManipulation
Control.PP.IRB.r <- a[1,]
Control.PP.IRB.r <- Control.PP.IRB.r[2]
CSManip.PP.IRB.r <- a[2,]
CSManip.PP.IRB.r <- CSManip.PP.IRB.r[2]
cocor.indep.groups(as.numeric(Control.PP.IRB.r), as.numeric(CSManip.PP.IRB.r), Controln, CSManipn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis retained using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results. (update)

##2. PP and IRB: Control vs. ItemRand
ItemRand.PP.IRB.r <- a[3,]
ItemRand.PP.IRB.r <- ItemRand.PP.IRB.r[2]
cocor.indep.groups(as.numeric(Control.PP.IRB.r), as.numeric(ItemRand.PP.IRB.r), Controln, ItemRandn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis retained using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results. (update)

##3. PP and IRB: Control vs. FScales
FScales.PP.IRB.r <- a[4,]
FScales.PP.IRB.r <- FScales.PP.IRB.r[2]
cocor.indep.groups(as.numeric(Control.PP.IRB.r), as.numeric(FScales.PP.IRB.r), Controln, FScalesn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis rejected using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results. (update)

##4. PP and IRB: Control vs. ScaleRand
ScaleRand.PP.IRB.r <- a[5,]
ScaleRand.PP.IRB.r <- ScaleRand.PP.IRB.r[2]
cocor.indep.groups(as.numeric(Control.PP.IRB.r), as.numeric(ScaleRand.PP.IRB.r), Controln, ScaleRandn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis retained using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results. (update)


####Note for Wayne: Text above should be replicated for the other intercorrelations other than PP-OCBI (e.g., PP-OCBO, PP-IRB).

##Praoctive Personality and OCBO
xx <- data.frame(group = final$Condition, a = final$PP, b = final$OCBO)
a <- ddply(xx, .(group), func)

##Praoctive Personality and IRB
xx <- data.frame(group = final$Condition, a = final$PP, b = final$IRB)
a <- ddply(xx, .(group), func)

##OCBI and OCBO
xx <- data.frame(group = final$Condition, a = final$OCBI, b = final$OCBO)
a <- ddply(xx, .(group), func)

##OCBI and IRB
xx <- data.frame(group = final$Condition, a = final$OCBI, b = final$IRB)
a <- ddply(xx, .(group), func)

##OCBO and IRB
xx <- data.frame(group = final$Condition, a = final$OCBO, b = final$IRB)
a <- ddply(xx, .(group), func)

#Bayesian estimate of correlation between PP and OCBI for across conditions.
b.Controlr1 <- bayes.cor.test( ~ PP + OCBI, data = final[final$Condition == "Control", ])
b.ItemRandr1 <- bayes.cor.test( ~ PP + OCBI, data = final[final$Condition == "ItemRand", ])
b.CSManpipr1 <- bayes.cor.test( ~ PP + OCBI, data = final[final$Condition == "CSManipulation", ])
b.FScalesr1 <- bayes.cor.test( ~ PP + OCBI, data = final[final$Condition == "FillerScales", ])
b.ScaleRandr1 <- bayes.cor.test( ~ PP + OCBI, data = final[final$Condition == "ScaleRand", ])

#Bayesian estimate of correlation between PP and OCBO for across conditions.
b.Controlr2 <- bayes.cor.test( ~ PP + OCBO, data = final[final$Condition == "Control", ])
b.ItemRandr2 <- bayes.cor.test( ~ PP + OCBO, data = final[final$Condition == "ItemRand", ])
b.CSManpipr2 <- bayes.cor.test( ~ PP + OCBO, data = final[final$Condition == "CSManipulation", ])
b.FScalesr2 <- bayes.cor.test( ~ PP + OCBO, data = final[final$Condition == "FillerScales", ])
b.ScaleRandr2 <- bayes.cor.test( ~ PP + OCBO, data = final[final$Condition == "ScaleRand", ])

#Bayesian estimate of correlation between PP and IRB for across conditions.
b.Controlr3 <- bayes.cor.test( ~ PP + IRB, data = final[final$Condition == "Control", ])
b.ItemRandr3 <- bayes.cor.test( ~ PP + IRB, data = final[final$Condition == "ItemRand", ])
b.CSManpipr3 <- bayes.cor.test( ~ PP + IRB, data = final[final$Condition == "CSManipulation", ])
b.FScalesr3 <- bayes.cor.test( ~ PP + IRB, data = final[final$Condition == "FillerScales", ])
b.ScaleRandr3 <- bayes.cor.test( ~ PP + IRB, data = final[final$Condition == "ScaleRand", ])

#Examine fit of the models for PP and OCBI
plot(b.Controlr1)
plot(b.ItemRandr1)
plot(b.CSManpipr1)
plot(b.FScalesr1)
plot(b.ScaleRandr1)

#Examine fit of the models for PP and OCBO
plot(b.Controlr2)
plot(b.ItemRandr2)
plot(b.CSManpipr2)
plot(b.FScalesr2)
plot(b.ScaleRandr2)

#Examine fit of the models for PP and IRB
plot(b.Controlr3)
plot(b.ItemRandr3)
plot(b.CSManpipr3)
plot(b.FScalesr3)
plot(b.ScaleRandr3)

#Plot differences for PP-OCBI relationship and estimate proportion of effects that are consistent with a null effect, which here is defined as r = [-.10, .10].
#Item Randomization
control_mcmc <- as.data.frame(b.Controlr1)
ItemRandr1_mcmc <- as.data.frame(b.ItemRandr1)
hist(ItemRandr1_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-OCBI r Between Control and Item-Randomized Scales", yaxt = "n")
1-(mean(ItemRandr1_mcmc$rho - control_mcmc$rho > .1 )+mean(ItemRandr1_mcmc$rho - control_mcmc$rho < -.1 ))
#48% chance that filler scales have no effect defined by r = [-.10, .10]. 

control_mcmc <- as.data.frame(b.Controlr2)
ItemRandr2_mcmc <- as.data.frame(b.ItemRandr2)
hist(ItemRandr2_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-OCBO r Between Control and Item-Randomized Scales", yaxt = "n")
1-(mean(ItemRandr2_mcmc$rho - control_mcmc$rho > .1 )+mean(ItemRandr2_mcmc$rho - control_mcmc$rho < -.1 ))
#48% chance that filler scales have no effect defined by r = [-.10, .10]. (update)

control_mcmc <- as.data.frame(b.Controlr3)
ItemRandr3_mcmc <- as.data.frame(b.ItemRandr3)
hist(ItemRandr3_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-IRB r Between Control and Item-Randomized Scales", yaxt = "n")
1-(mean(ItemRandr3_mcmc$rho - control_mcmc$rho > .1 )+mean(ItemRandr3_mcmc$rho - control_mcmc$rho < -.1 ))
#48% chance that filler scales have no effect defined by r = [-.10, .10]. (update)

#Cover Story Manipulation
control_mcmc <- as.data.frame(b.Controlr1)
CSManpipr1_mcmc <- as.data.frame(b.CSManpipr1)
hist(CSManpipr1_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-OCBI r Between Control and Coverstory", yaxt = "n")
1-(mean(CSManpipr1_mcmc$rho - control_mcmc$rho > .1 )+mean(CSManpipr1_mcmc$rho - control_mcmc$rho < -.1 ))
#51% chance that filler scales have no effect defined by r = [-.10, .10]. 

control_mcmc <- as.data.frame(b.Controlr2)
CSManpipr2_mcmc <- as.data.frame(b.CSManpipr2)
hist(CSManpipr2_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-OCBO r Between Control and Coverstory", yaxt = "n")
1-(mean(CSManpipr2_mcmc$rho - control_mcmc$rho > .1 )+mean(CSManpipr2_mcmc$rho - control_mcmc$rho < -.1 ))
#51% chance that filler scales have no effect defined by r = [-.10, .10]. (update)

control_mcmc <- as.data.frame(b.Controlr3)
CSManpipr3_mcmc <- as.data.frame(b.CSManpipr3)
hist(CSManpipr3_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-IRB r Between Control and Coverstory", yaxt = "n")
1-(mean(CSManpipr3_mcmc$rho - control_mcmc$rho > .1 )+mean(CSManpipr3_mcmc$rho - control_mcmc$rho < -.1 ))
#51% chance that filler scales have no effect defined by r = [-.10, .10]. (update)

#Large difference between control and filler scales. Examine the probability that filler scales weaken correlation.
control_mcmc <- as.data.frame(b.Controlr1)
Fscales1_mcmc <- as.data.frame(b.FScalesr1)
head(control_mcmc)
hist(Fscales1_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-OCBI r Between Control and Filler Scales", yaxt = "n")
1-(mean(Fscales1_mcmc$rho - control_mcmc$rho > .1 )+mean(Fscales1_mcmc$rho - control_mcmc$rho < -.1 ))
#12% chance that filler scales have no effect defined by r = [-.10, .10]. 

control_mcmc <- as.data.frame(b.Controlr2)
Fscales2_mcmc <- as.data.frame(b.FScalesr2)
head(control_mcmc)
hist(Fscales2_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-OCBO r Between Control and Filler Scales", yaxt = "n")
1-(mean(Fscales2_mcmc$rho - control_mcmc$rho > .1 )+mean(Fscales2_mcmc$rho - control_mcmc$rho < -.1 ))
#12% chance that filler scales have no effect defined by r = [-.10, .10]. (update)

control_mcmc <- as.data.frame(b.Controlr3)
Fscales3_mcmc <- as.data.frame(b.FScalesr3)
head(control_mcmc)
hist(Fscales3_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-IRB r Between Control and Filler Scales", yaxt = "n")
1-(mean(Fscales3_mcmc$rho - control_mcmc$rho > .1 )+mean(Fscales3_mcmc$rho - control_mcmc$rho < -.1 ))
#12% chance that filler scales have no effect defined by r = [-.10, .10]. (update)

#Scale Randomization
control_mcmc <- as.data.frame(b.Controlr1)
ScaleRandr1_mcmc <- as.data.frame(b.ScaleRandr1)
hist(ScaleRandr1_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-OCBI r Between Control and Scale-Randomized Scales", yaxt = "n")
1-(mean(ScaleRandr1_mcmc$rho - control_mcmc$rho > .1 )+mean(ScaleRandr1_mcmc$rho - control_mcmc$rho < -.1 ))
#40% chance that filler scales have no effect defined by r = [-.10, .10]. 

control_mcmc <- as.data.frame(b.Controlr2)
ScaleRandr2_mcmc <- as.data.frame(b.ScaleRandr2)
hist(ScaleRandr2_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-OCBO r Between Control and Scale-Randomized Scales", yaxt = "n")
1-(mean(ScaleRandr2_mcmc$rho - control_mcmc$rho > .1 )+mean(ScaleRandr2_mcmc$rho - control_mcmc$rho < -.1 ))
#40% chance that filler scales have no effect defined by r = [-.10, .10]. (update)

control_mcmc <- as.data.frame(b.Controlr3)
ScaleRandr3_mcmc <- as.data.frame(b.ScaleRandr3)
hist(ScaleRandr3_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-IRB r Between Control and Scale-Randomized Scales", yaxt = "n")
1-(mean(ScaleRandr3_mcmc$rho - control_mcmc$rho > .1 )+mean(ScaleRandr3_mcmc$rho - control_mcmc$rho < -.1 ))
#40% chance that filler scales have no effect defined by r = [-.10, .10]. (update)

##Note for Wayne: Text above should be replicated for the other intercorrelations other than PP-OCBI (e.g., PP-OCBO, PP-IRB, etc.).
#Plot differences for PP-OCBO relationship
```

#General Discussion
#References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
