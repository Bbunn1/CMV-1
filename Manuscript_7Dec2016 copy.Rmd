---
title: You Gotta Keep em' Separated: Examining the Efficacy of Proximal Remedies for Causes of Method Variance 
author:
- address: 253 Rumford Ct., Newtown, PA 18940
  affiliation: '1'
  corresponding: yes
  email: chris_castille@mac.com
  name: Christopher M. Castille
- affiliation: '2'
  name: Marcia Simmering
affiliation:
- id: '1'
  institution: Rutgers Business School
- id: '2'
  institution: Louisiana Tech University
output: pdf_document
bibliography: r-references.bib
class: man
figsintext: no
figurelist: no
footnotelist: no
keywords: method variance, procedural remedies, statistical remedies
lang: english
lineno: yes
author_note: null
shorttitle: PROXIMAL CMV REMEDIES
tablelist: no
abstract: "Organizational scholars have long argued that method factors introduce nuisance variance into observations and bias our results. In response to this threat, researchers have proposed procedural remedies. For studies where measures are gathered from the same source (e.g., employee) and same time-point (one survey administration), recommended remedies include (1) presenting participants with a cover story to disguise the purpose of the survey (which addresses respondents' ability to produce data consistent with researchers' hypotheses), (2) randomizing or counterbalancing the order of item presentation (which addresses item context effects), and (3) introducing a brief temporal separation (which addresses respondents' momentary mood). Though researchers have relied upon these proximal method variance remedies, there are no studies examining their efficacy. Here, we present the findings from two experiments utilizing the same measurement model and demonstrate that these remedies do reduce method variance. However, and surprisingly, our findings yielded no instance in which the remedies reduced common method variance across the predictor and criteria measures of our model. Rather, other sources of variance (i.e., trait affectivity) that were not addressed by these remedies (and not addressed by available procedural remedies) led to significantly biased estimates because they were shared across predictors and criteria. We conclude with recommendations for researchers wishing to reduce the presence of method variance in their studies."    
wordcount: X
---

Hi Wayne! Again. 

```{r message = FALSE, warning = FALSE}
#library("papaja")
library("readr")
library("qualtRics")
library("datapack")

#Optional generic preliminaries.
graphics.off() # This closes all of R's graphics windows.
rm(list=ls())  # Careful! This clears all of R's memory!

######  BUILD DATASETS  ######

#Link to Qualtrics
registerOptions(api_token="U8ut239nN4WeqDo0V1cDniUGMBG1jRtICUgZw0hp", root_url="rutgers.qualtrics.com")
surveys <- getSurveys()
#Get survey data
mysurvey <- getSurvey(surveys$id[3], force_request = TRUE)
##Prune the stuff we don't need.#Note: Make sure to keep the "Finished" variable as some folks do not turn in the survey even though they complete it. 
mysurvey <- mysurvey[c(-1,-2,-3,-4,-5,-6,-7,-8,-9,-11,-13)]
data <- mysurvey
##Collapse like data across columns.
###Conditions; build independent data sets and then rename the condition variable.
###1 Control
data$Demand.ConsitMotif1[is.na(data$Demand.ConsitMotif1)] <- 0
data$Demand.ConsitMotif1 <- as.numeric(data$Demand.ConsitMotif1)
###2 Cover Story Manipulation
data$CoverStoryManip <- ifelse(data$CoverStoryManip == 1, c("2"))
data$CoverStoryManip[is.na(data$CoverStoryManip)] <- 0
data$CoverStoryManip <- as.numeric(data$CoverStoryManip)
###3 Randomized Items
data$Demand.ConsitMotif3 <- ifelse(data$Demand.ConsitMotif3 == 1, c("3"))
data$Demand.ConsitMotif3[is.na(data$Demand.ConsitMotif3)] <- 0
data$Demand.ConsitMotif3 <- as.numeric(data$Demand.ConsitMotif3)
###4 Filler Scales
data$Demand.ConsitMotif4 <- ifelse(data$Demand.ConsitMotif4 == 1, c("4"))
data$Demand.ConsitMotif4[is.na(data$Demand.ConsitMotif4)] <- 0
data$Demand.ConsitMotif4 <- as.numeric(data$Demand.ConsitMotif4)
###5 Randomized Scales
data$Demand.ConsitMotif2 <- ifelse(data$Demand.ConsitMotif2 == 1, c("5"))
data$Demand.ConsitMotif2[is.na(data$Demand.ConsitMotif2)] <- 0
data$Demand.ConsitMotif2 <- as.numeric(data$Demand.ConsitMotif2)
#Combine condition data into single variable column
data$Condition <- data$Demand.ConsitMotif1 + data$CoverStoryManip + data$Demand.ConsitMotif3 + data$Demand.ConsitMotif4 + data$Demand.ConsitMotif2
####Delete old condition data.
data <- subset(data, select=-c(Demand.ConsitMotif1, CoverStoryManip, Demand.ConsitMotif3, Demand.ConsitMotif4, Demand.ConsitMotif2))
#Apply value lables
data$Condition <- factor(data$Condition, levels = c(1,2,3,4,5), labels = c("Control", "CSManipulation","ItemRand","FillerScales","ScaleRand"))
###Combine all open-ended responses to the manipulation check for study purpose
data$PurpCk1 <- paste(data$PurpChk1a, data$PurpChk2a,data$PurpChk3a, data$PurpChk4a, data$PurpChk5a)
data$PurpCk2 <- paste(data$PurpChk1b, data$PurpChk2b,data$PurpChk3b, data$PurpChk4b, data$PurpChk5b) 
####Delete old purpose chk data. 
data <- subset(data, select=-c(PurpChk1a, PurpChk1b, PurpChk2a, PurpChk2b, PurpChk3a, PurpChk3b, PurpChk4a, PurpChk4b, PurpChk5a, PurpChk5b))
####Delte meaningless condition data. IC = Informed Consent. FOR = Frame of Reference.
data <- subset(data, select=-c(IC1, IC2, IC3, Q136, FOR1, FOR2, FOR4, FOR5, FORx))
#Move Finished to the end of the file.
moveMe <- function(data, tomove, where = "last", ba = NULL) {
  temp <- setdiff(names(data), tomove)
  x <- switch(
    where,
    first = data[c(tomove, temp)],
    last = data[c(temp, tomove)],
    before = {
      if (is.null(ba)) stop("must specify ba column")
      if (length(ba) > 1) stop("ba must be a single character string")
      data[append(temp, values = tomove, after = (match(ba, temp)-1))]
    },
    after = {
      if (is.null(ba)) stop("must specify ba column")
      if (length(ba) > 1) stop("ba must be a single character string")
      data[append(temp, values = tomove, after = (match(ba, temp)))]
    })
  x
}
data <- moveMe(data, c("Finished"),"after", "PurpCk2")
#Delete Prep and Text variables.
data <- data[c(-2,-3,-69,-135,-201,-268)]
#Consolidate item response measures
##Form Likert datasets, re-lable values, convert to numeric data, consolidate measures, and delete data.
agree <- data[c(2:41,67:106,132:171,198:209,235:302,329:335)]
agree <- ifelse(agree == "Strongly Disagree", 1, ifelse(agree == "Disagree", 2, ifelse(agree == "Neither Agree nor Disagree", 3, ifelse(agree == "Agree", 4, ifelse(agree == "Strongly Agree", 5,0)))))
write.csv(agree, file = "agree.csv")
agree <- read_csv("agree.csv")
#####Proactive Personality 
agree$PP1	<-	agree$PP1_1	+	agree$PP2_1	+	agree$PP5_1	+	agree$PP6_1	+	agree$PPx_1
agree$PP2	<-	agree$PP1_2	+	agree$PP2_2	+	agree$PP5_2	+	agree$PP6_2	+	agree$PPx_2
agree$PP3	<-	agree$PP1_3	+	agree$PP2_3	+	agree$PP5_3	+	agree$PP6_3	+	agree$PPx_3
agree$PP4	<-	agree$PP1_4	+	agree$PP2_4	+	agree$PP5_4	+	agree$PP6_4	+	agree$PPx_4
agree$PP5	<-	agree$PP1_5	+	agree$PP2_5	+	agree$PP5_5	+	agree$PP6_5	+	agree$PPx_5
agree$PP6	<-	agree$PP1_6	+	agree$PP2_6	+	agree$PP5_6	+	agree$PP6_6	+	agree$PPx_6
agree$PP7	<-	agree$PP1_7	+	agree$PP2_7	+	agree$PP5_7	+	agree$PP6_7	+	agree$PPx_7
agree$PP8	<-	agree$PP1_8	+	agree$PP2_8	+	agree$PP5_8	+	agree$PP6_8	+	agree$PPx_8
agree$PP9	<-	agree$PP1_9	+	agree$PP2_9	+	agree$PP5_9	+	agree$PP6_9	+	agree$PPx_9
agree$PP10	<-	agree$PP1_10	+	agree$PP2_10	+	agree$PP5_10	+	agree$PP6_10	+	agree$PPx_10
#####OCBI
agree$OCBI1	<-	agree$OCBI1_1	+	agree$OCBI2_1	+	agree$OCBI5_1	+	agree$OCBI6_1	+	agree$OCBIx_1
agree$OCBI2	<-	agree$OCBI1_3	+	agree$OCBI2_3	+	agree$OCBI5_3	+	agree$OCBI6_3	+	agree$OCBIx_3
agree$OCBI3	<-	agree$OCBI1_4	+	agree$OCBI2_4	+	agree$OCBI5_4	+	agree$OCBI6_4	+	agree$OCBIx_4
agree$OCBI4	<-	agree$OCBI1_5	+	agree$OCBI2_5	+	agree$OCBI5_5	+	agree$OCBI6_5	+	agree$OCBIx_5
agree$OCBI5	<-	agree$OCBI1_6	+	agree$OCBI2_6	+	agree$OCBI5_6	+	agree$OCBI6_6	+	agree$OCBIx_6
agree$OCBI6	<-	agree$OCBI1_7	+	agree$OCBI2_7	+	agree$OCBI5_7	+	agree$OCBI6_7	+	agree$OCBIx_7
agree$OCBI7	<-	agree$OCBI1_8	+	agree$OCBI2_8	+	agree$OCBI5_8	+	agree$OCBI6_8	+	agree$OCBIx_8
#####OCBO
agree$OCBO1	<-	agree$OCBO1_1	+	agree$OCBO2_1	+	agree$OCBO5_1	+	agree$OCBO6_1	+	agree$OCBOx_1
agree$OCBO2	<-	agree$OCBO1_2	+	agree$OCBO2_2	+	agree$OCBO5_2	+	agree$OCBO6_2	+	agree$OCBOx_2
agree$OCBO3	<-	agree$OCBO1_3	+	agree$OCBO2_3	+	agree$OCBO5_3	+	agree$OCBO6_3	+	agree$OCBOx_3
agree$OCBO4	<-	agree$OCBO1_5	+	agree$OCBO2_5	+	agree$OCBO5_5	+	agree$OCBO6_5	+	agree$OCBOx_5
agree$OCBO5	<-	agree$OCBO1_6	+	agree$OCBO2_6	+	agree$OCBO5_6	+	agree$OCBO6_6	+	agree$OCBOx_6
agree$OCBO6	<-	agree$OCBO1_7	+	agree$OCBO2_7	+	agree$OCBO5_7	+	agree$OCBO6_7	+	agree$OCBOx_7
agree$OCBO7	<-	agree$OCBO1_8	+	agree$OCBO2_8	+	agree$OCBO5_8	+	agree$OCBO6_8	+	agree$OCBOx_8
#####IRB
agree$IRB1	<-	agree$IRB1_1	+	agree$IRB2_1	+	agree$IRB5_1	+	agree$IRB6_1	+	agree$IRBx_1
agree$IRB2	<-	agree$IRB1_3	+	agree$IRB2_3	+	agree$IRB5_3	+	agree$IRB6_3	+	agree$IRBx_3
agree$IRB3	<-	agree$IRB1_4	+	agree$IRB2_4	+	agree$IRB5_4	+	agree$IRB6_4	+	agree$IRBx_4
agree$IRB4	<-	agree$IRB1_5	+	agree$IRB2_5	+	agree$IRB5_5	+	agree$IRB6_5	+	agree$IRBx_5
agree$IRB5	<-	agree$IRB1_6	+	agree$IRB2_6	+	agree$IRB5_6	+	agree$IRB6_6	+	agree$IRBx_6
agree$IRB6	<-	agree$IRB1_8	+	agree$IRB2_8	+	agree$IRB5_8	+	agree$IRB6_8	+	agree$IRBx_8
agree$IRB7	<-	agree$IRB1_9	+	agree$IRB2_9	+	agree$IRB5_9	+	agree$IRB6_9	+	agree$IRBx_9
#####Inattentive Responding
agree$IR	<-	agree$PP1_11	+	agree$PP2_11	+	agree$PP5_11	+	agree$PP6_11	+	agree$PPx_11
#####Consistency Motif
#Brave
agree$CM1	<-	agree$PP1_12	+	agree$PP2_12	+	agree$PP5_12	+	agree$PP6_12	+	agree$PPx_12
#Often feel blue.
agree$CM2	<-	agree$OCBI1_2	+	agree$OCBI2_2	+	agree$OCBI5_2	+	agree$OCBI6_2	+	agree$OCBIx_2
#Talkative
agree$CM3	<-	agree$OCBI1_9	+	agree$OCBI2_9	+	agree$OCBI5_9	+	agree$OCBI6_9	+	agree$OCBIx_9
#Pessimistic Person
agree$CM4	<-	agree$OCBO1_4	+	agree$OCBO2_4	+	agree$OCBO5_4	+	agree$OCBO6_4	+	agree$OCBOx_4
#Silent Person
agree$CM5	<-	agree$OCBO1_9	+	agree$OCBO2_9	+	agree$OCBO5_9	+	agree$OCBO6_9	+	agree$OCBOx_9
#Courageous
agree$CM6	<-	agree$IRB1_2	+	agree$IRB2_2	+	agree$IRB5_2	+	agree$IRB6_2	+	agree$IRBx_2
#Optimistic 
agree$CM7	<-	agree$IRB1_7	+	agree$IRB2_7	+	agree$IRB5_7	+	agree$IRB6_7	+	agree$IRBx_7
#Seldom feel blue.
agree$CM8	<-	agree$IRB1_10	+	agree$IRB2_10	+	agree$IRB5_10	+	agree$IRB6_10	+	agree$IRBx_10
#Consolidate "agree" dataset.
agree <- agree[c(202:248)]
#HALO
halo <- data[c(42:45,107:110,172:175,210:213,303:306)]
halo <- ifelse(halo == "Very bad", 1, ifelse(halo == "Bad", 2, ifelse(halo == "Somewhat bad", 3, ifelse(halo == "Neither good nor bad", 4, ifelse(halo == "Somewhat good", 5, ifelse(halo == "Good", 6, ifelse(halo == "Very good", 7, 0)))))))
write.csv(halo, file = "halo.csv")
halo <- read_csv("halo.csv")
halo$HALO1	<-	halo$HALO1_1	+	halo$HALO2_1	+	halo$HALO5_1	+	halo$HALO6_1 +	halo$HALOx_1
halo$HALO2	<-	halo$HALO1_2	+	halo$HALO2_2	+	halo$HALO5_2	+	halo$HALO6_2 +	halo$HALOx_2
halo$HALO3	<-	halo$HALO1_3	+	halo$HALO2_3	+	halo$HALO5_3	+	halo$HALO6_3 +	halo$HALOx_3
halo$HALO4	<-	halo$HALO1_4	+	halo$HALO2_4	+	halo$HALO5_4	+	halo$HALO6_4 +	halo$HALOx_4
#Consolidate "halo" dataset.
halo <- halo[c(22:25)]
#PANAS
PANAS <- data[c(46:65,111:130,176:195,214:233,307:326)]
PANAS <- ifelse(PANAS == "Very slightly or not at all", 1, ifelse(PANAS == "A little", 2, ifelse(PANAS == "Moderately", 3, ifelse(PANAS == "Quite a bit", 4, ifelse(PANAS == "Extremely", 5, 0)))))
write.csv(PANAS, file = "PANAS.csv")
PANAS <- read_csv("PANAS.csv")
PANAS$PAff1	<-	PANAS$PAff1_1	+	PANAS$PAff2_1	+	PANAS$PAff5_1	+	PANAS$PAff6_1	+	PANAS$PAffx_1
PANAS$PAff2	<-	PANAS$PAff1_2	+	PANAS$PAff2_2	+	PANAS$PAff5_2	+	PANAS$PAff6_2	+	PANAS$PAffx_2
PANAS$PAff3	<-	PANAS$PAff1_3	+	PANAS$PAff2_3	+	PANAS$PAff5_3	+	PANAS$PAff6_3	+	PANAS$PAffx_3
PANAS$PAff4	<-	PANAS$PAff1_4	+	PANAS$PAff2_4	+	PANAS$PAff5_4	+	PANAS$PAff6_4	+	PANAS$PAffx_4
PANAS$PAff5	<-	PANAS$PAff1_5	+	PANAS$PAff2_5	+	PANAS$PAff5_5	+	PANAS$PAff6_5	+	PANAS$PAffx_5
PANAS$PAff6	<-	PANAS$PAff1_6	+	PANAS$PAff2_6	+	PANAS$PAff5_6	+	PANAS$PAff6_6	+	PANAS$PAffx_6
PANAS$PAff7	<-	PANAS$PAff1_7	+	PANAS$PAff2_7	+	PANAS$PAff5_7	+	PANAS$PAff6_7	+	PANAS$PAffx_7
PANAS$PAff8	<-	PANAS$PAff1_8	+	PANAS$PAff2_8	+	PANAS$PAff5_8	+	PANAS$PAff6_8	+	PANAS$PAffx_8
PANAS$PAff9	<-	PANAS$PAff1_9	+	PANAS$PAff2_9	+	PANAS$PAff5_9	+	PANAS$PAff6_9	+	PANAS$PAffx_9
PANAS$PAff10	<-	PANAS$PAff1_10	+	PANAS$PAff2_10	+	PANAS$PAff5_10	+	PANAS$PAff6_10	+	PANAS$PAffx_10
PANAS$NAff1	<-	PANAS$NAff1_1	+	PANAS$NAff2_1	+	PANAS$NAff5_1	+	PANAS$NAff6_1	+	PANAS$NAffx_1
PANAS$NAff2	<-	PANAS$NAff1_2	+	PANAS$NAff2_2	+	PANAS$NAff5_2	+	PANAS$NAff6_2	+	PANAS$NAffx_2
PANAS$NAff3	<-	PANAS$NAff1_3	+	PANAS$NAff2_3	+	PANAS$NAff5_3	+	PANAS$NAff6_3	+	PANAS$NAffx_3
PANAS$NAff4	<-	PANAS$NAff1_4	+	PANAS$NAff2_4	+	PANAS$NAff5_4	+	PANAS$NAff6_4	+	PANAS$NAffx_4
PANAS$NAff5	<-	PANAS$NAff1_5	+	PANAS$NAff2_5	+	PANAS$NAff5_5	+	PANAS$NAff6_5	+	PANAS$NAffx_5
PANAS$NAff6	<-	PANAS$NAff1_6	+	PANAS$NAff2_6	+	PANAS$NAff5_6	+	PANAS$NAff6_6	+	PANAS$NAffx_6
PANAS$NAff7	<-	PANAS$NAff1_7	+	PANAS$NAff2_7	+	PANAS$NAff5_7	+	PANAS$NAff6_7	+	PANAS$NAffx_7
PANAS$NAff8	<-	PANAS$NAff1_8	+	PANAS$NAff2_8	+	PANAS$NAff5_8	+	PANAS$NAff6_8	+	PANAS$NAffx_8
PANAS$NAff9	<-	PANAS$NAff1_9	+	PANAS$NAff2_9	+	PANAS$NAff5_9	+	PANAS$NAff6_9	+	PANAS$NAffx_9
PANAS$NAff10	<-	PANAS$NAff1_10	+	PANAS$NAff2_10	+	PANAS$NAff5_10	+	PANAS$NAff6_10	+	PANAS$NAffx_10
#Consolidate "PANAS" dataset.
PANAS <- PANAS[c(102:121)]
#Consolidate manipulation checks. 
##Deleting likert/psychological data.
data <- subset(data,select=-c(2:327,329:335))
##Deleting other nuissance variables.
data <- subset(data,select=-c(2,13,23:36))
##Reorganize data.
#data <- data[,c(1,6:14, 15,16, 2, 3, 4, 5)]
#write.csv(data, file = "proximalCMV2.csv")
#Bind all datasets.
data <- cbind(data,agree,halo,PANAS)

#MTurk IGNORE THIS SEGMENT as MTurkR is now compromised.
#require("MTurkR")
#Link to MTurk (Work on MTurkR)
#Sys.setenv("AWS_ACCESS_KEY_ID" = "")
#Sys.setenv("AWS_SECRET_ACCESS_KEY" = "")
#Test connection to live server
#AccountBalance()
#SearchHITS
#hits <- SearchHITs()
#Buy more HITS. CAREFUL. 
#ExtendHIT(hit.type = "3SVZJ4A2P5CW4G2BZY7UE78GFL1IEV",add.assignments = 60, add.seconds = seconds(days=7))
#GetHITs and merge into MTurk dataframe. Use "SearchHITS()" to find the correct HIT. Make sure you have the right HITId. The followering are seprated out by batches.
#Go to MTurk account and manually download batch results. Place the batch in the working directory. 
a <- read_csv("Batch_2846969_batch_results.csv")
#Manually edit confirmation codes.
#a$Answer.surveycode[a$Answer.surveycode=="A11FRLH5KWRLBV"] <- "96090306"
#How long did the average participant take to complete the survey?
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
getmode(a$WorkTimeInSeconds)/60

#Create seperate datasets to identify possible those who did not complete the survey. Convert confirmation_code from character to numeric.
data.mt <- a[c(15,16,17,24,28)]
#Relabel variables to allow merging.
names(data.mt) <- c("AssignmentID","WorkerID","AssignmentStatus","Seconds","confirmation_code")
data.mt <- transform(data.mt, confirmation_code = as.numeric(confirmation_code))
#Ensure that paid participants' data are analyzed
data1 <- merge(data.mt, data, ID="confirmation_code",  all.x = TRUE)
#Acquire data from individuals who completed the survey in earnest. 
data1.1 <- merge(data.mt, data, ID="confirmation_code",  all.y = TRUE)
data1.1 <- data1.1[is.na(data1.1$WorkerID),]
rownames(data1.1) <- seq(length=nrow(data1.1)) 
#Subset in individuals providing a confirmation code that is not in our database. 
false.code <- data1[is.na(data1$Finished),]
false.code <- subset(false.code, AssignmentStatus=='Submitted')
#Subset in individuals with complete data and get rid of the workerIDs.
data2 <- merge(data.mt, data, ID="confirmation_code")
data2 <- rbind(data2,data1.1)
data <- data2[c(-2)]
#Subset into submitted, accepted, and rejected.
submitted <- data.mt[ which(data.mt$AssignmentStatus=='Submitted'), ]
approved <- data.mt[ which(data.mt$AssignmentStatus=='Approved'), ]
rejected <- data.mt[ which(data.mt$AssignmentStatus=='Rejected'), ]
#Build dataset
a <- a[c(28,24)]
names(a) <- c("confirmation_code", "WorkTimeInSeconds")
df <- merge(a,data,by="confirmation_code")
#Get RID of the worker IDS.
data <- data[c(-2)]

###### ENGAGE MACHINE LEARNING TO AUTOMATE MANIPULATION CHECK ######
#Engage supervised machine learning to analyze purpose check data, which will tell us if participants actually watched the video and got the purpose of the study. 
#library(caret)
#library(tm)
#library(SnowballC)
#library(stringr)
#Training data.NOTE: You'll need to filter in the 30 cases that force participants to distinguish purpose 1 from purpose 2. This will improve the accuracy of the algorithms.
#data.t <- data[1:60]
#data.t <- data.t[c(23,24,25,26)]
#data.t[] <- lapply(data.t, str_trim)
#is.na(data.t) <- data.t==''
#data.t <- na.omit(data.t)
#cond.code <- data.t[c(1)]
#data.t <- data.t[c(2)]
#corpus <- VCorpus(VectorSource(data.t$PurpCk1))
##Create a text document matrix.
#tdm <- DocumentTermMatrix(corpus, list(removePunctuation = TRUE, stripWhitespace = TRUE, content_transformer(tolower), stopwords = TRUE, stemming = FALSE, removeNumbers = FALSE))
##Convert to a data.frame for training and assign a classification (factor) to each document.
#train <- as.matrix(tdm)
#Create condition code that differentiates individuals from conditions.
#cond.code <- as.data.frame(ifelse(cond.code$Condition== "CSManipulation", 1,0))
#train <- cbind(train, cond.code)
#colnames(train)[ncol(train)] <- 'y'
#train <- as.data.frame(train)
#train$y <- as.factor(train$y)
##Train.
#require(foreach)
#registerDoSEQ()
#Training control prevents model over-fitting. 
#tc <- trainControl(method = "cv", number = 7, verboseIter=FALSE , #preProcOptions="pca", allowParallel=TRUE)
#bayesglm <- train(y ~ ., data = train, method = 'bayesglm', trControl=tc)
#rf <- train(y ~ ., data = train, method = 'rf', trControl=tc)
#NN <- train(y ~ ., data = train, method = 'nnet', trControl=tc, verbose=FALSE)
#svml <- train(y ~ ., data = train, method = 'svmLinear', trControl=tc)
#logitboost <- train(y ~ ., data = train, method = 'LogitBoost', trControl=tc)

#This is used to complare the models against one anoher.
#model <- c("Bayes GLM", "Neural Net", "SVM (linear)", "LogitBoost")
#Accuracy <- c(max(bayesglm$results$Accuracy),
#              max(NN$results$Accuracy),
#              max(svml$results$Accuracy),
#              max(logitboost$results$Accuracy))
#Kappa <- c(max(bayesglm$results$Kappa),
#           max(NN$results$Kappa),
#           max(svml$results$Kappa),
#           max(logitboost$results$Kappa))
#performance <- cbind(model,Accuracy,Kappa)
#knitr::kable(performance)
#Create new dataset and fit models to new data.
#data.n <- data[1:60]
#data.n <- data.n[c(24,25,26)]
#data.n <- rbind(data.t,data.n)
#data.n[] <- lapply(data.n, str_trim)
#is.na(data.n) <- data.n==''
#data.n <- na.omit(data.n)
#data.n <- unique(data.n)
#data.n <- data.n$PurpCk1
#data.n <- as.data.frame(data.n)
#Fit model to new data
#corpus <- VCorpus(VectorSource(data.n$data.n))
##Create a text document matrix.
#tdm <- DocumentTermMatrix(corpus, list(removePunctuation = TRUE, stripWhitespace = TRUE, content_transformer(tolower), stopwords = TRUE, stemming = FALSE, removeNumbers = FALSE))
#test <- as.matrix(tdm)
##Convert to a data.frame for training and assign a classification (factor) to each document.
#predict(svml,test)
###IT WORKS!!!! Send to team. 
#share <- subset(data, Condition=="CSManipulation")
#share <- share[c(24,25)]

#Assuming that some people correctly guessed the hypothesis, then engage supervised machine learning to analyze hypothesis guessing data. 
#data <- data[1:30,]
#data <- df$HypoGuessing2

######  DEVELOP SCALE SCORES AND EVALUATE USING CLASSICAL TEST THEORY ###### 
require(psych)
#see  the example including the bfi data set
keys.list <- list(PP=c("PP1","PP2","PP3","PP4","PP5","PP6","PP7","PP8","PP9","PP10"),OCBI=c("OCBI1","OCBI2","OCBI3","OCBI4","OCBI5","OCBI6","OCBI7"), OCBO=c("OCBO1","OCBO2","-OCBO3","-OCBO4","-OCBO5","OCBO6","OCBO7"), IRB=c("IRB1","IRB2","IRB3","IRB4","IRB5","-IRB6","-IRB7")) 
keys <- make.keys(df,keys.list)  #no longer necessary
scores <- scoreItems(keys,df,min=1,max=5)  #using a keys matrix 
#Create scale scores
scores <- as.data.frame(scores$scores)
#Bind scales with condition variables. 
final <- as.data.frame(df$Condition)
names(final) <- c("Condition")
final <- cbind(final,scores)

###Evaluate Scale Reliability. Code examining scale reliability is needed. Wayne, I can teach you how to do this. 
library(CTT)

######  TEST HYPOTHESES ###### 
require(BayesFactor)
plot(PP ~ Condition, data = final)
plot(OCBI ~ Condition, data = final)
plot(OCBO ~ Condition, data = final)
plot(IRB ~ Condition, data = final)

#Check the BayesFactor package to make sure that you are using the correct formula. 

#Proactive Personality 
describeBy(final$PP, final$Condition )
summary(aov(PP ~ Condition, data = final))
anovaBF(PP ~ Condition, data = final, progress=FALSE)
###Seems to favor null so far...

#OCBI
describeBy(final$OCBI, final$Condition )
summary(aov(OCBI ~ Condition, data = final))
anovaBF(OCBI ~ Condition, data = final, progress=FALSE)
###Seems to favor null so far...

#OCBO
describeBy(final$OCBO, final$Condition )
summary(aov(OCBO ~ Condition, data = final))
anovaBF(OCBO ~ Condition, data = final, progress=FALSE)

#IRB
describeBy(final$IRB, final$Condition )
summary(aov(IRB ~ Condition, data = final))
anovaBF(IRB ~ Condition, data = final, progress=FALSE)

######  Bayesian Measurement Invariance ######  
#The goal of the analysis is to examine the invariance (or lackthereof) of various invariant or non-invariant models. In other words, test for stricter and stricter forms of invariance. If survey design features are causing differences, then a non-invariant model will provide a better fit to the data than a model assuming invariance.
#require(blavaan)
#SubstantiveModel <- '
##Substantive Factors
##  PR =~ PP1+PP2+PP3+PP4+PP5+PP6+PP7+PP8+PP9+PP10
##  IRB =~ IRB1+IRB2+IRB3+IRB4+IRB5+IRB6+IRB7
##  OCBI =~ OCBI1+OCBI2+OCBI3+OCBI4+OCBI5+OCBI6+OCBI7
#  OCBO =~ OCBO1+OCBO2+OCBO3+OCBO4+OCBO5+OCBO6+OCBO7
#'
#bfit1 <- bcfa(SubstantiveModel, data = df, burnin = 4000, sample = 10000, jagcontrol = list(method = "rjparallel"))
#summary(bfit1, standardized = TRUE)
##Model is poor fit to the data.
#Equal loadings
#bfit2 <- bcfa(HS.model, data = HolzingerSwineford1939, group = "school", burnin = 1000, sample = 1000, group.equal = "loadings", jagcontrol = list(method = "rjparallel"))
#blavCompare(bfit1, bfit2)
##Conclusion: Configural invariance is 19.836 times more probable than configural non-invariance (though this model asssuming configural invariance is also, still, a bad model). 
#bfit3 <- bcfa(HS.model, data = HolzingerSwineford1939, group = "school", burnin = 1000, sample = 1000, group.equal = c("loadings","intercepts"), jagcontrol = list(method = "rjparallel"))
#blavCompare(bfit2, bfit3)
#fitmeasures(bfit3)
#summary(bfit1, standardized = TRUE)
#blavInspect(bfit1,"hpd")
##Plots:
#plot(bfit1, plot.type="trace")
#plot(bfit1, plot.type="autocorr")
#plot(bfit1, "histogram")
```


Notes below are the Bayesian test of the invariance of scale intercorrelations. 
```{r}
library(bayesboot)
library(BayesFactor)
library(BayesMed)
library(cocor)
library(plyr)
library(psych)
library(cocor)
library(BayesianFirstAid)
library(ggplot2)
library(QRM)

#Frequentist estimate of correlation between PP and OCBI for across conditions.
f.Controlr <- cor.test( ~ PP + OCBI, data = final[final$Condition == "Control", ])
f.ItemRandr <- cor.test( ~ PP + OCBI, data = final[final$Condition == "ItemRand", ])
f.CSManpipr <- cor.test( ~ PP + OCBI, data = final[final$Condition == "CSManipulation", ])
f.FScalesr <- cor.test( ~ PP + OCBI, data = final[final$Condition == "FillerScales", ])
f.ScaleRandr <- cor.test( ~ PP + OCBI, data = final[final$Condition == "ScaleRand", ])

#Examining correlations between variables across conditions.
#Extract sample sizes
c <- describe.by(final, group="Condition")
Controln <- c$Control$n[c(1)]
CSManipn <- c$CSManipulation$n[c(1)]
ItemRandn <- c$ItemRand$n[c(1)]
FScalesn <- c$FillerScales$n[c(1)]
ScaleRandn <- c$ScaleRand$n[c(1)]

#Extract correlations and compare
func <- function(xx)
{
return(data.frame(COR = cor(xx$a, xx$b)))
}
##PP and OCBI
xx <- data.frame(group = final$Condition, a = final$PP, b = final$OCBI)
a <- ddply(xx, .(group), func)

#Evaluate the difference in correlations using frequentist method: Fisher r to z transformation and test of significant differences. Note: these tests are conducted singly as in the test compares only two conditions at a time. It is possible to run an omnibus test that examines whether a correlation varies across all groups. However, the code for this test would need to be written. No big deal, but an easy alternative would be to correct for the number of tests that we run, which may be too conservative. See http://davidakenny.net/doc/statbook/chapter_16.pdf for more information. 

##1. PP and OCBI: Control vs. CSManipulation
Control.PP.OCBI.r <- a[1,]
Control.PP.OCBI.r <- Control.PP.OCBI.r[2]
CSManip.PP.OCBI.r <- a[2,]
CSManip.PP.OCBI.r <- CSManip.PP.OCBI.r[2]
cocor.indep.groups(as.numeric(Control.PP.OCBI.r), as.numeric(CSManip.PP.OCBI.r), Controln, CSManipn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis retained using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results.

##2. PP and OCBI: Control vs. ItemRand
ItemRand.PP.OCBI.r <- a[3,]
ItemRand.PP.OCBI.r <- ItemRand.PP.OCBI.r[2]
cocor.indep.groups(as.numeric(Control.PP.OCBI.r), as.numeric(ItemRand.PP.OCBI.r), Controln, ItemRandn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis retained using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results.

##3. PP and OCBI: Control vs. FScales
FScales.PP.OCBI.r <- a[4,]
FScales.PP.OCBI.r <- FScales.PP.OCBI.r[2]
cocor.indep.groups(as.numeric(Control.PP.OCBI.r), as.numeric(FScales.PP.OCBI.r), Controln, FScalesn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis rejected using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results.

##4. PP and OCBI: Control vs. ScaleRand
ScaleRand.PP.OCBI.r <- a[5,]
ScaleRand.PP.OCBI.r <- ScaleRand.PP.OCBI.r[2]
cocor.indep.groups(as.numeric(Control.PP.OCBI.r), as.numeric(ScaleRand.PP.OCBI.r), Controln, ScaleRandn, alternative = "two.sided", test = "all", alpha = 0.05, conf.level = 0.95, null.value = 0, data.name = NULL, var.labels = NULL, return.htest = FALSE)
###Null Hypothesis retained using frequentist methods. Conclusion is tentative as we may not have the power to detect effects. See Bayesian results.

####Note for Wayne: Text above should be replicated for the other intercorrelations other than PP-OCBI (e.g., PP-OCBO, PP-IRB, etc.).
##Praoctive Personality and OCBO
xx <- data.frame(group = final$Condition, a = final$PP, b = final$OCBO)
a <- ddply(xx, .(group), func)

##Praoctive Personality and IRB
xx <- data.frame(group = final$Condition, a = final$PP, b = final$IRB)
a <- ddply(xx, .(group), func)

##OCBI and OCBO
xx <- data.frame(group = final$Condition, a = final$OCBI, b = final$OCBO)
a <- ddply(xx, .(group), func)

##OCBI and IRB
xx <- data.frame(group = final$Condition, a = final$OCBI, b = final$IRB)
a <- ddply(xx, .(group), func)

##OCBO and IRB
xx <- data.frame(group = final$Condition, a = final$OCBO, b = final$IRB)
a <- ddply(xx, .(group), func)

#Bayesian estimate of correlation between PP and OCBI for across conditions.
b.Controlr <- bayes.cor.test( ~ PP + OCBI, data = final[final$Condition == "Control", ])
b.ItemRandr <- bayes.cor.test( ~ PP + OCBI, data = final[final$Condition == "ItemRand", ])
b.CSManpipr <- bayes.cor.test( ~ PP + OCBI, data = final[final$Condition == "CSManipulation", ])
b.FScalesr <- bayes.cor.test( ~ PP + OCBI, data = final[final$Condition == "FillerScales", ])
b.ScaleRandr <- bayes.cor.test( ~ PP + OCBI, data = final[final$Condition == "ScaleRand", ])

#Examine fit of the models
plot(b.Controlr)
plot(b.ItemRandr)
plot(b.CSManpipr)
plot(b.FScalesr)
plot(b.ScaleRandr)

#Plot differences for PP-OCBI relationship and estimate proportion of effects that are consistent with a null effect, which here is defined as r = [-.10, .10].
#Item Randomization
control_mcmc <- as.data.frame(b.Controlr)
ItemRandr_mcmc <- as.data.frame(b.ItemRandr)
hist(ItemRandr_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-OCBI r Between Control and Item-Randomized Scales", yaxt = "n")
1-(mean(ItemRandr_mcmc$rho - control_mcmc$rho > .1 )+mean(ItemRandr_mcmc$rho - control_mcmc$rho < -.1 ))
#48% chance that filler scales have no effect defined by r = [-.10, .10]. 

#Cover Story Manipulation
control_mcmc <- as.data.frame(b.Controlr)
CSManpipr_mcmc <- as.data.frame(b.CSManpipr)
hist(CSManpipr_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-OCBI r Between Control and Coverstory", yaxt = "n")
1-(mean(CSManpipr_mcmc$rho - control_mcmc$rho > .1 )+mean(CSManpipr_mcmc$rho - control_mcmc$rho < -.1 ))
#51% chance that filler scales have no effect defined by r = [-.10, .10]. 

#Large difference between control and filler scales. Examine the probability that filler scales weaken correlation.
control_mcmc <- as.data.frame(b.Controlr)
Fscales_mcmc <- as.data.frame(b.FScalesr)
head(control_mcmc)
hist(Fscales_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-OCBI r Between Control and Filler Scales", yaxt = "n")
1-(mean(Fscales_mcmc$rho - control_mcmc$rho > .1 )+mean(Fscales_mcmc$rho - control_mcmc$rho < -.1 ))
#12% chance that filler scales have no effect defined by r = [-.10, .10]. 

#Scale Randomization
control_mcmc <- as.data.frame(b.Controlr)
ScaleRandr_mcmc <- as.data.frame(b.ScaleRandr)
hist(ScaleRandr_mcmc$rho - control_mcmc$rho, 30, xlim = c(-1, 1), main = "Difference in PP-OCBI r Between Control and Scale-Randomized Scales", yaxt = "n")
1-(mean(ScaleRandr_mcmc$rho - control_mcmc$rho > .1 )+mean(ScaleRandr_mcmc$rho - control_mcmc$rho < -.1 ))
#40% chance that filler scales have no effect defined by r = [-.10, .10]. 

##Note for Wayne: Text above should be replicated for the other intercorrelations other than PP-OCBI (e.g., PP-OCBO, PP-IRB, etc.).
#Plot differences for PP-OCBO relationship
```

The results for the default two-sided test can be obtained as follows: 
```{r}
bf01Study1 <- 1/bf10JeffreysIntegrate(n=nStudy1, r=rStudy1)
bf01Study1
```

The results for the default one-sided test can be obtained as follows: 
```{r}
bf0PlusStudy1 <- 1/bfPlus0JeffreysIntegrate(n=nStudy1, r=rStudy1)
bf0PlusStudy1
```

For the replication Bayes factor, the prior for the correlation coefficient is taken to be the posterior based on the original research. Here we use Study 1a by Bargh and Shalev (2012), which yielded $r=.57$ with $N=51$. The replication Bayes factor can then be obtained as follows:
```{r}
rBargh <- .57
nBargh <- 51

bf0RStudy1 <- 1/repBfR0(nOri=nBargh, rOri=rBargh, nRep=nStudy1, rRep=rStudy1)
bf0RStudy1
```

We turn to the sensitivity analysis, which we illustrate for the case of the two-sided test. As outlined in the main text, the sensitivity involves a stretched beta with parameter $\gamma = 1/a$; the setting $\gamma=1$ corresponds to the default uniform prior, whereas $\gamma = 0$ means that $\mathcal{H}_1$ equals $\mathcal{H}_0$. The code below executes the sensitivity analysis and plots the results:

```{r}
gammaLength <- 36

myGamma <- makeGammas(gammaLength)
bf01Sense <- 1/bf10JeffreysIntegrate(n=nStudy1, r=rStudy1, alpha=1/myGamma)

plotSensitivity(bf01Sense, myGamma)
```

In order to understand better what prior distributions correspond to a stretched beta with parameter $\gamma$, the code below produces a figure that shows the prior distribution. 

```{r}
someGammas <- c(1, 1/2, 1/3, 1/25)

plotRhoPrior(someGammas)
```

For $\gamma=1$ we have the following priors and posteriors:

```{r}
gamma <- 1

plotRhoPriorPosterior(n=nStudy1, r=rStudy1, alpha=1/gamma, yLim=6)
```

For $\gamma=1/2$ we have the following priors and posteriors:

```{r}
gamma <- 1/2

plotRhoPriorPosterior(n=nStudy1, r=rStudy1, alpha=1/gamma, yLim=6)

```

For $\gamma=1/3$ we have the following priors and posteriors:

```{r}
gamma <- 1/3

plotRhoPriorPosterior(n=nStudy1, r=rStudy1, alpha=1/gamma, yLim=6)
```

For $\gamma=1/10$ we have the following priors and posteriors:

```{r}
gamma <- 1/80

plotRhoPriorPosterior(n=nStudy1, r=rStudy1, alpha=1/gamma, yLim=7)
```




```{r}
#Note: Possibly change the HITID
#b <- "Our systems flagged your HIT and may be rejected unless you take action. Individuals are flagged when, after completing our survey, they provide a confirmation code to us that was not assigned to a survey taker which can occur when (1) individuals incorrectly provide a confirmation code that was never issued or (2) individuals fabricate a confirmation code. To determine if you incorrectly provided a confirmation code that was never issued, which will not result in a rejection, we will need to examine our dataset to find the confirmation code that was most probably assigned to your survey. Therefore, we will need the following information from you: (1) your gender, (2) your age, (3) your job title, and (4) the number of years worked at your current job. We will not share this information. It is being asked for only to identify your data and confirmation code. If we can identify your data, then your HIT will not be rejected. However, if you did provide a confirmation code to us that was fabricated, then we will reject your HIT unless you notify us that you wish to take our survey in earnest. Then we will provide you with access to our survey. Related to this point, if you did take part in our survey but were screened out, then we can provide you with access to the portion of your survey that was flagged for inattentive responding and give you an opportunity to complete your survey in earnest. We will need you to provide us with your gender, age, job title, and the number of years worked so we can identify your survey and allow you to complete it. Please send all information to the lead researcher's email address (christopher.castille@nicholls.edu) along with any questions that you may have. We want to resolve this matter in ways that benefit all workers and allow fair treatment. Thank you for your time and effort with regard to our project."

#Note: Message to clarify HIT.
#m <- "Possible Solution to HIT (HITId: 3L4YG5VW9NSSWYQBDZ8P84QJT1KDD5) - Please Take Action"
#n <- "We previously notified you regarding an issue with your HIT. We may have come across a solution to this problem that is both simple and quick to do. When you received your confirmation code, you may not have clicked on the 'Next Page' button, which would both submit your results and allow me to verify your confirmation code. I suspect that this is the problem for the majority of individuals who received a message from me previously. To see if this applies to you, simply revisit the survey via this link: https://rutgers.qualtrics.com/SE/?SID=SV_4TrJYU5QFy0nGPX. After you click on the link, you should see the last page you were on, which I suspect is the confirmation code page. To the lower right is the 'Next Page' button. Click on that and you should be told that the survey has been submitted. Once you do that, email me to let me know that you completed this task so I can verify your results. Now, a few of you did not enter a confirmation code and, if this is you, then please visit the survey link (https://rutgers.qualtrics.com/SE/?SID=SV_4TrJYU5QFy0nGPX), obtain your code, and email it to me directly. I'll manually change your data so there is no issue. Again, thank you for your time and effort with regard to our project. It is greatly appreciated."

# contact multiple workers in batch. Change the worker IDs. Workers contacted are: "A47UVNSR0PBHV","A3P7AXWF57BHNX","ANPGL7XW7U766", "A3TUCOUVSP9ZGY", "AH5HMXXK35KZV".
##Previously messaged workers.
#c1 <- as.data.frame(c("A47UVNSR0PBHV","A3P7AXWF57BHNX","ANPGL7XW7U766", "A3TUCOUVSP9ZGY", "AH5HMXXK35KZV"))
#names(c1) <- c("WorkerID")

#Workers who must be messaged.
#c2 <- as.character(false.code$WorkerID)
#names(c2) <- c("WorkerID")
#c3 <- c2[12:28,]
#c3 <- rbind(c1, c2)
#c4 <- c3[!duplicated(c3), ]
#c4 <- as.character(c4)
#Delete cases.A3P7AXWF57BHNX = 2
#c4 <- as.character(c4[-2])
#c5 <- as.character(c4[6:47,])
#names(c5) <- c("WorkerID")
##Mssage workers. BE CAREFUL.
#e <- ContactWorker(subjects = a,
#                   msgs = b,
#                   workers = c2,
#                   batch = FALSE)

#Rejection materials
#assignments <- as.data.frame(as.character(false.code$AssignmentID))
#names(assignments) <- c("AssignmentID")
#feedback <- "Your HIT has been rejected because the confirmation code that you provided could not be verified. If you believe that you received this message in error, then please contact the requester. We will reverse rejections for individuals who can provide evidence that they completed our HIT."

#Reject: Unfortunately, it appears that these can only be done one at a time...
#reject(assignments="32AT8R96GL9M29H8MMY51NCRATPSU1", feedback=feedback)

#Pay workers.
##Subset non-rejected workers.
good <- submitted[complete.cases(submitted),]

ApproveAssignments(assignments = good$AssignmentID)
```

#Introduction
  Few methodological problems have been discussed more frequently than the presence and impact of method rvariance (Spector, Rosen, Richardson, Williams, & Johnson 2016), or variation in observations attributable to methodolgoical sources than to substantive/theoretical constructs of interest (Campbell & Fiske, 1959). Though long debated over the past several decades (e.g., Campbell & Fiske, 1959; Lance & Simonovsky, 2015; Richardson, Simmering, & Sturman, 2009; Spector, 2006), the problem of method variance is often viewed as a serious one in the organizational sciences, where researchers often rely upon observations made using sources believed to share common method variance (i.e., variation in observations that is attributable to a common cause, such mood held by respondents; see Podsakoff et al., 2003). Those who believe strongly that CMV is persistent problem believe that method variance threatens the substantive knowledgebase upon which practitioners rely, and so many procedural solutions have been proposed to address this eminent problem (Podsakoff, MacKenzie, Lee, & Podsakoff, 2003; Podsakoff, MacKenzie, & Podsakoff, 2012). The proposed solutions are often simple (e.g., use a cover story to disguise the purpose of a study, counterbalance item presentation, and separate measurement by an interval of time) and routinely encouraged by journal editors (e.g., Ashkanasy, 2009).   
  Unfortunately, these procedural method variance remedies have not received close examination, particularly using experimental designs that compare observations obtained using these remedies to those obtained without. These experiments are needed because they can demonstrate the extent to which a remedy addresses a proposed cause of method variance (e.g., participant beliefs about item covariation, respondent mood), resulting in reduced method variance and less biased results. Therefore, we set out to conduct these experiments and to this end we make the following contributions to the literature. First, we test the causal role of hypothesized proximal 8causes of method variance, specifically resondent consistency motifs, item context effects, and mood. Second, drawing on recent literature distinguishing between common and uncommon method variance (see Spector et al., 2016), we demonstrate the extent to which these different sources of method variance differentially affect observations, affecting reserachers' ability to produce accurate effect size estimates. Thirdly, we explore causes of method variance that are specific to the substantive theory under investigation in our study, allowing a comparison of general and specific causes of method variance. 
##Theoretical Overview of Method Variance
	Before we dive too deeply into the literature on method bias, we should clarify the terms 'method,' 'method variance,' and 'method bias,' as there are differing perspectives. Summarizing the available literature, Podsakoff et al. (2012) distinguished between a broad and narrow definition. Drawing on Fisk (1982) and citing several researchers who support this position (e.g., Johnson et al., 2011; Edwards, 2008; Siemsen et al., 2010; Weijters et al., 2010), Podsakoff et al. noted that method encompasses several abstract elements (e.g., taking a paper-and-pencil instrument, responding using Likert scales, characteristics of the examiner) and when these elements are shared across methods or measures in the same investigation, there will be a convergence resulting in bias. By contrast, Lance et al. (2009, 2010) restricted method to aspects that refer to alternative means of enumerating observations to indicate standings on latent traits (e.g., self- vs. other-report), which has been critized as omitting common rater sources of method variance (e.g., consistency motifs and illusory rater correlations; see Podsakoff et al., 2012). Method variance refers to the impact of a methodological aspect of study on a particular observation(s) (e.g.,  mood affecting observations obtained with a single-source single time-point design), which differs from the extent to which method variance is shared across methods, causing biased/inflated parameter estimates (e.g., negative mood both contaminating observations of two variables, inflating their covariances; see Williams & Anderson, 1994). This distinction is important because method variance can be unique to a specific set of observations within a dataset (e.g., halo effects contaminate supervisor appraisals of employee performance, but not subordinate variables). When such uncommon method variance is present in one's observations, observed covariances with other observations are attentuated (see Spector et al., 2016).
	Conventionally, method variance is a concern when researchers's conclusions dervie from self-report surveys administered at a single time-point because it is believed that methods affect the same item response process. Therefore, it is important to understand how methods affect the underlying item response process resulting in contaminated observations. Research from cognitive psychology suggests that four cognitive processes are associated with responding to survey items (see Tourangeau et al., 2000): (1) comprehension (i.e., understanding items), (2) retrieval (i.e., recalling item-relevant memories), (3) judgment (i.e., assessing the completeness of a relevant memory), and (4) response (i.e., mapping one's judgement onto the available item responses). Podsakoff et al. (2003) later divided the last process into response selection and (5) response reporting to highlight how individuals might modify their responses to adhere to certain criteria (e.g., adjusting previous item responses to appear more consistent). Podsakoff et al. (2003) summarized the method variance literature into four broad potential sources of common method bias (i.e., common rater effects, item characteristic effects, item context effeccts, and measurement context effects) to explain how response processes are corrupted, resulting in contaminated observations. 
	In our manuscript, we examine the role of a few sources identified by Podsakoff et al. (2003) that are believed to play a role in all same-source surveys. These sources of method variance are referred to as proximal causes of method variance. In two studies, we examine the role of a specific common rater effects (i.e., implicit theories, consistency motifs, mood states, trait affectivity factors), item context effects (i.e., item/scale priming and item/scale embeddedness effects), and measurement context effects (i.e., gathering observations on predictor and criterion variables at the same point in time vs a differnt point in time), which are believed to corrupt the item response process, contaminate observations, and ultimately bias estimates of covariation. We have decided to focus on proximal causes of method variance for single-source designs because these designs are likely to be the most prevalent in the organizational and social science (applied and academic settings) due to their relatively lower cost and reduced administrative burden. As such, ensuring that our remedies work is paramount to helping researchers make the most of their research designs. Furthermore, single-source designs should be ripe for method variance because the cognitive processes of retrieval (i.e., remembering responses to previously answered items rather than the whole of one's experience), judgment (i.e., falsely judging that one has effectively retrieved relevant memories), and in some instances reporting (i.e., modifying one's responses to be consistent) may not be addressed unless a remedy is applied (see Tourangeau et al., 2000; Podsakoff et al., 2003, 2012). 
	
	Question: Why not examine response style indicators as well (e.g., acquiescence, disacquiescence, intra-individual response consistency)? More recently, researchers have examined the stability of response styles, or the preference for particular response categories, which reflects a clear consistency motif. In an 8-year ongoing panel study, Wetzel et al. (2015) found that between 49 and 59% of the variance in state response style factors could be explained by trait response style factors, indicating a remarkable amount of response consistency across time and also raising questions about their state-like properties. Answer: Don't bring this up unless reviewers ask you to look into this. 
	Concern: Be sure to maintain a distinction between "causes" and "effects" when writing. Others don't do this, but we should be more responsible.
		
###Relevant Common Rater Effects  
	Common rater effects are any artifactual covariation introduced by gathering observations on (or from) the same rater (Podsakoff et al., 2003). Though there are many common rater effects (see Podsakoff et al., 2003), there are four that are particularly relevant for our study: consistency motifs, implicit theories, mood states, and trait affectivity factors. Consistency motifs refer to the propensity for respondents to maintain consistency in their responses to questions, which social psychologists suggest are produced by a basic human drive to appear consistent (Cialdini, 2009). Clearly, the drive to appear consistent should affect response reporting, as individuals will desire to appear rational (Podsakoff et al., 2003). Early research suggested that some individuals edit their responses to appear more consistent (Smith, 1983) or avoid inconsistencies (McGuire, 1960). Later research revealed consistency motifs might bias estimates of covariation (Harrison, McLaughlin, & Coalter, 1996). Similarly, implicit theories or illusory correlations refer to respondents beliefs about the covariation among particular traits, behaviors, and or outcomes that may not accurately reflect reality (Sternberg, Conway, Ketron, & Bernstein, 1981). These biases are believed to affect judgments of response appriopriateness (Podsakoff et al., 2003).
	To address consistency motifs and implicit theories, researchers have used cover stories (e.g., Harrison et al., 1996), which are narratives designed to both disguise the hypotheses under investigation and dispel implicit theories held by the participants (Podsakoff et al., 2003). Importantly, there is no experimental research evaluating this procedural remedy for these sources of method variance. 
	Somewhat differntly from consistency motifs and implicit theories are mood states and trait affectivity factors, which are clearly interrelated. Mood states refer to respondents momentary or brief mood state. A great deal of research indicates that one's emotional state influences the contents of what is recalled from memory by priming similarly valenced material stored in memory (cf. Blaney, 1986; Blower, 1981; Isen & Baron, 1991; Parrott & Sabini, 1990). Sometimes this has a desirable effect, such as when mood-congruency facilitates recall (Bower, 1981; Isen & Baron, 1991). However, when individuals are highly aroused, errors in recall seem more likley (Carson & Verrier, 2007). One's mood state should also be related to general affectivity (positive and negative), which refers to a respondent's propensity to view themselves or their environments in positive or negative ways (Podsakoff et al., 2003). Trait affectivity should exert similar effects on the item response process as mood, and may explain the effect of momentary mood state on the item response process. Brief, Burke, George, Robinson, and Webster (1988) observed that negative affectivity contaminates observations of stress, job and life satisfaction, depression, and the amount of affect experienced at work, resulting in biased estimates of covariation. Similarly, Williams and Anderson (1994) found that positive emotionality (analogous to positive affectivity) contaminates self-report measurements of several commonly studied organizational variables (sp. job satisfaction, organizational commitment, leader-contingent reward behavior, and job characteristics). Additionally, they found that negative emotionality (alaogous to negative affectivity) played a weaker role, only affecting  observations of job satisfaction. Similarly, Chen and Spector (1991) and Jex and Spector (1996) observed weak influences of negative affectivity on the relationships between self-reported job stress and strain variables. In short, both trait affectivity and momentary mood states may be a common source of method variance.
	To address the role of mood states, researchers have proposed using a temporal separation of measurement that increases the amount of time between when observations of  predictor and criterion variables are made (Podsakoff et al., 2003; 2012). Though it involves separation by time, this remedy addresses a proximal cause of method variance (i.e., mood state). It is assumed that introducing a temporal separation (e.g., separating measures by one week) will reduce the systematic influence of momentary mood states, resulting in estimates that are less baised. Again, there is no research examining the efficacy of this remedy.
	
	Note: Reviewers might point out that social desirability would be a better control variable than positive or negative affectivity for our substnative measurement model because the factors that we have measured are all socially desirable. However, we would like to point out that social desirabilty reflects a personality disposition high in need for approval that would become activated in contexts where there are clear cues/signals that changes in item response behavior to appear socially desirable would be desired/rewarded. We took efforts to ensure that no such cues were present in both of our conditions. Rather, trait affectivity would relate to cognitive biases (e.g., self-However, this trait could be relevant in settings where the need for approval could be motivated, resulting in changes to item response behavior (e.g., high-stakes employee selection contexts).
	  
###Relevant Context Effects	
	Context effects refer to any artifactual covariation among observations caused by an item or scale’s location or relationship to other items or scales in a survey (e.g., item/scale priming and item/scale embeddedness effects). Podakoff et al. (2003) proposed that item context can affect retrieval and judgments of response appropriateness. Though these effects are elusive (i.e., they can be difficult to predict and control a prioi; see Tourangeau & Rasinski, 1988), Tourangeau, Rasinski, and D’Andrade (1991) demonstrated that participants respond more quickly to similar items placed closer together rather than further apart, suggesting that prior responses were more easily accessible in short-term memory. Tourangeau et al.’s (1991) findings also suggest that item intercorrelations may be a function of the consistent ordering of items within a particular instrument. If so, then participants observed data may artificially discriminate on a latent trait. Examining this possibility, Steinberg (1994) found that one item in a 20-item scale became slightly more discriminating when it was presented later in the scale rather than as the first item, which Steinberg attributed to the increase in self-awareness that occurs when individuals are asked to repeatedly reflect on their own tendencies. Additionally, Harrison et al. (1996) found that context effects produced by scale location and scale valence (negative scales presented before positive scales) affected psychometric properties such that scales presented later were less reliable. Further, when negative scales were presented first, estimated inter-scale correlations were much higher, suggesting the presence of scale embeddedness effects. 
	To address item priming and item embeddedness effects, researchers have proposed counterbalancing or randomizing the order of item and/or scales within a survey, particular around a filler scale or series of filler scales (Podakoff et al., 2003; Salancik & Pfeffer, 1977). It is assumed that this remedy will reduce the systematic influence of context effects that might introduce common method variance across predictors and criteria, resulting in estimates of scale covariation that are less biased by context. Again, there is no research examining the efficacy of this remedy.

  Note: Make sure to consistently use the disctinction between scale and item covariation. 
  
###Measurement-Specific Causes of Both Common and Uncommon Method Variance
  Advocating a more nuanced view of common method variance, Spector et al. (2016) suggest that certain causes of method variance are specific to the measurement strategy employed by researchers. For instance, when gathering observations on both affective/attitudinal and behavioral factors, Spector et al. encouraged researchers to consider ways to capture the mood state exhibited by their participates, as this would be a likely common cause of method variance that would contaminate the response process underlying both sets of observations, resulting in biased estimates of covariation. Addtionally, there may be sources of method variance that are uncommon (i.e., unique) to particular methods of observation. For instance, affective/attitudinal measures might be more strongly affected by response sets (e.g., consistency motifs) than behavioral measures, which would be affected by impression management strategies. Such effects would attenuate scale covariances unless controlled (see Spector et al., 2016). Addressing these sources of variance requires statistical strategies (e.g., measured cause models; see Williams et al., 2010).
  In line with Spector et al.'s call for controlling both common and uncommon method variance, we built in measures of method variance causes that were shared across our measures and unique to specific mesures. In both of our studies, we used the same measurement model and tested the same underlying theory; namely, that proactive personality relates positively to proactive behaviors (i.e., voice and taking charge) as well as behaviors reflecting task performance (i.e., in-role behavior) and contextual performance (i.e., OCB) (see Figure 1). We decided to study these predictor-criterion relationships for three reasons that are important for both organizational scholars and method variance testing in general. First, these relationships are often described in the literature (e.g., Fuller & Marler, 2009; Grant, Gino, & Hoffman, 2011), suggesting that assuming a relationship is important to a broad audience of organizational scholars. Second, prior research suggests that these relationships may be a function of method variance (e.g., Fuller & Marler, 2009). For instance, a meta-analysis by Fuller and Marler (2009) demonstrated that these relationships are often inflated by common method variance, particularly when observations are gathered using the same source rather than multiple sources (see Table 1). They observed that the relationships between proactive personality and workplace behaviors was substantially inflated by between 129% and 308% when observations were made using a common (rather than independent) source. Though researchers have suggested that such percentages mistate the impact of method variance (e.g., Lance & Simonovsky, 2015; Spector et al., 2016), it seems clear that methodological differences are resulting and diverging estimates. Third, and most importantly, it is not clear what sources of method variance would explain such these differences. To respolve these disparities, we tested the same measurement model in our two studies, examining the effects of various hypothesized common method factors (e.g., mood, affectivity, and consistency motifs) and uncommon method factors (e.g., negative item wording). Figure b illustrates how the method factors were modeled.
  
  (note: Readers may have noted an apparent disparity between Podsakoff et al. (2003) and Spector et al. (2016) on the role of consistency motifs. Spector et al. (2016) suggest this factor may be more relevant for affective/attitudinal factors than behavioral, whereas Podsakoff et al. (2003) suggests that consistency motifs could be equally relevant for items/scales within a measure). 
  
###General Purpose
  In two experiments, we examine the efficacy of procedural remedies for different proximal causes of common method variance while also examining the role played by other proximal causes of method variance that are specific to our measures. In both studies, we randomly assigned individuals to one of two conditions where remedies for different proximal causes of CMV have been applied or have not been applied. This allows us to test the causal role of hypothesized mechanisms that give rise method variance and also to test the extent to which these causes produce biased estimates.

##Study 1 - The Role of Consistency Motifs, Implicit Theories, and Context Effects 
  In study 1, we examined the role of consistency motifs, implicit theories, and context effects. More specifically, we tested whether using a cover story and randomzing items and scales around a filler scale worked (i.e., reduced the presence of method variance). Participants were randomly assigned to a condition where observations were obtained with or without these remedies. For our non-remedied (i.e., control) condition, an online self-reported survey was designed such that all items and scales appeared in the same order, but were separated by different webpages. This is a conventional kind of survey design likely in use by many organizations. For our remedied condition, we used a cover story that was designed to both blind participants to the purpose of our study and dispel implicit theories regarding the substantive intention guiding a study such as ours: test for positive relationships linking proactive personality to workplace behaviors. Also, the presentation of items within scales and also the whole scales were randomized around a series of filler scale consisting of measured method effect factors. We provide more detail for these remedies below.
#Methods
##Sample and Procedure
```{r include=FALSE}
require("foreign")
require("car")
require("lordif")
require("mirt")
require("memisc")
require("dplyr")
set.seed(19)

#PCProximal.spss <- read.spss("C:\\Users\\ccastille\\iCloudDrive\\Documents\\Research\\Common Method Variance\\R Analyses\\Proactivity at Work Survey - Item Counterbalancing - Copy_October 18, 2016_09.25.sav", to.data.frame=TRUE, use.value.labels=FALSE)

#MAC
Proximal.spss <- read.spss("/Users/ccasti02/Documents/Research/Common Method Variance/R Analyses/Proactivity at Work Survey - Item Counterbalancing - Copy_October 18, 2016_09.25.sav", to.data.frame=TRUE, use.value.labels=FALSE)
data <- as.data.frame(Proximal.spss)
```
Six hundred and twenty-one workers from Amazon's Mechanical Turk were paid $1.30 for completing a survey, of which 556 agreed to participate in our study after reading our informed consent form. 
```{r}
nrow(data)
count(data, Q42) # "1" refers to agreeing to participate in the non-remedied condition. 
count(data, Q44) # "1" refers to agreeing to participate in the remedied condition.
#Look for values of "1": 274 + 282 = 556.
```
These 556 participants were randomly assigned to one of two conditions that began with the content in their informed consent form. In the control condition (n = 274), participants were given a message making the purpose of the study transparent:
The purpose of this study is to test for relationships between proactive personality and workplace behaviors (including taking charge at work, having a voice in the workplace, organizational citizenship behavior at work, and job performance). 
These participants were also given a survey with all items and scales presented in the same order (demographics were presented last). 
In the experimental condition where the proximal separation of measurement remedy was used (n = 282), participants were given a cover story designed to disguise the purpose of the study:
In this study, you will be asked to respond to statements about yourself and how you behave at work. Separate researchers built the content of this survey for separate purposes, and so the questions may or may not relate to one another. As such, there are no right or wrong answers, so please provide honest responses.
Following this manipulation, we asked our participants to respond to the following item indicating whether they understood the purpose of our study: "Before you take our survey, please tell us which of the following correctly describes the purpose of this study." Three response options were given: (a) "The purpose of this study is to test for relationships between personality and workplace behaviors. As such, there is a clear purpose to the study." (b) "Separate researchers built the content of this survey for separate purposes, and so the questions may or may not relate to one another. As such, there is no single clear purpose to this study." and (c) "The purpose of this study is to measure emotional intelligence and workplace behaviors." Individuals in the control condition who selected "a" were allowed to participate whereas individuals in the treatment condition who select "b" were allowed to participate. 
  In addition to the use of a cover story, two blocks of scales were created, one for the proactive personality and another for the remaining criteria scales. We decided to separate proactive personality from the criteria scales to minimize the shared impact of context effects on these measures. These two blocks were then randomized around a set of filler scales that included (1) positive attitudes toward the color blue (4 items), (2) positive brand label attitudes (5 items), (3) the positive and negative affective schedule (20 items), (4) momentary mood (1 item), a measure of survey enjoyment and value (6 items), and two psychometric synonyms and six psychometric antonyms. Items within all scales were also randomized. All participants, regardless of their assigned condition, completed the demographic questionnaire. 
  Interestingly, while our remedy for context effects was designed to reduce the impact of serial order effects across both the predictor and criteria measures, by combining all criteria into the same survey block or webpage, we introduced uncommon method variance attributable to items being presented on the same page/in the same block (hence, the "common block factor" in Figure 1b) (see also Weijters, Beuckelaer, & Baumgartner, 2014). Research suggests that such uncommon method variance would attenuate estimates of predictor-criterion covariances unless controlled (Spector et al., 2016). Therefore, we modeled this source of method variance using the unmeasured latent method construct technique (ULMC; see Williams, Cote, & Buckley, 1989). 
##Measures
```{r echo=FALSE}
#Data Scrubbing Block.
#Select those who agreed to participate. Q42=1 OR Q44=1. 
Agree <- filter(data, Q42 %in% c("1") & Q47 %in% c("1") | Q44 %in% c("1") & Q79 %in% c("2"))
#View(Agree)

#Filter in attentive responders.
Inattent <- filter(Agree, Q26_11 == "1" | Q105_11 ==  "1")
#View(Inattent)

#Setup demographics.
#Rename variables.
Inattent$COND <- Inattent$Q42 #"1" means they have been sent to the "control/non-remedied"" condition, whereas "0" means they have been sent to the "treatment/remedied condition'.
Inattent$AGE <- Inattent$Q33
Inattent$RACE <- Inattent$Q34
Inattent$GENDER <- Inattent$Q35
Inattent$EDUCAT <- Inattent$Q36.0
Inattent$EMPLOYED <- Inattent$Q37
Inattent$JOBTENURE <- Inattent$Q38
Inattent$USREGION <- Inattent$Q40
Inattent$PROFESSION <- Inattent$Q39
Inattent$PROF_SPECIFY <- Inattent$Q39_TEXT

#Delete old dems.
Inattent$Q42 <- NULL
Inattent$Q33 <- NULL
Inattent$Q34 <- NULL
Inattent$Q35 <- NULL
Inattent$Q36.0 <- NULL
Inattent$Q37 <- NULL
Inattent$Q38 <- NULL
Inattent$Q40 <- NULL
Inattent$Q39 <- NULL
Inattent$Q39_TEXT <- NULL

#Then, combine data from separate surveys in order to form the final cleaned and well-structured dataset. Note: there were two surveys setup on Qualtrics, hence why data needs to be combined. You'll see redundant items moving forward.
##Convert NAs to 0 to allow merging. Later, you'll need to reasign NAs to conduct a missing data analysis. 
Inattent[is.na(Inattent)] <- 0

#View(Inattent)
#Proactive personality
Inattent$PP1	<-	Inattent$Q26_1	+Inattent$Q105_1
Inattent$PP2	<-	Inattent$Q26_2	+Inattent$Q105_2
Inattent$PP3	<-	Inattent$Q26_3	+Inattent$Q105_3
Inattent$PP4	<-	Inattent$Q26_4	+Inattent$Q105_4
Inattent$PP5	<-	Inattent$Q26_5	+Inattent$Q105_5
Inattent$PP6	<-	Inattent$Q26_6	+Inattent$Q105_6
Inattent$PP7	<-	Inattent$Q26_7	+Inattent$Q105_7
Inattent$PP8	<-	Inattent$Q26_8	+Inattent$Q105_8
Inattent$PP9	<-	Inattent$Q26_9	+Inattent$Q105_9
Inattent$PP10	<-	Inattent$Q26_10	+Inattent$Q105_10

#Voice
Inattent$VC1	<-	Inattent$Q27_1	+	Inattent$Q106_1
Inattent$VC2	<-	Inattent$Q27_2	+	Inattent$Q106_2
Inattent$VC3	<-	Inattent$Q27_3	+	Inattent$Q106_3
Inattent$VC4	<-	Inattent$Q27_4	+	Inattent$Q106_4
Inattent$VC5	<-	Inattent$Q27_5	+	Inattent$Q106_5
Inattent$VC6	<-	Inattent$Q27_6	+	Inattent$Q106_6

#Taking Charge
Inattent$TC1	<-	Inattent$Q28_1	+	Inattent$Q106_8
Inattent$TC2	<-	Inattent$Q28_2	+	Inattent$Q106_9
Inattent$TC3	<-	Inattent$Q28_3	+	Inattent$Q106_10
Inattent$TC4	<-	Inattent$Q28_4	+	Inattent$Q106_11
Inattent$TC5	<-	Inattent$Q28_5	+	Inattent$Q106_12
Inattent$TC6	<-	Inattent$Q28_6	+	Inattent$Q106_13
Inattent$TC7	<-	Inattent$Q28_7	+	Inattent$Q106_14
Inattent$TC8	<-	Inattent$Q28_8	+	Inattent$Q106_15
Inattent$TC9	<-	Inattent$Q28_9	+	Inattent$Q106_16
Inattent$TC10	<-	Inattent$Q28_10	+	Inattent$Q106_17

#OCBI
Inattent$OCBI1	<-	Inattent$Q29_1	+	Inattent$Q106_19
Inattent$OCBI2	<-	Inattent$Q29_2	+	Inattent$Q106_20
Inattent$OCBI3  <-	Inattent$Q29_3	+	Inattent$Q106_21
Inattent$OCBI4	<-	Inattent$Q29_4	+	Inattent$Q106_22
Inattent$OCBI5	<-	Inattent$Q29_5	+	Inattent$Q106_23
Inattent$OCBI6	<-	Inattent$Q29_6	+	Inattent$Q106_24
Inattent$OCBI7	<-	Inattent$Q29_7	+	Inattent$Q106_25

#OCBO
Inattent$OCBO1	<-	Inattent$Q30_1	+	Inattent$Q106_27
Inattent$OCBO2	<-	Inattent$Q30_2	+	Inattent$Q106_28
Inattent$OCBO3	<-	Inattent$Q30_3	+	Inattent$Q106_29
Inattent$OCBO4	<-	Inattent$Q30_4	+	Inattent$Q106_30
Inattent$OCBO5	<-	Inattent$Q30_5	+	Inattent$Q106_31
Inattent$OCBO6	<-	Inattent$Q30_6	+	Inattent$Q106_32
Inattent$OCBO7	<-	Inattent$Q30_7	+	Inattent$Q106_33

#IRB
Inattent$IRB1	<-	Inattent$Q31_1	+	Inattent$Q106_35
Inattent$IRB2	<-	Inattent$Q31_2	+	Inattent$Q106_36
Inattent$IRB3	<-	Inattent$Q31_3	+	Inattent$Q106_37
Inattent$IRB4	<-	Inattent$Q31_4	+	Inattent$Q106_38
Inattent$IRB5	<-	Inattent$Q31_5	+	Inattent$Q106_39
Inattent$IRB6	<-	Inattent$Q31_6	+	Inattent$Q106_40
Inattent$IRB7	<-	Inattent$Q31_7	+	Inattent$Q106_41

require(car)
#Consistency Motif
#"I am a brave person"
Inattent$CM1a <- Inattent$Q27_7 + Inattent$Q106_7 
#"I am a courageous person"
Inattent$CM1b <- Inattent$Q28_11 + Inattent$Q106_18
Inattent$CM1 <- Inattent$CM1a - Inattent$CM1b
Inattent$CM1 <- abs(Inattent$CM1)
Inattent$CM1a <- NULL
Inattent$CM1b <- NULL
"I am a talkative person"
Inattent$CM2a <- Inattent$Q29_8 + Inattent$Q106_26
#"I am a silent person"
Inattent$Q30_8r <- recode(Inattent$Q30_8, '1=5; 2=4; 4=2; 5=1')
Inattent$Q106_34r <- recode(Inattent$Q106_34, '1=5; 2=4; 4=2; 5=1')
Inattent$CM2b <- Inattent$Q30_8r + Inattent$Q106_34r
Inattent$CM2 <- Inattent$CM2a - Inattent$CM2b
Inattent$CM2 <- abs(Inattent$CM2)
Inattent$CM2a <- NULL
Inattent$CM2b <- NULL
Inattent$Q30_8r <- NULL
Inattent$Q106_34r <- NULL
#"I am an optimistic person"
Inattent$CM3a <- Inattent$Q33_6 + Inattent$Q112_6
#"I am a pessimistic person"
Inattent$Q41_7r <- recode(Inattent$Q41_7, '1=5; 2=4; 4=2; 5=1')
Inattent$Q116_7r <- recode(Inattent$Q116_7, '1=5; 2=4; 4=2; 5=1')
Inattent$CM3b <- Inattent$Q41_7r + Inattent$Q116_7r
Inattent$CM3 <- Inattent$CM3a - Inattent$CM3b
Inattent$CM3 <- abs(Inattent$CM3)
Inattent$CM3a <- NULL
Inattent$CM3b <- NULL
Inattent$Q41_7r <- NULL
Inattent$Q116_7r <- NULL
#"I seldom feel blue."
Inattent$CM4a <- Inattent$Q31_8 + Inattent$Q106_42
 #"I often feel blue."
Inattent$Q32_5r <- recode(Inattent$Q32_5, '1=5; 2=4; 4=2; 5=1')
Inattent$Q111_5r <- recode(Inattent$Q111_5, '1=5; 2=4; 4=2; 5=1')
Inattent$CM4b <- Inattent$Q32_5r + Inattent$Q111_5r
Inattent$CM4 <- Inattent$CM4a - Inattent$CM4b
Inattent$CM4 <- abs(Inattent$CM4)
Inattent$CM4a <- NULL
Inattent$CM4b <- NULL
Inattent$Q32_5r <- NULL
Inattent$Q111_5r <- NULL
Inattent$CM <- Inattent$CM1 + Inattent$CM2 + Inattent$CM3 + Inattent$CM4

#Inattent$Q27_7<-	NULL #"I am a brave person"
#Inattent$Q106_7<- NULL
#Inattent$Q28_11<-	NULL #"I am a courageous person"
#Inattent$Q106_18<-	NULL
#Inattent$Q29_8<-	NULL #"I am a talkative person"
#Inattent$Q106_26<-	NULL
#Inattent$Q30_8<-	NULL #"I am a silent person"
#Inattent$Q106_34<-	NULL
#Inattent$Q33_6<-	NULL #"I am an optimistic person"
#Inattent$Q112_6<-	NULL
#Inattent$Q41_7 <- NULL #"I am a pessimistic person"
#Inattent$Q116_7<-	NULL
#Inattent$Q31_8<-	NULL #"I seldom feel blue."
#Inattent$Q106_42<-	NULL
#Inattent$Q32_5<-	NULL #"I often feel blue."
#Inattent$Q111_5<-	NULL

#Create Daily Mood item.
Inattent$MOOD <- Inattent$Q36 + Inattent$Q115

###Create PANAS items.
##PA
#Interested
Inattent$PA1 <- Inattent$Q34_1 + Inattent$Q113_1
#Excited
Inattent$PA2 <- Inattent$Q34_3 + Inattent$Q113_3
#Strong
Inattent$PA3 <- Inattent$Q34_5 + Inattent$Q113_5
#Enthusiastic
Inattent$PA4 <- Inattent$Q34_9 + Inattent$Q113_9
#Proud
Inattent$PA5 <- Inattent$Q34_10 + Inattent$Q113_10
#Alert
Inattent$PA6 <- Inattent$Q35_2 + Inattent$Q114_2
#Inspired
Inattent$PA7 <- Inattent$Q35_4 + Inattent$Q114_4
#Determined
Inattent$PA8 <- Inattent$Q35_6 + Inattent$Q114_6
#Attentive
Inattent$PA9 <- Inattent$Q35_7 + Inattent$Q114_7
#Active
Inattent$PA10 <- Inattent$Q35_9 + Inattent$Q114_9


##NA
#Distressed
Inattent$NA1 <- Inattent$Q34_2 + Inattent$Q113_2
#Upset
Inattent$NA2 <- Inattent$Q34_4 + Inattent$Q113_4
#Guilty
Inattent$NA3 <- Inattent$Q34_6 + Inattent$Q113_6
#Scared
Inattent$NA4 <- Inattent$Q34_7 + Inattent$Q113_7
#Hostile
Inattent$NA5 <- Inattent$Q34_8 + Inattent$Q113_8
#Irritable
Inattent$NA6 <- Inattent$Q35_1 + Inattent$Q114_1
#Ashamed
Inattent$NA7 <- Inattent$Q35_3 + Inattent$Q114_3
#Nervous
Inattent$NA8 <- Inattent$Q35_5 + Inattent$Q114_5
#Jittery
Inattent$NA9 <- Inattent$Q35_8 + Inattent$Q114_8
#Afraid
Inattent$NA10 <- Inattent$Q35_10 + Inattent$Q114_10

	
#Delete PANAS terms
Inattent$Q34_1<-	NULL
Inattent$Q34_2<-	NULL
Inattent$Q34_3<-	NULL
Inattent$Q34_4<-	NULL
Inattent$Q34_5<-	NULL
Inattent$Q34_6<-	NULL
Inattent$Q34_7<-	NULL
Inattent$Q34_8<-	NULL
Inattent$Q34_9<-	NULL
Inattent$Q34_10<-	NULL
Inattent$Q35_1<-	NULL
Inattent$Q35_2<-	NULL
Inattent$Q35_3<-	NULL
Inattent$Q35_4<-	NULL
Inattent$Q35_5<-	NULL
Inattent$Q35_6<-	NULL
Inattent$Q35_7<-	NULL
Inattent$Q35_8<-	NULL
Inattent$Q35_9<-	NULL
Inattent$Q35_10<-	NULL
Inattent$Q113_1<-	NULL
Inattent$Q113_2<-	NULL
Inattent$Q113_3<-	NULL
Inattent$Q113_4<-	NULL
Inattent$Q113_5<-	NULL
Inattent$Q113_6<-	NULL
Inattent$Q113_7<-	NULL
Inattent$Q113_8<-	NULL
Inattent$Q113_9<-	NULL
Inattent$Q113_10<-	NULL
Inattent$Q114_1<-	NULL
Inattent$Q114_2<-	NULL
Inattent$Q114_3<-	NULL
Inattent$Q114_4<-	NULL
Inattent$Q114_5<-	NULL
Inattent$Q114_6<-	NULL
Inattent$Q114_7<-	NULL
Inattent$Q114_8<-	NULL
Inattent$Q114_9<-	NULL
Inattent$Q114_10<-	NULL

#Delete original variables.
Inattent$Q26_1	<-	NULL
Inattent$Q26_2	<-	NULL
Inattent$Q26_3	<-	NULL
Inattent$Q26_4	<-	NULL
Inattent$Q26_5	<-	NULL
Inattent$Q26_6	<-	NULL
Inattent$Q26_7	<-	NULL
Inattent$Q26_8	<-	NULL
Inattent$Q26_9	<-	NULL
Inattent$Q26_10	<-	NULL
Inattent$Q27_1	<-	NULL
Inattent$Q27_2	<-	NULL
Inattent$Q27_3	<-	NULL
Inattent$Q27_4	<-	NULL
Inattent$Q27_5	<-	NULL
Inattent$Q27_6	<-	NULL
Inattent$Q28_1	<-	NULL
Inattent$Q28_2	<-	NULL
Inattent$Q28_3	<-	NULL
Inattent$Q28_4	<-	NULL
Inattent$Q28_5	<-	NULL
Inattent$Q28_6	<-	NULL
Inattent$Q28_7	<-	NULL
Inattent$Q28_8	<-	NULL
Inattent$Q28_9	<-	NULL
Inattent$Q28_10	<-	NULL
Inattent$Q29_1	<-	NULL
Inattent$Q29_2	<-	NULL
Inattent$Q29_3	<-	NULL
Inattent$Q29_4	<-	NULL
Inattent$Q29_5	<-	NULL
Inattent$Q29_6	<-	NULL
Inattent$Q29_7	<-	NULL
Inattent$Q30_1	<-	NULL
Inattent$Q30_2	<-	NULL
Inattent$Q30_3	<-	NULL
Inattent$Q30_4	<-	NULL
Inattent$Q30_5	<-	NULL
Inattent$Q30_6	<-	NULL
Inattent$Q30_7	<-	NULL
Inattent$Q31_1	<-	NULL
Inattent$Q31_2	<-	NULL
Inattent$Q31_3	<-	NULL
Inattent$Q31_4	<-	NULL
Inattent$Q31_5	<-	NULL
Inattent$Q31_6	<-	NULL
Inattent$Q31_7	<-	NULL
Inattent$Q105_1	<-	NULL
Inattent$Q105_2	<-	NULL
Inattent$Q105_3	<-	NULL
Inattent$Q105_4	<-	NULL
Inattent$Q105_5	<-	NULL
Inattent$Q105_6	<-	NULL
Inattent$Q105_7	<-	NULL
Inattent$Q105_8	<-	NULL
Inattent$Q105_9	<-	NULL
Inattent$Q105_10	<-	NULL
Inattent$Q106_1	<-	NULL
Inattent$Q106_2	<-	NULL
Inattent$Q106_3	<-	NULL
Inattent$Q106_4	<-	NULL
Inattent$Q106_5	<-	NULL
Inattent$Q106_6	<-	NULL
Inattent$Q106_8	<-	NULL
Inattent$Q106_9	<-	NULL
Inattent$Q106_10	<-	NULL
Inattent$Q106_11	<-	NULL
Inattent$Q106_12	<-	NULL
Inattent$Q106_13	<-	NULL
Inattent$Q106_14	<-	NULL
Inattent$Q106_15	<-	NULL
Inattent$Q106_16	<-	NULL
Inattent$Q106_17	<-	NULL
Inattent$Q106_19	<-	NULL
Inattent$Q106_20	<-	NULL
Inattent$Q106_21	<-	NULL
Inattent$Q106_22	<-	NULL
Inattent$Q106_23	<-	NULL
Inattent$Q106_24	<-	NULL
Inattent$Q106_25	<-	NULL
Inattent$Q106_27	<-	NULL
Inattent$Q106_28	<-	NULL
Inattent$Q106_29	<-	NULL
Inattent$Q106_30	<-	NULL
Inattent$Q106_31	<-	NULL
Inattent$Q106_32	<-	NULL
Inattent$Q106_33	<-	NULL
Inattent$Q106_35	<-	NULL
Inattent$Q106_36	<-	NULL
Inattent$Q106_37	<-	NULL
Inattent$Q106_38	<-	NULL
Inattent$Q106_39	<-	NULL
Inattent$Q106_40	<-	NULL
Inattent$Q106_41	<-	NULL

#Other variables in the survey that are deleted:
##Note: There were two 
#1. Redundant condition variables
Inattent$Q47<-	NULL
Inattent$Q44<-	NULL
Inattent$Q79<-	NULL

#2. Attention checks (used and unused; the latter refer to psychometric consistency/synonym/antonym items. I chose not to use these because I believe that these can create artificial consistencies in the data).
Inattent$Q26_11<-	NULL #"Click on the first circle indicating "Strongly Disagree."
Inattent$Q105_11<-	NULL

#2b. These items might be used to capture a consistency motif.
Inattent$Q27_7<-	NULL #"I am a brave person"
Inattent$Q106_7<- NULL
Inattent$Q28_11<-	NULL #"I am a courageous person"
Inattent$Q106_18<-	NULL
Inattent$Q29_8<-	NULL #"I am a talkative person"
Inattent$Q106_26<-	NULL
Inattent$Q30_8<-	NULL #"I am a silent person"
Inattent$Q106_34<-	NULL
Inattent$Q33_6<-	NULL #"I am an optimistic person"
Inattent$Q112_6<-	NULL
Inattent$Q41_7 <- NULL #"I am a pessimistic person"
Inattent$Q116_7<-	NULL
Inattent$Q31_8<-	NULL #"I seldom feel blue."
Inattent$Q106_42<-	NULL

#3. Remove common scaling technique measured method effect factor (preference for the color blue).
Inattent$Q32_1<-	NULL #"I prefer blue to other colors."
Inattent$Q32_2<-	NULL #"I like the color blue."
Inattent$Q32_3<-	NULL #"I like blue colors."
Inattent$Q32_4<-	NULL #"I hope my next car is blue."
Inattent$Q32_5<-	NULL #"I often feel blue."
Inattent$Q111_1<-	NULL
Inattent$Q111_2<-	NULL
Inattent$Q111_3<-	NULL
Inattent$Q111_4<-	NULL
Inattent$Q111_5<-	NULL

#4. Remove common scaling technique measured method effect factor (private brand label attitudes).
Inattent$Q33_1<-	NULL #"Buying private label brands makes me feel good."
Inattent$Q33_2<-	NULL #"I love it when private label brands are..."
Inattent$Q33_3<-	NULL #"For most product categories..."
Inattent$Q33_4<-	NULL #"Considering the value for the money, I prefer..."
Inattent$Q33_5<-	NULL #"When I buy a private lable brand, I always..."
Inattent$Q112_1<-	NULL
Inattent$Q112_2<-	NULL
Inattent$Q112_3<-	NULL
Inattent$Q112_4<-	NULL
Inattent$Q112_5<-	NULL

#5. Remove PANAS items.
Inattent$Q34_1<-	NULL
Inattent$Q34_2<-	NULL
Inattent$Q34_3<-	NULL
Inattent$Q34_4<-	NULL
Inattent$Q34_5<-	NULL
Inattent$Q34_6<-	NULL
Inattent$Q34_7<-	NULL
Inattent$Q34_8<-	NULL
Inattent$Q34_9<-	NULL
Inattent$Q34_10<-	NULL
Inattent$Q35_1<-	NULL
Inattent$Q35_2<-	NULL
Inattent$Q35_3<-	NULL
Inattent$Q35_4<-	NULL
Inattent$Q35_5<-	NULL
Inattent$Q35_6<-	NULL
Inattent$Q35_7<-	NULL
Inattent$Q35_8<-	NULL
Inattent$Q35_9<-	NULL
Inattent$Q35_10<-	NULL
Inattent$Q113_1<-	NULL
Inattent$Q113_2<-	NULL
Inattent$Q113_3<-	NULL
Inattent$Q113_4<-	NULL
Inattent$Q113_5<-	NULL
Inattent$Q113_6<-	NULL
Inattent$Q113_7<-	NULL
Inattent$Q113_8<-	NULL
Inattent$Q113_9<-	NULL
Inattent$Q113_10<-	NULL
Inattent$Q114_1<-	NULL
Inattent$Q114_2<-	NULL
Inattent$Q114_3<-	NULL
Inattent$Q114_4<-	NULL
Inattent$Q114_5<-	NULL
Inattent$Q114_6<-	NULL
Inattent$Q114_7<-	NULL
Inattent$Q114_8<-	NULL
Inattent$Q114_9<-	NULL
Inattent$Q114_10<-	NULL

#6. Remove daily hedonic tone.
Inattent$Q36<-	NULL
Inattent$Q115<-	NULL

#7. Remove survey enjoyment and value measure.
Inattent$Q41_1 <-	NULL
Inattent$Q41_2<-	NULL
Inattent$Q41_3<-	NULL
Inattent$Q41_4<-	NULL
Inattent$Q41_5<-	NULL
Inattent$Q41_6<-	NULL
Inattent$Q116_1<-	NULL
Inattent$Q116_2<-	NULL
Inattent$Q116_3<-	NULL
Inattent$Q116_4<-	NULL
Inattent$Q116_5<-	NULL
Inattent$Q116_6<-	NULL

#8. Delete location data.
Inattent$LocationLatitude <- NULL
Inattent$LocationLongitude <- NULL
Inattent$LocationAccuracy <- NULL

#9. Delete confirmation code.
#Inattent$confirmation_code <- NULL

#Rename dataset
data <- Inattent

#Recode 0 values to missing.
data$PP1[data$PP1==0] <- NA
data$PP2[data$PP2==0] <- NA
data$PP3[data$PP3==0] <- NA
data$PP4[data$PP4==0] <- NA
data$PP5[data$PP5==0] <- NA
data$PP6[data$PP6==0] <- NA
data$PP7[data$PP7==0] <- NA
data$PP8[data$PP8==0] <- NA
data$PP9[data$PP9==0] <- NA
data$PP10[data$PP10==0] <- NA
data$VC1[data$VC1==0] <- NA
data$VC2[data$VC2==0] <- NA
data$VC3[data$VC3==0] <- NA
data$VC4[data$VC4==0] <- NA
data$VC5[data$VC5==0] <- NA
data$VC6[data$VC6==0] <- NA
data$TC1[data$TC1==0] <- NA
data$TC2[data$TC2==0] <- NA
data$TC3[data$TC3==0] <- NA
data$TC4[data$TC4==0] <- NA
data$TC5[data$TC5==0] <- NA
data$TC6[data$TC6==0] <- NA
data$TC7[data$TC7==0] <- NA
data$TC8[data$TC8==0] <- NA
data$TC9[data$TC9==0] <- NA
data$TC10[data$TC10==0] <- NA
data$OCBI1[data$OCBI1==0] <- NA
data$OCBI2[data$OCBI2==0] <- NA
data$OCBI3[data$OCBI3==0] <- NA
data$OCBI4[data$OCBI4==0] <- NA
data$OCBI5[data$OCBI5==0] <- NA
data$OCBI6[data$OCBI6==0] <- NA
data$OCBI7[data$OCBI7==0] <- NA
data$OCBO1[data$OCBO1==0] <- NA
data$OCBO2[data$OCBO2==0] <- NA
data$OCBO3[data$OCBO3==0] <- NA
data$OCBO4[data$OCBO4==0] <- NA
data$OCBO5[data$OCBO5==0] <- NA
data$OCBO6[data$OCBO6==0] <- NA
data$OCBO7[data$OCBO7==0] <- NA
data$IRB1[data$IRB1==0] <- NA
data$IRB2[data$IRB2==0] <- NA
data$IRB3[data$IRB3==0] <- NA
data$IRB4[data$IRB4==0] <- NA
data$IRB5[data$IRB5==0] <- NA
data$IRB6[data$IRB6==0] <- NA
data$IRB7[data$IRB7==0] <- NA

#Delete unnecessary variables
data <- data[c(-1,-2,-3,-4,-5,-7, -8,-9,-10,-11,-12,-13,-14,-15,-16,-17,-18,-19, -20, -21, -22,-23,-24,-25,-26,-27,-28,-29,-30,-31,-32,-33,-34,-35,-36)]
```
###Consistency motif
  To capture the presence of a consistency motif, we employed psychometric synonyms and antonyms that have been used previously (Goldberg & Kilkowski, 1988). Specifically we asked individuals to respond to the following items with a 5-point agreement scale: [synonyms] (1a) "I am a brave person", (1b) "I am a courageous person"; [antonyms] (2a) "I am a talkative person", (2b) "I am a silent person", (3a) " I am an optimistic person" (3b) "I am a pessimistic person", (4a) "I am seldom blue", and (4b) "I am often blue". These items were interspersed throughout the questionnaire, allowing us to capture a consistency motif across the survey. Four difference scores were calculated to capture the extent to which scores for a pair of items diverged. The overall index was scaled such that higher scores on this correspond to higher levels of inconsistent responding.
###Proactive personality
  Bateman and Crant's (1993) 10-item proactive personality scale was used to capture proactive personality. Example items include: "I am constantly on the lookout for new ways to improve," "If I see something I don't like, I fix it," and "I excel at identifying opportunities." This, and all scales included in this study, utilized a five-point agreement Likert rating scale (1 = strongly disagree, 5 = strongly agree). 

###Voice
  Van Dyne and LePine's (1998) 6-item scale, which are based on a modification of Van Dyne, Graham, and Dienesch's (1994) scale, were used. Example items include "I develop and make recommendations concerning issues that affect my work group" and "I speak up and ecnourage others in this group to get involved in issues that affect this group."

###Taking charge
  Morrison and Phelp's (1999) 10-item measure of taking charge was used for this study. Example items include "I often try to bring about improved procedures for my work unit or department" and "I often try to adopt improved procedures for doing my job."

###In-role and organizational citizenship behavior 
  Williams and Anderson's (1991) In-Role Performance Behavior (IRB) and Organizational Citizenship Behavior (OCB) scales, the latter of which includes OCBI (OCB directed at the individual) and OCBO (OCB directed at the organization, were used as dependent variables. Example items include: "I perform tasks that are expected of me" (IRB), "I take a personal interest in other employees"" (OCBI), and "I conserve and protect organizational property" (OCBO). Cronbach alphas were .82, .83, and .71 for IRB, OCBI, and OCBO, respectively.

###Data Quality
  To help ensure data quality, we used an inattentive responding check (Mead & Craig, 2012), specifically we asked individuals to "Click on the first circle indicating "Strongly Disagree?""
##Analytical Approach
  To examine the presence of method variance, we used latent variable modleing strategies proposed by method variance researchers (e.g., Williams et al., 2010; McGonagle & Williams, 2015). More specifically, we aplied the same latent variable model to both remedied and non-remedied observations seperately, following the approached used by a similar investigation (e.g., Johnson et al., 2011). A series of diifferent models were nested in the data, varying in their assumptions regarding the role played by method factors. For instance, method variance attributable to a specific cause could present equivalent effects on all measures or vary in its extent and impact across all or only a few measures. We followed guidance offered by McGonagle and Williams (2015) to conduct the series of model comparisons needed to indicate the impact of CMV and to also test for biased parameter estimates. Following convention (Williams et al., 2010; McGonagle & Williams, 2015), we used chi-square difference testing to compare our models and an alpha level of .05 was chosen for declaring models as statistically different. We carried out our analyses using 'lavaan' in R.
  Other key modeling details require explanation. Given the presence of negatively-keyed items in our measurement model, we created a negatively-keyed method factor that explained variance in the negatively-keyed items only (see Zhang & Savalei, 2015; Dalal & Carter, 2015). This negative method factor was theoretically independent from the other latent factors given the assumption that methods be uncorrelated with traits (see Conway & Lance, 2010). Also, we modeled an ULMC in our remedied condition to control for method variance shared by our criteria measures that was attributable to those items being on the same webage (see Weijters et al., 2014). Additionally, an item parceling was employed and we followed guidance by Hall, Snell, and Foust (1999). We used exploratory factor analysis to inform the construction of three to five parcels per trait These parcels were designed to combine unmodeled yet secondary influences shared amongst a set of items into the same parcel, thus increasing the accuracy of our parameter estimates (i.e., factor loadings and latent construct correlations). Data across both groups were analyzed simultaneously to create these parcels. For the present investigation, there is a legitimate concern that parceling in this way might force common method variance into residuals. For instance, residual covariation that is common to a set of items and caused by context effects would be forced into this parcel's residual, resulting in conclusions that method variance is not playing a biasing role when in fact it is. However, we urge readers to keep in mind that if this common method effect is truly common across a measure, then this common source of variance would be be roughly equally distributed across our parcels and therefore should not be forced into specific residual(s) for a measure. Our results without parceling are available upon request.
##Data screening process 
Approximately 75% (n = 204) percent of participants randomly assigned to the control condition and approximately 50% (n = 143) in the experimental condition correctly responded to the manipulation check and were allowed to participate in our study. 
```{r}
#Control
count(Agree, Q47) #"a" is "1" for this figure.
#Treatment
count(Agree, Q79) #"b" is "2" for this figure.
```
Seven more individuals were screened out for incorrectly responding to attentive responding item. 
```{r}
count(Agree, Q26_11)
count(Agree, Q105_11)
##Sum all frequencies that aren't 1 and you'll obtain a total 7.
```
Thirty five people were omitted for failing to respond to the inattentive responding check quesiton. Lastly, individuals who did not report thier demographics, who abandoned the survey, or did not complete an entire measure were deleted from the analysis (n = 6).
```{r echo=FALSE}
data <- na.omit(data) 
```
This filtering process resulted in a final sample of 299 individuals (182 in the control and 117 in the experimental condition).
```{r}
count(data, COND)
```
##Parcel Construction
```{r}
#Parceling strategy
#Following suggestions for item parceling by Hall, Snell, and Foust (1999), we used exploratory factor analysis to inform the construction of three or four parcels per factor. These parcels were designed to combine unmodeled yet secondary influences shared amongst a set of items into the same parcel, thus increasing the accuracy of our parameter estimates (i.e., factor loadings and latent construct correlations). Data across both groups were analyzed simultaneously. For the present investigation, there is a legitimate concern that parceling in this way might force common method variance into residuals. For instance, residual covariation that is common to a set of items and caused by context effects would be forced into this parcel's residual, resulting in conclusions that method variance is not playing a biasing role when in fact it is. However, we urge readers to keep in mind that if this common method effect is truly common across a measure, then this common source of variance would be be roughly equally distributed across our parcels and therefore should not be forced into specific residual(s) for a measure. For those who would, we can make our results without parceling available upon request.

# Maximum Likelihood Factor Analysis entering raw data and extracting factors, with promax rotation.
require(psych)
require(nFactors)
require(CTT)

##Proactive Personality
PP <- data[c(12:21)]
reliability(PP)
fit <- factanal(PP, 4, rotation="promax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
# plot factor 1 by factor 2 
load <- fit$loadings[,1:2] 
plot(load,type="n") # set up plot 
text(load,labels=names(PP),cex=.7) # add variable names
#Build PP parcels
data$PPp1	<-	data$PP4 + data$PP2
data$PPp2 <-  data$PP1 + data$PP10
data$PPp3 <-  data$PP3 + data$PP7 + data$PP8 + data$PP6
data$PPp4 <-  data$PP5 + data$PP9

##Voice
VC <- data[c(22:27)]
reliability(VC)
fit <- factanal(VC, 3, rotation="promax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
# plot factor 1 by factor 2 
load <- fit$loadings[,1:3] 
plot(load,type="n") # set up plot 
text(load,labels=names(VC),cex=.7) # add variable names
#Build VC parcels
data$VCp1	<-	data$VC1 + data$VC2 + data$VC6
data$VCp2 <-  data$VC3
data$VCp3 <-  data$VC5 + data$VC4

##Taking Charge
TC <- data[c(28:37)]
reliability(TC)
fit <- factanal(TC, 3, rotation="promax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
# plot factor 1 by factor 2 
load <- fit$loadings[,1:3] 
plot(load,type="n") # set up plot 
text(load,labels=names(TC),cex=.7) # add variable names
#Build TC parcels
data$TCp1	<-	data$TC3 + data$TC4 + data$TC9 + data$TC10
data$TCp2 <-  data$TC5 + data$TC6 + data$TC7 + data$TC8
data$TCp3 <-  data$TC1 + data$TC2

##OCBI
OCBI <- data[c(38:44)]
reliability(OCBI)
fit <- factanal(OCBI, 3, rotation="promax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
# plot factor 1 by factor 2 
load <- fit$loadings[,1:2] 
plot(load,type="n") # set up plot 
text(load,labels=names(OCBI),cex=.7) # add variable names
#Build OCBI parcels
data$OCBIp1	<-	data$OCBI4 + data$OCBI5 + data$OCBI6 + data$OCBI7
data$OCBIp2 <-  data$OCBI1 + data$OCBI2
data$OCBIp3 <-  data$OCBI3

##OCBO
OCBO <- data[c(45:51)]
key <- c(1,1,-1,-1,-1,1,1)
OCBOr <- reverse.code(key,OCBO)
reliability(OCBOr)
fit <- factanal(OCBO, 3, rotation="promax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
# plot factor 1 by factor 2 
load <- fit$loadings[,1:2] 
plot(load,type="n") # set up plot 
text(load,labels=names(OCBO),cex=.7) # add variable names
#Build OCBO parcels. OCBOp3 is a reverse-scored parcel and so this should be flipped before testing for equal method effects.
data$OCBOp1	<-	data$OCBO1 + data$OCBO2
data$OCBOp2 <-  data$OCBO6 + data$OCBO7
data$OCBOp3 <-  15-(data$OCBO4 + data$OCBO5 + data$OCBO3)

##IRB
IRB <- data[c(52:58)]
key <- c(1,1,1,1,1,-1,-1)
IRBr <- reverse.code(key,IRB)
reliability(IRBr)
fit <- factanal(IRB, 3, rotation="promax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
# plot factor 1 by factor 2 
load <- fit$loadings[,1:2] 
plot(load,type="n") # set up plot 
text(load,labels=names(IRB),cex=.7) # add variable names
#Build IRB parcels. IRBp2 is a reverse-scored parcel and so this should be flipped before testing for equal method effects.
data$IRBp1	<-	data$IRB1 + data$IRB2 + data$IRB3 + data$IRB4
data$IRBp2  <-  11-(data$IRB6 + data$IRB7)
data$IRBp3  <-  data$IRB5

##Positive Affectivity
PA <- data[c(65:74)]
reliability(PA)
fit <- factanal(PA, 5, rotation="promax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
# plot factor 1 by factor 2 
load <- fit$loadings[,1:2] 
plot(load,type="n") # set up plot 
text(load,labels=names(PA),cex=.7) # add variable names
#Build PA parcels
data$PAp1	<-	data$PA2 + data$PA4 + data$PA5 + data$PA10
data$PAp2  <-  data$PA9
data$PAp3  <-  data$PA6
data$PAp4  <-  data$PA1 + data$PA8 + data$PA7
data$PAp5  <-  data$PA3

##Negative Affectivity
Na <- data[c(75:84)]
reliability(Na)
fit <- factanal(Na, 5, rotation="promax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
# plot factor 1 by factor 2 
load <- fit$loadings[,1:2] 
plot(load,type="n") # set up plot 
text(load,labels=names(Na),cex=.7) # add variable names
#Build NA parcels
data$Nap1	<-	data$NA4 + data$NA10
data$Nap2  <-  data$NA2 + data$NA1
data$Nap3  <-  data$NA6 + data$NA5
data$Nap4  <-  data$NA8 + data$NA9
data$Nap5  <-  data$NA3 + data$NA7

#Subset data.
control <- subset(data, COND > 0)
PP <- control[c(12:21)]
reliability(PP)
PP$PP  <-  PP$PP1 + PP$PP2 + PP$PP3 + PP$PP4 + PP$PP5 + PP$PP6 + PP$PP7 + PP$PP8 + PP$PP9 + PP$PP10
PP <- PP[c(11)]
ID <- rownames(PP)
PP <- cbind(id=ID, PP)

VC <- control[c(22:27)]
reliability(VC)
VC$VC  <-  VC$VC1 + VC$VC2 + VC$VC3 + VC$VC4 + VC$VC5 + VC$VC6
VC <- VC[c(7)]
ID <- rownames(VC)
VC <- cbind(id=ID, VC)
#Merge1
merge1 <- merge(PP,VC,by="id")

TC <- control[c(28:37)]
reliability(TC)
TC$TC  <-  TC$TC1 + TC$TC2 + TC$TC3 + TC$TC4 + TC$TC5 + TC$TC6 + TC$TC7 + TC$TC8 + TC$TC9 + TC$TC10
TC <- TC[c(11)]
ID <- rownames(TC)
TC <- cbind(id=ID, TC)
#Merge2
merge2 <- merge(merge1,TC,by="id")

OCBI <- control[c(38:44)]
reliability(OCBI)
OCBI$OCBI  <-  OCBI$OCBI1 + OCBI$OCBI2 + OCBI$OCBI3 + OCBI$OCBI4 + OCBI$OCBI5 + OCBI$OCBI6 + OCBI$OCBI7
OCBI <- OCBI[c(8)]
ID <- rownames(OCBI)
OCBI <- cbind(id=ID, OCBI)
#Merge3
merge3 <- merge(merge2,OCBI,by="id")

OCBO <- control[c(45:51)]
key <- c(1,1,-1,-1,-1,1,1)
OCBOr <- reverse.code(key,OCBO)
reliability(OCBOr)
OCBOr <- as.data.frame(OCBOr)
OCBOr$OCBO  <-  OCBOr$OCBO1 + OCBOr$OCBO2 + OCBOr$OCBO3 + OCBOr$OCBO4 + OCBOr$OCBO5 + OCBOr$OCBO6 + OCBOr$OCBO7
OCBO <- OCBOr[c(8)]
ID <- rownames(OCBO)
OCBO <- cbind(id=ID, OCBO)
#Merge4
merge4 <- merge(merge3,OCBO,by="id")

IRB <- control[c(52:58)]
key <- c(1,1,1,1,1,-1,-1)
IRBr <- reverse.code(key,IRB)
reliability(IRBr)
IRBr <- as.data.frame(IRBr)
IRBr$IRB  <-  IRBr$IRB1 + IRBr$IRB2 + IRBr$IRB3 + IRBr$IRB4 + IRBr$IRB5 + IRBr$IRB6 + IRBr$IRB7
IRB <- IRBr[c(8)]
ID <- rownames(IRB)
IRB <- cbind(id=ID, IRB)
#Merge5
merge5 <- merge(merge4,IRB,by="id")

PA <- control[c(65:74)]
reliability(PA)
PA$PA  <-  PA$PA1 + PA$PA2 + PA$PA3 + PA$PA4 + PA$PA5 + PA$PA6 + PA$PA7 + PA$PA8 + PA$PA9 + PA$PA10
PA <- PA[c(11)]
ID <- rownames(PA)
PA <- cbind(id=ID, PA)
#Merge6
merge6 <- merge(merge5,PA,by="id")

Na <- control[c(75:84)]
reliability(Na)
Na$Na  <-  Na$NA1 + Na$NA2 + Na$NA3 + Na$NA4 + Na$NA5 + Na$NA6 + Na$NA7 + Na$NA8 + Na$NA9 + Na$NA10
Na <- Na[c(11)]
ID <- rownames(Na)
Na <- cbind(id=ID, Na)
#Merge7
merge7 <- merge(merge6,Na,by="id")

#APA Style table
library(apaTables)
apa.cor.table(merge7, filename = NA, table.number = NA, show.conf.interval = FALSE, landscape = TRUE)

treatment <- subset(data, COND < 1)
PP <- treatment[c(12:21)]
reliability(PP)
PP$PP  <-  PP$PP1 + PP$PP2 + PP$PP3 + PP$PP4 + PP$PP5 + PP$PP6 + PP$PP7 + PP$PP8 + PP$PP9 + PP$PP10
PP <- PP[c(11)]
ID <- rownames(PP)
PP <- cbind(id=ID, PP)

VC <- treatment[c(22:27)]
reliability(VC)
VC$VC  <-  VC$VC1 + VC$VC2 + VC$VC3 + VC$VC4 + VC$VC5 + VC$VC6
VC <- VC[c(7)]
ID <- rownames(VC)
VC <- cbind(id=ID, VC)
#Merge1
merge1 <- merge(PP,VC,by="id")

TC <- treatment[c(28:37)]
reliability(TC)
TC$TC  <-  TC$TC1 + TC$TC2 + TC$TC3 + TC$TC4 + TC$TC5 + TC$TC6 + TC$TC7 + TC$TC8 + TC$TC9 + TC$TC10
TC <- TC[c(11)]
ID <- rownames(TC)
TC <- cbind(id=ID, TC)
#Merge2
merge2 <- merge(merge1,TC,by="id")

OCBI <- treatment[c(38:44)]
reliability(OCBI)
OCBI$OCBI  <-  OCBI$OCBI1 + OCBI$OCBI2 + OCBI$OCBI3 + OCBI$OCBI4 + OCBI$OCBI5 + OCBI$OCBI6 + OCBI$OCBI7
OCBI <- OCBI[c(8)]
ID <- rownames(OCBI)
OCBI <- cbind(id=ID, OCBI)
#Merge3
merge3 <- merge(merge2,OCBI,by="id")

OCBO <- treatment[c(45:51)]
key <- c(1,1,-1,-1,-1,1,1)
OCBOr <- reverse.code(key,OCBO)
reliability(OCBOr)
OCBOr <- as.data.frame(OCBOr)
OCBOr$OCBO  <-  OCBOr$OCBO1 + OCBOr$OCBO2 + OCBOr$OCBO3 + OCBOr$OCBO4 + OCBOr$OCBO5 + OCBOr$OCBO6 + OCBOr$OCBO7
OCBO <- OCBOr[c(8)]
ID <- rownames(OCBO)
OCBO <- cbind(id=ID, OCBO)
#Merge4
merge4 <- merge(merge3,OCBO,by="id")

IRB <- treatment[c(52:58)]
key <- c(1,1,1,1,1,-1,-1)
IRBr <- reverse.code(key,IRB)
reliability(IRBr)
IRBr <- as.data.frame(IRBr)
IRBr$IRB  <-  IRBr$IRB1 + IRBr$IRB2 + IRBr$IRB3 + IRBr$IRB4 + IRBr$IRB5 + IRBr$IRB6 + IRBr$IRB7
IRB <- IRBr[c(8)]
ID <- rownames(IRB)
IRB <- cbind(id=ID, IRB)
#Merge5
merge5 <- merge(merge4,IRB,by="id")

PA <- treatment[c(65:74)]
reliability(PA)
PA$PA  <-  PA$PA1 + PA$PA2 + PA$PA3 + PA$PA4 + PA$PA5 + PA$PA6 + PA$PA7 + PA$PA8 + PA$PA9 + PA$PA10
PA <- PA[c(11)]
ID <- rownames(PA)
PA <- cbind(id=ID, PA)
#Merge6
merge6 <- merge(merge5,PA,by="id")

Na <- treatment[c(75:84)]
reliability(Na)
Na$Na  <-  Na$NA1 + Na$NA2 + Na$NA3 + Na$NA4 + Na$NA5 + Na$NA6 + Na$NA7 + Na$NA8 + Na$NA9 + Na$NA10
Na <- Na[c(11)]
ID <- rownames(Na)
Na <- cbind(id=ID, Na)
#Merge7
merge7 <- merge(merge6,Na,by="id")

#APA Style table
library(apaTables)
apa.cor.table(merge7, filename = NA, table.number = NA, show.conf.interval = FALSE, landscape = TRUE)

#Delete items
data$PP1 <- NULL
data$PP2 <- NULL
data$PP3 <- NULL
data$PP4 <- NULL
data$PP5 <- NULL
data$PP6 <- NULL
data$PP7 <- NULL
data$PP8 <- NULL
data$PP9 <- NULL
data$PP10 <- NULL
data$VC1 <- NULL
data$VC2 <- NULL
data$VC3 <- NULL
data$VC4 <- NULL
data$VC5 <- NULL
data$VC6 <- NULL
data$TC1 <- NULL
data$TC2 <- NULL
data$TC3 <- NULL
data$TC4 <- NULL
data$TC5 <- NULL
data$TC6 <- NULL
data$TC7 <- NULL
data$TC8 <- NULL
data$TC9 <- NULL
data$TC10 <- NULL
data$OCBI1 <- NULL
data$OCBI2 <- NULL
data$OCBI3 <- NULL
data$OCBI4 <- NULL
data$OCBI5 <- NULL
data$OCBI6 <- NULL
data$OCBI7 <- NULL
data$OCBO1 <- NULL
data$OCBO2 <- NULL
data$OCBO3 <- NULL
data$OCBO4 <- NULL
data$OCBO5 <- NULL
data$OCBO6 <- NULL
data$OCBO7 <- NULL
data$IRB1 <- NULL
data$IRB2 <- NULL
data$IRB3 <- NULL
data$IRB4 <- NULL
data$IRB5 <- NULL
data$IRB6 <- NULL
data$IRB7 <- NULL
data$PA1 <- NULL
data$PA2 <- NULL
data$PA3 <- NULL
data$PA4 <- NULL
data$PA5 <- NULL
data$PA6 <- NULL
data$PA7 <- NULL
data$PA8 <- NULL
data$PA9 <- NULL
data$PA10 <- NULL
data$NA1 <- NULL
data$NA2 <- NULL
data$NA3 <- NULL
data$NA4 <- NULL
data$NA5 <- NULL
data$NA6 <- NULL
data$NA7 <- NULL
data$NA8 <- NULL
data$NA9 <- NULL
data$NA10 <- NULL
```
#Results
##Manipulation check for consistency motifs 
To determine if our treatment weakened the consistency motif, we conducted an independent samples t-test to test for a statistically significant differences in consistency motif scores. The results were significant (t = 2.015, df = 297, p = .045) and revealed a small to moderate effect (d = 0.24, 95%CI:[.008, .72]) linking our condition to consistency index scores. The pattern of the results suggested that scores in the control condition were more consistent than scores in the treatment condition (control: M = 1.62, SD = 1.46; treatment: M = 1.98, SD = 1.60). Though scores on the consistency motif index were low in both conditions and suggested that participants were generally consistent, our manipulation check suggests that we successfully weakened reliance on consistency motifs. 
```{r}
require("lsr")
independentSamplesTTest(CM ~ COND, data = data, var.equal = TRUE, conf.level = .95)
boxplot(data$CM ~ data$COND)
#Consider showing the boxplots.
```
Having demonstrated a succesful manipulation, we subsetted our data into separate groups, specified a causal model as outlined by figure 1b, examined the method effects present in each group. The process was applied for both control and remedied conditions separately as these conditions would vary in their respective method effects, rendering the results of a multi-group analysis suspect. Specifically for the remedied condition, an unmeasured latent method construct factor was included in the model as representing a common blocking factor that exerted equal contaminating effects on all criterion measurement variables. This unmeasured latent method factor was modeled as causing an equal amount of method variance that would be common to the criterion variables because these measures were in the same block of items. Because there were nested model comparisons, we have chosen to only report the best-fitting models for each respective dataset along with predictor-criterion standardized regression estimates (see Table x). Note: for the best-fitting method effects model applied to the remedied condition, we observed a noncongeneric (equal) method effect factor loading of .21 (p = .003) for all criterion variables, which we believed captured a common item blocking factor.

```{r}
affect.m <- '
  PA =~ PAp1 + PAp2 + PAp3 + PAp4 + PAp5 + MOOD
  NA =~ Nap1 + Nap2 + Nap3 + Nap4 + Nap5 + MOOD
'
affect.cfa <- cfa(affect.m, estimator = "MLM", data = data, group = "COND", std.lv = TRUE)
#Group 1 is the temporally separted condition
#Group 2 is the same-time-point condition
summary(affect.cfa, standardized = TRUE, fit.measures = TRUE)
```

```{r}
#CONTROL
initial.sem.c  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ PR
  TC ~ PR
  IRB ~ PR
  OCBI ~ PR
  OCBO ~ PR
#Method Factors
  PA =~ PAp1 + PAp2 + PAp3 + PAp4 + PAp5
  Na =~ Nap1 + Nap2 + Nap3 + Nap4 + Nap5
  M =~ MOOD
  CMi =~ CM
  Neg =~ IRBp2 + OCBOp3
'
##Obtain unstandardized factor loadings and residual residual variances to levels found in the initial CFA, which should be fixed in the baseline models. 
initial.c <- cfa(initial.sem.c, estimator = "MLM", data=control, std.lv = TRUE)
summary(initial.c, standardized = TRUE, fit.measures = TRUE)

#Control Baseline
#Fix factor loadings and residual variances to initial levels.
baseline.sem.c  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ PR
  TC ~ PR
  IRB ~ PR
  OCBI ~ PR
  OCBO ~ PR
#Define Method Factors
  PA =~ 3.450*PAp1 + 0.686*PAp2 + 0.745*PAp3 + 2.255*PAp4 + 0.861*PAp5
  Na =~ 0.954*Nap1 + 1.363*Nap2 + 0.943*Nap3 + 1.101*Nap4 + 0.835*Nap5
  M =~ 1.224*MOOD
  CMi =~ 1.458*CM
  Neg =~ 0.562*IRBp2 + 1.363*OCBOp3
#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 
#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO  
#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO  
#Fix all consistency motif covariances to zero.
  CMi ~~ 0*PR
  CMi ~~ 0*VC
  CMi ~~ 0*TC
  CMi ~~ 0*IRB
  CMi ~~ 0*OCBI
  CMi ~~ 0*OCBO  
  CMi ~~ 0*PA
  CMi ~~ 0*Na
  CMi ~~ 0*M
#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M
  Neg ~~ 0*CMi
#Method Regressions
  M ~ PA + Na
#Fixed residuals
  PAp1 ~~ 4.074*PAp1
  PAp2 ~~ 0.380*PAp2
  PAp3 ~~ 0.519*PAp3
  PAp4 ~~ 1.163*PAp4
  PAp5 ~~ 0.579*PAp5
  Nap1 ~~ 0.649*Nap1
  Nap2 ~~ 0.498*Nap2
  Nap3 ~~ 0.845*Nap3
  Nap4 ~~ 1.063*Nap4
  Nap5 ~~ 0.459*Nap5
  IRBp2 ~~ 0.670*IRBp2
  OCBOp3 ~~ 1.510*OCBOp3
  MOOD ~~ 0*MOOD
  CM   ~~ 0*CM
'
baseline.c <- cfa(baseline.sem.c, estimator = "MLM", data=control, std.lv = TRUE)
summary(baseline.c, standardized = TRUE, fit.measures = TRUE)

#Method C
methodc.sem.c  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ PR
  TC ~ PR
  IRB ~ PR
  OCBI ~ PR
  OCBO ~ PR
#Define Method Factors
  PA =~ 3.450*PAp1 + 0.686*PAp2 + 0.745*PAp3 + 2.255*PAp4 + 0.861*PAp5 + PPp1 + PPp2 + PPp3 + PPp4 + VCp1 + VCp2 + VCp3 + TCp1 + TCp2 + TCp3 + IRBp1 + IRBp2 + OCBIp1 + OCBIp2 + OCBIp3 + OCBOp1 + OCBOp2 + OCBOp3
  Na =~ 0.954*Nap1 + 1.363*Nap2 + 0.943*Nap3 + 1.101*Nap4 + 0.835*Nap5 + PPp2 + PPp4 + IRBp2 + OCBIp1
  M =~ 1.224*MOOD + TCp3 + IRBp1 + OCBIp1 + OCBOp1 + OCBOp2
  CMi =~ 1.458*CM + v4*PPp1 + v4*PPp2 + v4*PPp3 + v4*PPp4 + v4*VCp1 + v4*VCp2 + v4*VCp3 + v4*TCp1 + v4*TCp2 + v4*TCp3 + v4*IRBp1 + v4*IRBp2 + v4*IRBp3 + v4*OCBIp1 + v4*OCBIp2 + v4*OCBIp3 + v4*OCBOp1 + v4*OCBOp2 + v4*OCBOp3
  Neg =~ 0.562*IRBp2 + 1.363*OCBOp3
#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 
#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO  
#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO  
#Fix all consistency motif covariances to zero.
  CMi ~~ 0*PR
  CMi ~~ 0*VC
  CMi ~~ 0*TC
  CMi ~~ 0*IRB
  CMi ~~ 0*OCBI
  CMi ~~ 0*OCBO  
  CMi ~~ 0*PA
  CMi ~~ 0*Na
  CMi ~~ 0*M
#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M
  Neg ~~ 0*CMi
#Method Regressions
  M ~ PA + Na
#Fixed residuals
  PAp1 ~~ 4.074*PAp1
  PAp2 ~~ 0.380*PAp2
  PAp3 ~~ 0.519*PAp3
  PAp4 ~~ 1.163*PAp4
  PAp5 ~~ 0.579*PAp5
  Nap1 ~~ 0.649*Nap1
  Nap2 ~~ 0.498*Nap2
  Nap3 ~~ 0.845*Nap3
  Nap4 ~~ 1.063*Nap4
  Nap5 ~~ 0.459*Nap5
  IRBp2 ~~ 0.670*IRBp2
  OCBOp3 ~~ 1.510*OCBOp3
  MOOD ~~ 0*MOOD
  CM   ~~ 0*CM
'
methodc.sem.c <- cfa(methodc.sem.c, estimator = "MLM", data=control, std.lv = TRUE)
summary(methodc.sem.c, standardized = TRUE, fit.measures = TRUE)
anova(baseline.c,methodc.sem.c)
#Baseline is statistically significant from model c. 

#METHOD U
methodu.sem.c  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ PR
  TC ~ PR
  IRB ~ PR
  OCBI ~ PR
  OCBO ~ PR
#Define Method Factors
  PA =~ 3.450*PAp1 + 0.686*PAp2 + 0.745*PAp3 + 2.255*PAp4 + 0.861*PAp5 + PPp1 + PPp2 + PPp3 + PPp4 + VCp1 + VCp2 + VCp3 + TCp1 + TCp2 + TCp3 + IRBp1 + IRBp2 + OCBIp1 + OCBIp2 + OCBIp3 + OCBOp1 + OCBOp2 + OCBOp3
  Na =~ 0.954*Nap1 + 1.363*Nap2 + 0.943*Nap3 + 1.101*Nap4 + 0.835*Nap5
  M =~ 1.224*MOOD + TCp3 + IRBp1 + OCBIp1 + OCBOp1 + OCBOp2
  CMi =~ 1.458*CM + OCBIp3 + OCBOp2
  Neg =~ 0.562*IRBp2 + 1.363*OCBOp3
#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 
#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO  
#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO  
#Fix all consistency motif covariances to zero.
  CMi ~~ 0*PR
  CMi ~~ 0*VC
  CMi ~~ 0*TC
  CMi ~~ 0*IRB
  CMi ~~ 0*OCBI
  CMi ~~ 0*OCBO  
  CMi ~~ 0*PA
  CMi ~~ 0*Na
  CMi ~~ 0*M
#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M
  Neg ~~ 0*CMi
#Method Regressions
  M ~ PA + Na
#Fixed residuals
  PAp1 ~~ 4.074*PAp1
  PAp2 ~~ 0.380*PAp2
  PAp3 ~~ 0.519*PAp3
  PAp4 ~~ 1.163*PAp4
  PAp5 ~~ 0.579*PAp5
  Nap1 ~~ 0.649*Nap1
  Nap2 ~~ 0.498*Nap2
  Nap3 ~~ 0.845*Nap3
  Nap4 ~~ 1.063*Nap4
  Nap5 ~~ 0.459*Nap5
  IRBp2 ~~ 0.670*IRBp2
  OCBOp3 ~~ 1.510*OCBOp3
  MOOD ~~ 0*MOOD
  CM   ~~ 0*CM
'
methodu.sem.c <- cfa(methodu.sem.c, estimator = "MLM", data=control, std.lv = TRUE)
summary(methodu.sem.c, standardized = TRUE, fit.measures = TRUE)
anova(methodu.sem.c,methodc.sem.c)
anova(baseline.method.model.c, method.sem.model.u)
#Method U is superior to baseline model. Congeneric CMV is present. 

#METHOD R
methodr.sem.c  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ 1.221*PR
  TC ~ 1.199*PR
  IRB ~ 0.410*PR
  OCBI ~ 0.763*PR
  OCBO ~ 0.444*PR
#Define Method Factors
  PA =~ 3.450*PAp1 + 0.686*PAp2 + 0.745*PAp3 + 2.255*PAp4 + 0.861*PAp5 + PPp1 + PPp2 + PPp3 + PPp4 + VCp1 + VCp2 + VCp3 + TCp1 + TCp2 + TCp3 + IRBp1 + IRBp2 + OCBIp1 + OCBIp2 + OCBIp3 + OCBOp1 + OCBOp2 + OCBOp3
  Na =~ 0.954*Nap1 + 1.363*Nap2 + 0.943*Nap3 + 1.101*Nap4 + 0.835*Nap5
  M =~ 1.224*MOOD + TCp3 + IRBp1 + OCBIp1 + OCBOp1 + OCBOp2
  CMi =~ 1.458*CM + OCBIp3 + OCBOp2
  Neg =~ 0.562*IRBp2 + 1.363*OCBOp3
#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 
#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO  
#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO  
#Fix all consistency motif covariances to zero.
  CMi ~~ 0*PR
  CMi ~~ 0*VC
  CMi ~~ 0*TC
  CMi ~~ 0*IRB
  CMi ~~ 0*OCBI
  CMi ~~ 0*OCBO  
  CMi ~~ 0*PA
  CMi ~~ 0*Na
  CMi ~~ 0*M
#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M
  Neg ~~ 0*CMi
#Method Regressions
  M ~ PA + Na
#Fixed residuals
  PAp1 ~~ 4.074*PAp1
  PAp2 ~~ 0.380*PAp2
  PAp3 ~~ 0.519*PAp3
  PAp4 ~~ 1.163*PAp4
  PAp5 ~~ 0.579*PAp5
  Nap1 ~~ 0.649*Nap1
  Nap2 ~~ 0.498*Nap2
  Nap3 ~~ 0.845*Nap3
  Nap4 ~~ 1.063*Nap4
  Nap5 ~~ 0.459*Nap5
  IRBp2 ~~ 0.670*IRBp2
  OCBOp3 ~~ 1.510*OCBOp3
  MOOD ~~ 0*MOOD
  CM   ~~ 0*CM
'
methodr.sem.c <- cfa(methodr.sem.c, estimator = "MLM", data=control, std.lv = TRUE)
summary(methodr.sem.c, standardized = TRUE, fit.measures = TRUE)
anova(methodr.sem.c,methodu.sem.c)
#NOTE: Method effects attributable to positive affectivity, negative affectivity, mood, and negative item wording were observed. However, a test for method effects attributable to consistency motifs was not observed.
```
```{r}
#TREATMENT
initial.t  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ PR
  TC ~ PR
  IRB ~ PR
  OCBI ~ PR
  OCBO ~ PR
#Method Factors
  PA =~ PAp1 + PAp2 + PAp3 + PAp4 + PAp5
  Na =~ Nap1 + Nap2 + Nap3 + Nap4 + Nap5
  M =~ MOOD
  CMi =~ CM
  Neg =~ IRBp2 + OCBOp3
  Block =~ a*VCp1 + a*VCp2 + a*VCp3 + a*TCp1 + a*TCp2 + a*TCp3 + a*TCp1 + a*TCp2 + a*TCp3 + a*IRBp1 + a*IRBp2 + a*IRBp3 + a*OCBIp1 + a*OCBIp2 + a*OCBIp3 + a*OCBOp1 + a*OCBOp2 + a*OCBOp3
#Fix all Block-Factor Covariances to zero.
  Block ~~ 0*PR
  Block ~~ 0*VC
  Block ~~ 0*TC
  Block ~~ 0*IRB
  Block ~~ 0*OCBI
  Block ~~ 0*OCBO
  Block ~~ 0*PA
  Block ~~ 0*Na
  Block ~~ 0*M
  Block ~~ 0*CMi
  Block ~~ 0*Neg
#Fix residual variances for Heywood cases.
  IRBp1 ~~ 0*IRBp1
  OCBOp3 ~~ 0*OCBOp3
'
#Note: Two Heywood cases emerged, but they were non-significant (IRBp1 and OCBOp3). So they were constrained to zero. 
##Obtain unstandardized factor loadings and residual residual variances to levels found in the initial CFA, which should be fixed in the baseline models. 
initial.sem.t <- cfa(initial.t, estimator = "MLM", data=treatment, std.lv = TRUE)
summary(initial.sem.t, standardized = TRUE, fit.measures = TRUE)

baseline.t  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ PR
  TC ~ PR
  IRB ~ PR
  OCBI ~ PR
  OCBO ~ PR
#Method Factors
  PA =~ 2.964*PAp1 + 0.653*PAp2 + 0.614*PAp3 + 2.358*PAp4 + 0.700*PAp5
  Na =~ 1.243*Nap1 + 1.285*Nap2 + 1.267*Nap3 + 1.273*Nap4 + 0.989*Nap5
  M =~ 1.495*MOOD
  CMi =~ 1.590*CM
  Neg =~ 0.675*IRBp2 + 1.903*OCBOp3
  Block =~ c1*VCp1 + c1*VCp2 + c1*VCp3 + c1*TCp1 + c1*TCp2 + c1*TCp3 + c1*IRBp1 + c1*IRBp2 + c1*IRBp3 + c1*OCBIp1 + c1*OCBIp2 + c1*OCBIp3 + c1*OCBOp1 + c1*OCBOp2 + c1*OCBOp3
#Fix all Block-Factor Covariances to zero.
  Block ~~ 0*PR
  Block ~~ 0*VC
  Block ~~ 0*TC
  Block ~~ 0*IRB
  Block ~~ 0*OCBI
  Block ~~ 0*OCBO
  Block ~~ 0*PA
  Block ~~ 0*Na
  Block ~~ 0*M
  Block ~~ 0*CMi
  Block ~~ 0*Neg
#Fix residual variances for Heywood cases.
  IRBp1 ~~ 0*IRBp1
  OCBOp3 ~~ 0*OCBOp3
#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 
#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO
#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO 
#Fix all consistency motif covariances to zero.
  CMi ~~ 0*PR
  CMi ~~ 0*VC
  CMi ~~ 0*TC
  CMi ~~ 0*IRB
  CMi ~~ 0*OCBI
  CMi ~~ 0*OCBO  
  CMi ~~ 0*PA
  CMi ~~ 0*Na
  CMi ~~ 0*M
#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M
  Neg ~~ 0*CMi
#Method Regressions
  M ~ PA + Na
#Fixed residuals
  PAp1 ~~ 4.502*PAp1
  PAp2 ~~ 0.576*PAp2
  PAp3 ~~ 0.539*PAp3
  PAp4 ~~ 1.363*PAp4
  PAp5 ~~ 0.553*PAp5
  Nap1 ~~ 0.363*Nap1
  Nap2 ~~ 0.658*Nap2
  Nap3 ~~ 0.410*Nap3
  Nap4 ~~ 1.126*Nap4
  Nap5 ~~ 0.786*Nap5
  IRBp2 ~~ 1.191*IRBp2
  MOOD ~~ 0*MOOD
  CM   ~~ 0*CM
'
baseline.sem.t <- cfa(baseline.t, estimator = "MLM", data=treatment, std.lv = TRUE)
summary(baseline.sem.t, standardized = TRUE, fit.measures = TRUE)
anova(baseline.sem.t, initial.sem.t)

#Method C
methodc.t  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ PR
  TC ~ PR
  IRB ~ PR
  OCBI ~ PR
  OCBO ~ PR
#Method Factors
  PA =~ 2.964*PAp1 + 0.653*PAp2 + 0.614*PAp3 + 2.358*PAp4 + 0.700*PAp5 + v1*PPp1 + v1*PPp2 + v1*PPp3 + v1*PPp4 + v1*VCp1 + v1*VCp2 + v1*VCp3 + v1*TCp1 + v1*TCp2 + v1*TCp3 + v1*IRBp1 + v1*IRBp2 + v1*IRBp3 + v1*OCBIp1 + v1*OCBIp2 + v1*OCBIp3 + v1*OCBOp1 + v1*OCBOp2 + v1*OCBOp3
  Na =~ 1.243*Nap1 + 1.285*Nap2 + 1.267*Nap3 + 1.273*Nap4 + 0.989*Nap5 + v2*PPp1 + v2*PPp2 + v2*PPp3 + v2*PPp4 + v2*VCp1 + v2*VCp2 + v2*VCp3 + v2*TCp1 + v2*TCp2 + v2*TCp3 + v2*IRBp1 + v2*IRBp2 + v2*IRBp3 + v2*OCBIp1 + v2*OCBIp2 + v2*OCBIp3 + v2*OCBOp1 + v2*OCBOp2 + v2*OCBOp3
  M =~ 1.495*MOOD + PPp1 + PPp2 + PPp3 + PPp4 + VCp1 + VCp2 + VCp3 + TCp1 + TCp2 + TCp3 + IRBp1 + IRBp2 + IRBp3 + OCBIp1 + OCBIp2 + OCBIp3 + OCBOp1 + OCBOp2 + OCBOp3
  CMi =~ 1.590*CM + v4*PPp1 + v4*PPp2 + v4*PPp3 + v4*PPp4 + v4*VCp1 + v4*VCp2 + v4*VCp3 + v4*TCp1 + v4*TCp2 + v4*TCp3 + v4*IRBp1 + v4*IRBp2 + v4*IRBp3 + v4*OCBIp1 + v4*OCBIp2 + v4*OCBIp3 + v4*OCBOp1 + v4*OCBOp2 + v4*OCBOp3
  Neg =~ 0.675*IRBp2 + 1.903*OCBOp3
  Block =~ c1*VCp1 + c1*VCp2 + c1*VCp3 + c1*TCp1 + c1*TCp2 + c1*TCp3 + c1*IRBp1 + c1*IRBp2 + c1*IRBp3 + c1*OCBIp1 + c1*OCBIp2 + c1*OCBIp3 + c1*OCBOp1 + c1*OCBOp2 + c1*OCBOp3
#Fix all Block-Factor Covariances to zero.
  Block ~~ 0*PR
  Block ~~ 0*VC
  Block ~~ 0*TC
  Block ~~ 0*IRB
  Block ~~ 0*OCBI
  Block ~~ 0*OCBO
  Block ~~ 0*PA
  Block ~~ 0*Na
  Block ~~ 0*M
  Block ~~ 0*CMi
  Block ~~ 0*Neg
#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 
#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO
#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO 
#Fix all consistency motif covariances to zero.
  CMi ~~ 0*PR
  CMi ~~ 0*VC
  CMi ~~ 0*TC
  CMi ~~ 0*IRB
  CMi ~~ 0*OCBI
  CMi ~~ 0*OCBO  
  CMi ~~ 0*PA
  CMi ~~ 0*Na
  CMi ~~ 0*M
#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M
  Neg ~~ 0*CMi
#Method Regressions
  M ~ PA + Na
#Fixed residuals
  PAp1 ~~ 4.502*PAp1
  PAp2 ~~ 0.576*PAp2
  PAp3 ~~ 0.539*PAp3
  PAp4 ~~ 1.363*PAp4
  PAp5 ~~ 0.553*PAp5
  Nap1 ~~ 0.363*Nap1
  Nap2 ~~ 0.658*Nap2
  Nap3 ~~ 0.410*Nap3
  Nap4 ~~ 1.126*Nap4
  Nap5 ~~ 0.786*Nap5
  IRBp2 ~~ 1.191*IRBp2
  MOOD ~~ 0*MOOD
  CM   ~~ 0*CM
#Fix Heywood cases
  IRBp1 ~~ 0*IRBp1
  OCBOp3 ~~ 0*OCBOp3
'
methodc.sem.t <- cfa(methodc.t, estimator = "MLM", data=treatment, std.lv = TRUE)
summary(methodc.sem.t, standardized = TRUE, fit.measures = TRUE)
anova(methodc.sem.t,baseline.sem.t)
#Heywood cases emerged (2; IRBp1, OCBOp3). They were non-significant.

#Method U
methodu.t  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ PR
  TC ~ PR
  IRB ~ PR
  OCBI ~ PR
  OCBO ~ PR
#Method Factors
  PA =~ 2.964*PAp1 + 0.653*PAp2 + 0.614*PAp3 + 2.358*PAp4 + 0.700*PAp5 + PPp1 + PPp2 + PPp3 + PPp4 + VCp1 + VCp3 + TCp1 + TCp2 + TCp3 + OCBIp1 + OCBIp2 + OCBIp3 + OCBOp1 + OCBOp2
  Na =~ 1.243*Nap1 + 1.285*Nap2 + 1.267*Nap3 + 1.273*Nap4 + 0.989*Nap5 + VCp3
  M =~ 1.495*MOOD + IRBp1
  CMi =~ 1.590*CM
  Neg =~ 0.675*IRBp2 + 1.903*OCBOp3
  Block =~ c1*VCp1 + c1*VCp2 + c1*VCp3 + c1*TCp1 + c1*TCp2 + c1*TCp3 + c1*IRBp1 + c1*IRBp2 + c1*IRBp3 + c1*OCBIp1 + c1*OCBIp2 + c1*OCBIp3 + c1*OCBOp1 + c1*OCBOp2 + c1*OCBOp3
#Fix all Block-Factor Covariances to zero.
  Block ~~ 0*PR
  Block ~~ 0*VC
  Block ~~ 0*TC
  Block ~~ 0*IRB
  Block ~~ 0*OCBI
  Block ~~ 0*OCBO
  Block ~~ 0*PA
  Block ~~ 0*Na
  Block ~~ 0*M
  Block ~~ 0*CMi
  Block ~~ 0*Neg
#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 
#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO
#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO 
#Fix all consistency motif covariances to zero.
  CMi ~~ 0*PR
  CMi ~~ 0*VC
  CMi ~~ 0*TC
  CMi ~~ 0*IRB
  CMi ~~ 0*OCBI
  CMi ~~ 0*OCBO  
  CMi ~~ 0*PA
  CMi ~~ 0*Na
  CMi ~~ 0*M
#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M
  Neg ~~ 0*CMi
#Method Regressions
  M ~ PA + Na
#Fixed residuals
  PAp1 ~~ 4.502*PAp1
  PAp2 ~~ 0.576*PAp2
  PAp3 ~~ 0.539*PAp3
  PAp4 ~~ 1.363*PAp4
  PAp5 ~~ 0.553*PAp5
  Nap1 ~~ 0.363*Nap1
  Nap2 ~~ 0.658*Nap2
  Nap3 ~~ 0.410*Nap3
  Nap4 ~~ 1.126*Nap4
  Nap5 ~~ 0.786*Nap5
  IRBp2 ~~ 1.191*IRBp2
  MOOD ~~ 0*MOOD
  CM   ~~ 0*CM
#Fix Heywood cases
  IRBp1 ~~ 0*IRBp1
  OCBOp3 ~~ 0*OCBOp3
'
methodu.sem.t <- cfa(methodu.t, estimator = "MLM", data=treatment, std.lv = TRUE)
summary(methodu.sem.t, standardized = TRUE, fit.measures = TRUE)
anova(methodu.sem.t,methodc.sem.t)

#METHOD R
methodr.t  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ 1.375*PR
  TC ~ 1.104*PR
  IRB ~ 0.234*PR
  OCBI ~ 0.507*PR
  OCBO ~ 0.501*PR
#Method Factors
  PA =~ 2.964*PAp1 + 0.653*PAp2 + 0.614*PAp3 + 2.358*PAp4 + 0.700*PAp5 + PPp1 + PPp2 + PPp3 + PPp4 + VCp1 + VCp3 + TCp1 + TCp2 + TCp3 + OCBIp1 + OCBIp2 + OCBIp3 + OCBOp1 + OCBOp2
  Na =~ 1.243*Nap1 + 1.285*Nap2 + 1.267*Nap3 + 1.273*Nap4 + 0.989*Nap5 + VCp3
  M =~ 1.495*MOOD + IRBp1
  CMi =~ 1.590*CM
  Neg =~ 0.675*IRBp2 + 1.903*OCBOp3
  Block =~ c1*VCp1 + c1*VCp2 + c1*VCp3 + c1*TCp1 + c1*TCp2 + c1*TCp3 + c1*IRBp1 + c1*IRBp2 + c1*IRBp3 + c1*OCBIp1 + c1*OCBIp2 + c1*OCBIp3 + c1*OCBOp1 + c1*OCBOp2 + c1*OCBOp3
#Fix all Block-Factor Covariances to zero.
  Block ~~ 0*PR
  Block ~~ 0*VC
  Block ~~ 0*TC
  Block ~~ 0*IRB
  Block ~~ 0*OCBI
  Block ~~ 0*OCBO
  Block ~~ 0*PA
  Block ~~ 0*Na
  Block ~~ 0*M
  Block ~~ 0*CMi
  Block ~~ 0*Neg
#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 
#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO  
#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO  
#Fix all consistency motif covariances to zero.
  CMi ~~ 0*PR
  CMi ~~ 0*VC
  CMi ~~ 0*TC
  CMi ~~ 0*IRB
  CMi ~~ 0*OCBI
  CMi ~~ 0*OCBO  
  CMi ~~ 0*PA
  CMi ~~ 0*Na
  CMi ~~ 0*M
#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M
  Neg ~~ 0*CMi
#Method Regressions
  M ~ PA + Na
#Fixed residuals
  PAp1 ~~ 4.502*PAp1
  PAp2 ~~ 0.576*PAp2
  PAp3 ~~ 0.539*PAp3
  PAp4 ~~ 1.363*PAp4
  PAp5 ~~ 0.553*PAp5
  Nap1 ~~ 0.363*Nap1
  Nap2 ~~ 0.658*Nap2
  Nap3 ~~ 0.410*Nap3
  Nap4 ~~ 1.126*Nap4
  Nap5 ~~ 0.786*Nap5
  IRBp2 ~~ 1.191*IRBp2
  MOOD ~~ 0*MOOD
  CM   ~~ 0*CM
#Fix Heywood cases
  IRBp1 ~~ 0*IRBp1
  OCBOp3 ~~ 0*OCBOp3
'
methodr.sem.t <- cfa(methodr.t, estimator = "MLM", data=control, std.lv = TRUE)
summary(methodr.sem.t, standardized = TRUE, fit.measures = TRUE)
anova(methodu.sem.t,methodr.sem.t)
```
The results of our tests and model comparisons are available in table x. The model data fit for our baseline models (i.e., models without method effects included) were generally unacceptable, which is common when method effects have not been modeled (see Williams et al., 2010). As can be seen, modeling method effects improved the fit of the model to the data relative to baseline levels. The best-fitting method effect models were the most flexible ones (i.e., congeneric) with some consistency to both substantive and method effects in terms of direction and magnitude. Across all models, the predictor-criterion relationships were positive. The series of model comparisons consistently (as in across conditions) revealed congeneric (unequal) method effects attributable to positive affectivity, negative affectivity, mood, and negative item wording. However, consistency motif effects were observed in the non-remedied condition for two of the OCBO parcels. While this does suggest that consistency motifs played a role in our non-remedied condition, and were absent in the remedied condition, the magnitude of these effects seem negligible. In fact, overall tests for method bias in the control condition failed to support method bias attributable to all method variance sources. Additionally, as expected, a blocking factor emerged in the remedied condition and exerted a constant effect on the measured variables for our criteria. Further testing revealed that all method effects in the remedied condition were enough to bias the observed correlations and that once these method effects were modeled, the proactive personality-role behavior relationships became non-significant. All other substantive effects were reduced in magnitude. The magnitude and direction of all method effects can be observed in table x. 

Table z displays the decomposed reliabilities of each scale as a function of substantive and method variance and also lists the percentage of overall method variance attributable to these different sources. As can be seen, there were multiple sources of method variance evident in our measurement model. Consistently, positive affectivity exerted an influence over responses to measures of proactive personality, voice, taking charge, in-role behavior, and OCBI. For OCBO, which was primarily method variance (63% in the control vs. 74% in the remedied condition), multiple method sources were present. 

#Discussion Study 1
With our first study, we sought to test the role played by multiple sources of method variance (i.e., consistency motif, measurement context, affectivity, mood, negative item wording, and measurement context). More specifically, we experimentally manipulated both consistency motifs and measurement context in a practical manner, allowing us to test the causal role of these method variance sources in a realistic research setting. Additionally, we controlled for a variety of other sources of method variance to ensure that our causal effects would be reliably estimated, allowing us to draw strong conclusions about the magnitude of consistency motifs and measurement context effects.  

Generally speaking, our results cast doubt onto the strength of consistency motifs and provide mixed support for measurement context effects. For consistency motifs, we observered that they played stronger role in our non-remedied condition compared to our remedied condition, but lacked the potency for causing method bias in this condition. Consistency motifs were absent in our remedied condition. This suggests that our treatment (e.g., a cover story) weakens consistency motifs. Specifically, we found only two significant effects (p < .05) linking consistency motif scores to our parcels. Though this was in the expected direction (i.e., increasing consistency) and in the expected condition (non-remedied), their magnitude was small and ultimately negligible (factor loadings = .13 and .14). These effects were not observed in our remedied condition. Therefore, our findings call into question the potency of consistency motifs as a cause of method bias.

Our results suggest that method variance is far more nuanced than suggested by the early literature. Firstly, our results support the view that method variance is best viewed multidimensionally and approached in an multipronged manner. For instance, we found several method effects linked to positive affect, mood, negative item wording, and (presumably) being in a common block. These forces were also strong enough to cause biased estimates of latent construct correlation in our remedied condition, some of which became non-significant once method effects were controlled. However, certain causes of method variance seem insufficient for causing bias. 

#Study 2 - Momentary Mood as a Cause of Method Variance
#Method - Study 2
##Sample, Measures, and Procedure
```{r}
library(foreign)
Temporal.spss <- read.spss("/Users/ccasti02/Documents/Research/Common Method Variance/Older Approaches/Older Data/CMV.TempSep.sav", to.data.frame=FALSE, use.value.labels=FALSE)
#View(Temporal.spss)
data2 <- data.frame(Temporal.spss)
#Note: 533 variables...lots of bullshit variables to trim. Thanks Surveymonkey. 

#Delete old dems.
data2$MVOutlier <- NULL
data2$Source <- NULL
data2$PartFullTime <- NULL

#Delete attitudes toward the color blue.
data2$Blue1 <- NULL
data2$Blue2 <- NULL
data2$Blue3 <- NULL
data2$Blue4 <- NULL
data2$BLUE1_T2 <- NULL
data2$BLUE2_T2 <- NULL
data2$BLUE3_T2 <- NULL
data2$BLUE4_T2 <- NULL
data2$BLUE <- NULL


#Delete positive brand label attitudes.
data2$PLBA1 <- NULL
data2$PLBA2 <- NULL
data2$PLBA3 <- NULL
data2$PLBA4 <- NULL
data2$PLBA5 <- NULL
data2$PLBA1_T2 <- NULL
data2$PLBA2_T2 <- NULL
data2$PLBA3_T2 <- NULL
data2$PLBA4_T2 <- NULL
data2$PLBA5_T2 <- NULL
data2$PLBA <- NULL

#Delete survey enjoyment and value
data2$S_ENJ1 <- NULL
data2$S_ENJ2 <- NULL
data2$S_ENJ3 <- NULL
data2$S_ENJ1_T2 <- NULL
data2$S_ENJ2_T2 <- NULL
data2$S_ENJ3_T2 <- NULL
data2$S_VAL1 <- NULL
data2$S_VAL2 <- NULL
data2$S_VAL3 <- NULL
data2$S_VAL1_T2 <- NULL
data2$S_VAL2_T2 <- NULL
data2$S_VAL3_T2 <- NULL

#Delete random age factors
data2$AGE0 <- NULL
data2$AGE1 <- NULL
data2$AGE2 <- NULL
data2$AGE3 <- NULL
data2$AGE4 <- NULL
data2$AGE5 <- NULL
data2$AGE6 <- NULL
data2$AGE7 <- NULL
data2$AGE8 <- NULL
data2$AGE9 <- NULL
data2$AGEA <- NULL
data2$AGEB <- NULL
data2$AGEC <- NULL
data2$AGED <- NULL
data2$AGEE <- NULL
data2$AGEF <- NULL
data2$AGEG <- NULL
data2$AGEH <- NULL
data2$AGEF <- NULL
data2$AGEH <- NULL
data2$AGEI <- NULL
data2$AGEJ <- NULL
data2$AGEK <- NULL
data2$AGEL <- NULL
data2$AGEM <- NULL
data2$AGEN <- NULL
data2$AGEL <- NULL
data2$AGEO <- NULL
data2$AGEP <- NULL
data2$AGEQ <- NULL
data2$AGER <- NULL
data2$AGES <- NULL
data2$AGET <- NULL
data2$AGEU <- NULL
data2$AGEV <- NULL
data2$AGEW <- NULL
data2$AGEX <- NULL
data2$AGEY <- NULL
data2$AGEZ <- NULL
data2$AGE10 <- NULL
data2$AGE11 <- NULL
data2$AGE12 <- NULL
data2$AGE13 <- NULL
data2$AGE14 <- NULL
data2$AGE15 <- NULL
data2$AGE16 <- NULL
data2$AGE17 <- NULL
data2$AGE18 <- NULL
data2$AGE19 <- NULL
data2$AGE1A <- NULL
data2$AGE1B <- NULL
data2$AGE1C <- NULL
data2$AGE1D <- NULL
data2$AGE1E <- NULL
data2$AGE1F <- NULL
data2$AGE1G <- NULL
data2$AGE1H <- NULL
data2$AGE1I <- NULL
data2$AGE1J <- NULL
data2$AGE1K <- NULL
data2$AGE1L <- NULL
data2$AGE1M <- NULL
data2$AGE1N <- NULL
data2$AGE1O <- NULL
data2$AGE1P <- NULL
data2$AGE1Q <- NULL
data2$AGE1R <- NULL
data2$AGE1S <- NULL
data2$AGE1T <- NULL
data2$AGE1U <- NULL
data2$AGE1V <- NULL
data2$AGE1W <- NULL
data2$AGE1X <- NULL
data2$AGE1Y <- NULL
data2$AGE1Z <- NULL
data2$AGE20 <- NULL
data2$AGE21 <- NULL
data2$AGE22 <- NULL
data2$AGE23 <- NULL
data2$AGE24 <- NULL
data2$AGE25 <- NULL
data2$AGE26 <- NULL
data2$AGE27 <- NULL
data2$AGE28 <- NULL
data2$AGE29 <- NULL
data2$AGE2A <- NULL
data2$AGE2B <- NULL
data2$AGE2C <- NULL
data2$AGE2D <- NULL
data2$AGE2E <- NULL
data2$AGE2F <- NULL
data2$AGE2G <- NULL
data2$AGE2H <- NULL
data2$AGE2I <- NULL
data2$AGE2J <- NULL
data2$AGE2K <- NULL
data2$AGE2L <- NULL
data2$AGE2M <- NULL
data2$AGE2N <- NULL
data2$AGE2O <- NULL
data2$AGE2P <- NULL
data2$AGE2Q <- NULL
data2$AGE2R <- NULL
data2$AGE2S <- NULL
data2$AGE2T <- NULL
data2$AGE2U <- NULL
data2$AGE2V <- NULL
data2$AGE2W <- NULL
data2$AGE2X <- NULL
data2$AGE2Y <- NULL
data2$AGE2Z <- NULL
data2$AGE30 <- NULL
data2$AGE31 <- NULL
data2$AGE32 <- NULL
data2$AGE33 <- NULL
data2$AGE34 <- NULL
data2$AGE35 <- NULL
data2$AGE36 <- NULL
data2$AGE37 <- NULL
data2$AGE38 <- NULL
data2$AGE39 <- NULL
data2$AGE3A <- NULL
data2$AGE3B <- NULL
data2$AGE3C <- NULL
data2$AGE3D <- NULL
data2$AGE3E <- NULL
data2$AGE3F <- NULL
data2$AGE3G <- NULL
data2$AGE3H <- NULL
data2$AGE3I <- NULL
data2$AGE3J <- NULL
data2$AGE3K <- NULL
data2$AGE3L <- NULL

#Delete random sex factors
data2$SEX0 <- NULL
data2$SEX1 <- NULL
data2$SEX2 <- NULL
data2$SEX3 <- NULL
data2$SEX4 <- NULL
data2$SEX5 <- NULL
data2$SEX6 <- NULL
data2$SEX7 <- NULL
data2$SEX8 <- NULL
data2$SEX9 <- NULL
data2$SEXA <- NULL
data2$SEXB <- NULL
data2$SEXC <- NULL
data2$SEXD <- NULL
data2$SEXE <- NULL
data2$SEXF <- NULL
data2$SEXG <- NULL
data2$SEXH <- NULL
data2$SEXF <- NULL
data2$SEXH <- NULL
data2$SEXI <- NULL
data2$SEXJ <- NULL
data2$SEXK <- NULL
data2$SEXL <- NULL
data2$SEXM <- NULL
data2$SEXN <- NULL
data2$SEXL <- NULL
data2$SEXO <- NULL
data2$SEXP <- NULL
data2$SEXQ <- NULL
data2$SEXR <- NULL
data2$SEXS <- NULL
data2$SEXT <- NULL
data2$SEXU <- NULL
data2$SEXV <- NULL
data2$SEXW <- NULL
data2$SEXX <- NULL
data2$SEXY <- NULL
data2$SEXZ <- NULL
data2$SEX10 <- NULL
data2$SEX11 <- NULL
data2$SEX12 <- NULL
data2$SEX13 <- NULL
data2$SEX14 <- NULL
data2$SEX15 <- NULL
data2$SEX16 <- NULL
data2$SEX17 <- NULL
data2$SEX18 <- NULL
data2$SEX19 <- NULL
data2$SEX1A <- NULL
data2$SEX1B <- NULL
data2$SEX1C <- NULL
data2$SEX1D <- NULL
data2$SEX1E <- NULL
data2$SEX1F <- NULL
data2$SEX1G <- NULL
data2$SEX1H <- NULL
data2$SEX1I <- NULL
data2$SEX1J <- NULL
data2$SEX1K <- NULL
data2$SEX1L <- NULL
data2$SEX1M <- NULL
data2$SEX1N <- NULL
data2$SEX1O <- NULL
data2$SEX1P <- NULL
data2$SEX1Q <- NULL
data2$SEX1R <- NULL
data2$SEX1S <- NULL
data2$SEX1T <- NULL
data2$SEX1U <- NULL
data2$SEX1V <- NULL
data2$SEX1W <- NULL
data2$SEX1X <- NULL
data2$SEX1Y <- NULL
data2$SEX1Z <- NULL
data2$SEX20 <- NULL
data2$SEX21 <- NULL
data2$SEX22 <- NULL
data2$SEX23 <- NULL
data2$SEX24 <- NULL
data2$SEX25 <- NULL
data2$SEX26 <- NULL
data2$SEX27 <- NULL
data2$SEX28 <- NULL
data2$SEX29 <- NULL
data2$SEX2A <- NULL
data2$SEX2B <- NULL
data2$SEX2C <- NULL
data2$SEX2D <- NULL
data2$SEX2E <- NULL
data2$SEX2F <- NULL
data2$SEX2G <- NULL
data2$SEX2H <- NULL
data2$SEX2I <- NULL
data2$SEX2J <- NULL
data2$SEX2K <- NULL
data2$SEX2L <- NULL
data2$SEX2M <- NULL
data2$SEX2N <- NULL
data2$SEX2O <- NULL
data2$SEX2P <- NULL
data2$SEX2Q <- NULL
data2$SEX2R <- NULL
data2$SEX2S <- NULL
data2$SEX2T <- NULL
data2$SEX2U <- NULL
data2$SEX2V <- NULL
data2$SEX2W <- NULL
data2$SEX2X <- NULL
data2$SEX2Y <- NULL
data2$SEX2Z <- NULL
data2$SEX30 <- NULL
data2$SEX31 <- NULL
data2$SEX32 <- NULL
data2$SEX33 <- NULL
data2$SEX34 <- NULL
data2$SEX35 <- NULL
data2$SEX36 <- NULL
data2$SEX37 <- NULL
data2$SEX38 <- NULL
data2$SEX39 <- NULL
data2$SEX3A <- NULL
data2$SEX3B <- NULL
data2$SEX3C <- NULL
data2$SEX3D <- NULL
data2$SEX3E <- NULL
data2$SEX3F <- NULL
data2$SEX3G <- NULL
data2$SEX3H <- NULL
data2$SEX3I <- NULL
data2$SEX3J <- NULL
data2$SEX3K <- NULL
data2$SEX3L <- NULL

#Delete random race factors
data2$RACE0 <- NULL
data2$RACE1 <- NULL
data2$RACE2 <- NULL
data2$RACE3 <- NULL
data2$RACE4 <- NULL
data2$RACE5 <- NULL
data2$RACE6 <- NULL
data2$RACE7 <- NULL
data2$RACE8 <- NULL
data2$RACE9 <- NULL
data2$RACEA <- NULL
data2$RACEB <- NULL
data2$RACEC <- NULL
data2$RACED <- NULL
data2$RACEE <- NULL
data2$RACEF <- NULL
data2$RACEG <- NULL
data2$RACEH <- NULL
data2$RACEF <- NULL
data2$RACEH <- NULL
data2$RACEI <- NULL
data2$RACEJ <- NULL
data2$RACEK <- NULL
data2$RACEL <- NULL
data2$RACEM <- NULL
data2$RACEN <- NULL
data2$RACEL <- NULL
data2$RACEO <- NULL
data2$RACEP <- NULL
data2$RACEQ <- NULL
data2$RACER <- NULL
data2$RACES <- NULL
data2$RACET <- NULL
data2$RACEU <- NULL
data2$RACEV <- NULL
data2$RACEW <- NULL
data2$RACEX <- NULL
data2$RACEY <- NULL
data2$RACEZ <- NULL
data2$RACE10 <- NULL
data2$RACE11 <- NULL
data2$RACE12 <- NULL
data2$RACE13 <- NULL
data2$RACE14 <- NULL
data2$RACE15 <- NULL
data2$RACE16 <- NULL
data2$RACE17 <- NULL
data2$RACE18 <- NULL
data2$RACE19 <- NULL
data2$RACE1A <- NULL
data2$RACE1B <- NULL
data2$RACE1C <- NULL
data2$RACE1D <- NULL
data2$RACE1E <- NULL
data2$RACE1F <- NULL
data2$RACE1G <- NULL
data2$RACE1H <- NULL
data2$RACE1I <- NULL
data2$RACE1J <- NULL
data2$RACE1K <- NULL
data2$RACE1L <- NULL
data2$RACE1M <- NULL
data2$RACE1N <- NULL
data2$RACE1O <- NULL
data2$RACE1P <- NULL
data2$RACE1Q <- NULL
data2$RACE1R <- NULL
data2$RACE1S <- NULL
data2$RACE1T <- NULL
data2$RACE1U <- NULL
data2$RACE1V <- NULL
data2$RACE1W <- NULL
data2$RACE1X <- NULL
data2$RACE1Y <- NULL
data2$RACE1Z <- NULL
data2$RACE20 <- NULL
data2$RACE21 <- NULL
data2$RACE22 <- NULL
data2$RACE23 <- NULL
data2$RACE24 <- NULL
data2$RACE25 <- NULL
data2$RACE26 <- NULL
data2$RACE27 <- NULL
data2$RACE28 <- NULL
data2$RACE29 <- NULL
data2$RACE2A <- NULL
data2$RACE2B <- NULL
data2$RACE2C <- NULL
data2$RACE2D <- NULL
data2$RACE2E <- NULL
data2$RACE2F <- NULL
data2$RACE2G <- NULL
data2$RACE2H <- NULL
data2$RACE2I <- NULL
data2$RACE2J <- NULL
data2$RACE2K <- NULL
data2$RACE2L <- NULL
data2$RACE2M <- NULL
data2$RACE2N <- NULL
data2$RACE2O <- NULL
data2$RACE2P <- NULL
data2$RACE2Q <- NULL
data2$RACE2R <- NULL
data2$RACE2S <- NULL
data2$RACE2T <- NULL
data2$RACE2U <- NULL
data2$RACE2V <- NULL
data2$RACE2W <- NULL
data2$RACE2X <- NULL
data2$RACE2Y <- NULL
data2$RACE2Z <- NULL
data2$RACE30 <- NULL
data2$RACE31 <- NULL
data2$RACE32 <- NULL
data2$RACE33 <- NULL
data2$RACE34 <- NULL
data2$RACE35 <- NULL
data2$RACE36 <- NULL
data2$RACE37 <- NULL
data2$RACE38 <- NULL
data2$RACE39 <- NULL
data2$RACE3A <- NULL
data2$RACE3B <- NULL
data2$RACE3C <- NULL
data2$RACE3D <- NULL
data2$RACE3E <- NULL
data2$RACE3F <- NULL
data2$RACE3G <- NULL
data2$RACE3H <- NULL
data2$RACE3I <- NULL
data2$RACE3J <- NULL
data2$RACE3K <- NULL
data2$RACE3L <- NULL

#Delete unnecessary reverse-scored items.
data2$R_IRB6 <- NULL
data2$R_IRB7 <- NULL
data2$R_S_ENJ1 <- NULL
data2$R_S_ENJ1_T2 <- NULL
data2$R_S_VAL2 <- NULL
data2$R_S_VAL2_T2 <- NULL
data2$R_OCBO2 <- NULL
data2$R_OCBO3 <- NULL
data2$R_OCBO4 <- NULL

#Delete scale scores
data2$PP <- NULL
data2$VOICE <- NULL
data2$TC <- NULL
data2$OCBI <- NULL
data2$IRB <- NULL
data2$OCBO <- NULL

#Delete old filter variable
data2$filter <- NULL
data2$filter_. <- NULL
data2$COND2 <- NULL

#Delete interaction terms
data2$xPP_COND <- NULL
data2$xPP_COND2 <- NULL
data2$xTC_COND <- NULL
data2$xTC_COND2 <- NULL
data2$xV_COND <- NULL
data2$xV_COND2 <- NULL

#Delete formerly considered parcels
data2$PPp1 <- NULL
data2$PPp2 <- NULL
data2$PPp3 <- NULL
data2$OCBIp1 <- NULL
data2$OCBIp2 <- NULL
data2$OCBIp3 <- NULL
data2$OCBOp1 <- NULL
data2$OCBOp2 <- NULL
data2$OCBOp3 <- NULL
data2$IRBp1 <- NULL
data2$IRBp2 <- NULL
data2$IRBp3 <- NULL

View(data2)

#Note: Address the mislabeled OCB variables. 
require(plyr)
data2 <- rename(data2, c(OCBI7 = "OCBO7", OCBO7 = "OCBI7"))

#Re-order variables (so affectivity stuff is next to one another).
data2 <- data2[c(1:34,42,36:41,35,43:50,52,54,58,59,61,63,65,66,68,51,53,55,56,57,60,62,64,67,69,70,71,72,73,74)]
#View(data2)
```
Participants were recruited using a SurveyMonkey panel and randomly assigned to a condition where they received all measures at the same time (i.e., control) or a condition whereby a temporal separation of one week was used to divide the adminstration of measures. More specifically for temporal remedy condition, proactive personality, voice, taking charge, and momentary mood were administered first. After one week, the role behavior measures were administered along with several method and control measures (e.g., positive and negative affectivity, positive brand lable attitudes, preferences for the color blue, survey enjoyment, and survey value), a demographics questionnaire, and another measure of momentary mood. Momentary mood was assessed with one item: "My mood today can best be described as..." (1 = unpleasant, 7 = pleasant). With the exception of momentary mood, the same measurement model was tested in this study. Respondents received $5 to complete this survey. Participants’ data was used if there were no more than two items on the independent and dependent variables that a respondent did not answer. For the cross-sectional data, 150 of 190 respondents (78.9%) provided usable data. The temporally separated condition resulted in a serious amount of attrition for wave two of the survey. Although 307 respondents completed the first wave of the survey, only 183 completed the second wave and provided usable data (59.6%). 
```{r}
#Note: I will need to examine the possibility of non-random attrition. In other words, the missing data may not be missing at random. This is important because temporal separation remedies may introduce bias into parameter estimates by creating data that are not MCAR. If so, then researchers will need to make efforts to ensure complete data collection. I may be able to compute adjusted values, but will need to speak with Daniel Newman.
require(BaylorEdPsych)
```
Mean replacement was used for any missing item, limited to only two per respondent, affecting less than 1% of all items for all respondents across both conditions. In the full sample, respondents’ age ranged from 18 to 83 (M = 45.61). The sample was 57.0% female and 82.9% Caucasian. A large majority (84.1%) of respondents worked full-time, and the other 15.9% worked part-time. 
##Analytical Approach
We used established measured method effect techniques to assess the presence and biasing role of momentary affect (Williams et al., 2010). Similar to study 1, our expectations were that method effects attributable to momentary affect would be stronger in our control condition because, as all measures were administered simultaneously, mood would act as a constant contaminating factor. In other words, we expect to observe significant measurement contamination attributable to momentary mood in the form of significant and positive path coefficients linking momentary affect to the indicators of our measurement model, which are expectations consistent with prior research (Williams & Anderson, 1994). Furthermore, we expected that momentary positive affect would result in biased estimates of correlation. 

Conversely, we expected momentary affect to have virtually no effect in our remedied condition involving a 1-week temporal separation. This pattern would suggest that CMV attributable to affect would be present in a same-source same-time-point survey (hypothesis 1a). Evidence of bias attributable to momentary affect would emerge if the interconstruct correlations varied as a function of momentary affect effects. Thus, supporting hypothesis 1b would provide evidence supporting the viability of a one-week temporal separation for reducing affect effects. Again, we employed Williams et al's (2010) measured method effects technique, which involve a series of nested model comparisons under different assumptions of method variance presence (i.e., CMV is not present, non equal/noncongeneric CMV is present, unequal/congeneric CMV is present) and bias (i.e., no bias or significant bias) while also examiing the non-invariance of our measurement model.

Following study 1, we carried out our analyses using 'lavaan' in R and used robust maximum likelihood estimation (maximum likelihood estimation with robust standard errors) (see Li, 2016: "Confirmatory factor analysis with ordinal data: Comparing robust maximum likelihood and diagnoally weighted least squares") and the same modeling assumptions were applied (i.e., negative method factor, correlated residuals, factor loadings, and latent construct correlations). Satorra-Bentler X2 difference testing was, again, used to compare our models.

##Parcel Construction
```{r}
#Parceling strategy
#Following suggestions for item parceling by Hall, Snell, and Foust (1999), we used exploratory factor analysis to inform the construction of three or four parcels per factor. These parcels were designed to combine unmodeled yet secondary influences shared amongst a set of items into the same parcel, thus increasing the accuracy of our parameter estimates (i.e., factor loadings and latent construct correlations). Data across both groups were analyzed simultaneously. For the present investigation, there is a legitimate concern that parceling in this way might force common method variance into residuals. For instance, residual covariation that is common to a set of items and caused by context effects would be forced into this parcel's residual, resulting in conclusions that method variance is not playing a biasing role when in fact it is. However, we urge readers to keep in mind that if this common method effect is truly common across a measure, then this common source of variance would be be roughly equally distributed across our parcels and therefore should not be forced into specific residual(s) for a measure. For those who would, we can make our results without parceling available upon request.

# Maximum Likelihood Factor Analysis entering raw data and extracting factors, with promax rotation.
require(psych)
require(nFactors)

##Proactive Personality
PP <- data2[c(3:12)]
PP <- na.omit(PP)
fit <- factanal(PP, 5, rotation="promax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
# plot factor 1 by factor 2 
load <- fit$loadings[,1:2] 
plot(load,type="n") # set up plot 
text(load,labels=names(PP),cex=.7) # add variable names
#Build PP parcels
data2$PPp1 <-	 data2$PP4 + data2$PP2 + data2$PP3
data2$PPp2 <-  data2$PP6 + data2$PP7
data2$PPp3 <-  data2$PP8 + data2$PP10 
data2$PPp4 <-  data2$PP5 + data2$PP9
data2$PPp5 <-  data2$PP1

##Voice
VC <- data2[c(13:18)]
fit <- factanal(VC, 3, rotation="promax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
# plot factor 1 by factor 2 
load <- fit$loadings[,1:3] 
plot(load,type="n") # set up plot 
text(load,labels=names(VC),cex=.7) # add variable names
#Build VC parcels
data2$VCp1 <-  data2$VOICE2
data2$VCp2 <-  data2$VOICE3
data2$VCp3 <-  data2$VOICE5 + data2$VOICE4 + data2$VOICE6 + data2$VOICE1

##Taking Charge
TC <- data2[c(19:28)]
fit <- factanal(TC, 5, rotation="promax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
# plot factor 1 by factor 2 
load <- fit$loadings[,1:3] 
plot(load,type="n") # set up plot 
text(load,labels=names(TC),cex=.7) # add variable names
#Build TC parcels
data2$TCp1 <-  data2$TC3 + data2$TC4 + data2$TC5
data2$TCp2 <-  data2$TC9 + data2$TC10
data2$TCp3 <-  data2$TC8 + data2$TC7
data2$TCp4 <-  data2$TC1 + data2$TC2
data2$TCp5 <-  data2$TC6

##OCBI
OCBI <- data2[c(29:35)]
fit <- factanal(OCBI, 3, rotation="promax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
# plot factor 1 by factor 2 
load <- fit$loadings[,1:2] 
plot(load,type="n") # set up plot 
text(load,labels=names(OCBI),cex=.7) # add variable names
#Build OCBI parcels
data2$OCBIp1 <-  data2$OCBI4 + data2$OCBI5 + data2$OCBI7
data2$OCBIp2 <-  data2$OCBI1 + data2$OCBI2 + data2$OCBI3
data2$OCBIp3 <-  data2$OCBI6

##OCBO
OCBO <- data2[c(36:42)]
fit <- factanal(OCBO, 3, rotation="promax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
# plot factor 1 by factor 2 
load <- fit$loadings[,1:2] 
plot(load,type="n") # set up plot 
text(load,labels=names(OCBO),cex=.7) # add variable names
#Build OCBO parcels. OCBOp3 is a reverse-scored parcel and so this should be flipped before testing for equal method effects.
data2$OCBOp1	<-	data2$OCBO1 + data2$OCBO7
data2$OCBOp2  <-  data2$OCBO5 + data2$OCBO6 
data2$OCBOp3  <-  18-(data2$OCBO2 + data2$OCBO3 + data2$OCBO4)

##IRB
IRB <- data2[c(43:49)]
IRB <- na.omit(IRB)
fit <- factanal(IRB, 3, rotation="promax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
# plot factor 1 by factor 2 
load <- fit$loadings[,1:2] 
plot(load,type="n") # set up plot 
text(load,labels=names(IRB),cex=.7) # add variable names
#Build IRB parcels. IRBp2 is a reverse-scored parcel and so this should be flipped before testing for equal method effects.
data2$IRBp1	 <-	data2$IRB1 + data2$IRB2 + data2$IRB3 + data2$IRB4
data2$IRBp2  <-  12-(data2$IRB6 + data2$IRB7)
data2$IRBp3  <-  data2$IRB5

##Positive Affectivity
PA <- data2[c(50:59)]
PA <- na.omit(PA) 
fit <- factanal(PA, 3, rotation="promax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
# plot factor 1 by factor 2 
load <- fit$loadings[,1:2] 
plot(load,type="n") # set up plot 
text(load,labels=names(PA),cex=.7) # add variable names
#Build PA parcels
data2$PAp1	 <-	 data2$PA1 + data2$PA2 + data2$PA3 + data2$PA7 + data2$PA10 + data2$PA5
data2$PAp2   <-  data2$PA8 + data2$PA9 + data2$PA6
data2$PAp3   <-  data2$PA4

##Negative Affectivity
Na <- data2[c(60:69)]
Na <- na.omit(Na) 
fit <- factanal(Na, 5, rotation="promax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
# plot factor 1 by factor 2 
load <- fit$loadings[,1:2] 
plot(load,type="n") # set up plot 
text(load,labels=names(Na),cex=.7) # add variable names
#Build NA parcels
data2$Nap1	<-	data2$NA1 + data2$NA2 + data2$NA6
data2$Nap2  <-  data2$NA4 + data2$NA10
data2$Nap3  <-  data2$NA9 + data2$NA8
data2$Nap4  <-  data2$NA3
data2$Nap5  <-  data2$NA5 + data2$NA7

#Subset data2.
control <- subset(data2, COND > 0)
treatment <- subset(data2, COND < 1)

PP <- control[c(3:12)]
PP <- na.omit(PP)
reliability(PP)
PP$PP  <-  PP$PP1 + PP$PP2 + PP$PP3 + PP$PP4 + PP$PP5 + PP$PP6 + PP$PP7 + PP$PP8 + PP$PP9 + PP$PP10
PP <- PP[c(11)]
ID <- rownames(PP)
PP <- cbind(id=ID, PP)

VC <- control[c(13:18)]
VC <- na.omit(VC)
reliability(VC)
VC$VC  <-  VC$VOICE1 + VC$VOICE2 + VC$VOICE3 + VC$VOICE4 + VC$VOICE5 + VC$VOICE6
VC <- VC[c(7)]
ID <- rownames(VC)
VC <- cbind(id=ID, VC)
#Merge1
merge1 <- merge(PP,VC,by="id")

TC <- control[c(19:28)]
reliability(TC)
TC$TC  <-  TC$TC1 + TC$TC2 + TC$TC3 + TC$TC4 + TC$TC5 + TC$TC6 + TC$TC7 + TC$TC8 + TC$TC9 + TC$TC10
TC <- TC[c(11)]
ID <- rownames(TC)
TC <- cbind(id=ID, TC)
#Merge2
merge2 <- merge(merge1,TC,by="id")

OCBI <- control[c(29:35)]
reliability(OCBI)
OCBI$OCBI  <-  OCBI$OCBI1 + OCBI$OCBI2 + OCBI$OCBI3 + OCBI$OCBI4 + OCBI$OCBI5 + OCBI$OCBI6 + OCBI$OCBI7
OCBI <- OCBI[c(8)]
ID <- rownames(OCBI)
OCBI <- cbind(id=ID, OCBI)
#Merge3
merge3 <- merge(merge2,OCBI,by="id")

OCBO <- control[c(36:42)]
key <- c(1,-1,-1,-1,1,1,1)
OCBOr <- reverse.code(key,OCBO)
reliability(OCBOr)
OCBOr <- as.data.frame(OCBOr)
OCBOr$OCBO  <-  OCBOr$OCBO1 + OCBOr$OCBO2 + OCBOr$OCBO3 + OCBOr$OCBO4 + OCBOr$OCBO5 + OCBOr$OCBO6 + OCBOr$OCBO7
OCBO <- OCBOr[c(8)]
ID <- rownames(OCBO)
OCBO <- cbind(id=ID, OCBO)
#Merge4
merge4 <- merge(merge3,OCBO,by="id")

IRB <- control[c(43:49)]
key <- c(1,1,1,1,1,-1,-1)
IRBr <- reverse.code(key,IRB)
reliability(IRBr)
IRBr <- as.data.frame(IRBr)
IRBr$IRB  <-  IRBr$IRB1 + IRBr$IRB2 + IRBr$IRB3 + IRBr$IRB4 + IRBr$IRB5 + IRBr$IRB6 + IRBr$IRB7
IRB <- IRBr[c(8)]
ID <- rownames(IRB)
IRB <- cbind(id=ID, IRB)
#Merge5
merge5 <- merge(merge4,IRB,by="id")

PA <- control[c(50:59)]
reliability(PA)
PA$PA  <-  PA$PA1 + PA$PA2 + PA$PA3 + PA$PA4 + PA$PA5 + PA$PA6 + PA$PA7 + PA$PA8 + PA$PA9 + PA$PA10
PA <- PA[c(11)]
ID <- rownames(PA)
PA <- cbind(id=ID, PA)
#Merge6
merge6 <- merge(merge5,PA,by="id")

Na <- control[c(60:69)]
reliability(Na)
Na$Na  <-  Na$NA1 + Na$NA2 + Na$NA3 + Na$NA4 + Na$NA5 + Na$NA6 + Na$NA7 + Na$NA8 + Na$NA9 + Na$NA10
Na <- Na[c(11)]
ID <- rownames(Na)
Na <- cbind(id=ID, Na)
#Merge7
merge7 <- merge(merge6,Na,by="id")

#APA Style table
library(apaTables)
apa.cor.table(merge7, filename = NA, table.number = NA, show.conf.interval = FALSE, landscape = TRUE)

treatment <- subset(data2, COND < 1)
PP <- treatment[c(3:12)]
reliability(PP)
PP$PP  <-  PP$PP1 + PP$PP2 + PP$PP3 + PP$PP4 + PP$PP5 + PP$PP6 + PP$PP7 + PP$PP8 + PP$PP9 + PP$PP10
PP <- PP[c(11)]
ID <- rownames(PP)
PP <- cbind(id=ID, PP)

VC <- treatment[c(13:18)]
reliability(VC)
VC$VC  <-  VC$VOICE1 + VC$VOICE2 + VC$VOICE3 + VC$VOICE4 + VC$VOICE5 + VC$VOICE6
VC <- VC[c(7)]
ID <- rownames(VC)
VC <- cbind(id=ID, VC)
#Merge1
merge1 <- merge(PP,VC,by="id")

TC <- treatment[c(19:28)]
reliability(TC)
TC$TC  <-  TC$TC1 + TC$TC2 + TC$TC3 + TC$TC4 + TC$TC5 + TC$TC6 + TC$TC7 + TC$TC8 + TC$TC9 + TC$TC10
TC <- TC[c(11)]
ID <- rownames(TC)
TC <- cbind(id=ID, TC)
#Merge2
merge2 <- merge(merge1,TC,by="id")

OCBI <- treatment[c(29:35)]
reliability(OCBI)
OCBI$OCBI  <-  OCBI$OCBI1 + OCBI$OCBI2 + OCBI$OCBI3 + OCBI$OCBI4 + OCBI$OCBI5 + OCBI$OCBI6 + OCBI$OCBI7
OCBI <- OCBI[c(8)]
ID <- rownames(OCBI)
OCBI <- cbind(id=ID, OCBI)
#Merge3
merge3 <- merge(merge2,OCBI,by="id")

OCBO <- treatment[c(36:42)]
key <- c(1,-1,-1,-1,1,1,1)
OCBOr <- reverse.code(key,OCBO)
reliability(OCBOr)
OCBOr <- as.data.frame(OCBOr)
OCBOr$OCBO  <-  OCBOr$OCBO1 + OCBOr$OCBO2 + OCBOr$OCBO3 + OCBOr$OCBO4 + OCBOr$OCBO5 + OCBOr$OCBO6 + OCBOr$OCBO7
OCBO <- OCBOr[c(8)]
ID <- rownames(OCBO)
OCBO <- cbind(id=ID, OCBO)
#Merge4
merge4 <- merge(merge3,OCBO,by="id")

IRB <- treatment[c(43:49)]
key <- c(1,1,1,1,1,-1,-1)
IRBr <- reverse.code(key,IRB)
reliability(IRBr)
IRBr <- as.data.frame(IRBr)
IRBr$IRB  <-  IRBr$IRB1 + IRBr$IRB2 + IRBr$IRB3 + IRBr$IRB4 + IRBr$IRB5 + IRBr$IRB6 + IRBr$IRB7
IRB <- IRBr[c(8)]
ID <- rownames(IRB)
IRB <- cbind(id=ID, IRB)
#Merge5
merge5 <- merge(merge4,IRB,by="id")

PA <- treatment[c(50:59)]
reliability(PA)
PA$PA  <-  PA$PA1 + PA$PA2 + PA$PA3 + PA$PA4 + PA$PA5 + PA$PA6 + PA$PA7 + PA$PA8 + PA$PA9 + PA$PA10
PA <- PA[c(11)]
ID <- rownames(PA)
PA <- cbind(id=ID, PA)
#Merge6
merge6 <- merge(merge5,PA,by="id")

Na <- treatment[c(60:69)]
reliability(Na)
Na$Na  <-  Na$NA1 + Na$NA2 + Na$NA3 + Na$NA4 + Na$NA5 + Na$NA6 + Na$NA7 + Na$NA8 + Na$NA9 + Na$NA10
Na <- Na[c(11)]
ID <- rownames(Na)
Na <- cbind(id=ID, Na)
#Merge7
merge7 <- merge(merge6,Na,by="id")

#APA Style table
library(apaTables)
apa.cor.table(merge7, filename = NA, table.number = NA, show.conf.interval = FALSE, landscape = TRUE)

#Delete items
data2$PP1 <- NULL
data2$PP2 <- NULL
data2$PP3 <- NULL
data2$PP4 <- NULL
data2$PP5 <- NULL
data2$PP6 <- NULL
data2$PP7 <- NULL
data2$PP8 <- NULL
data2$PP9 <- NULL
data2$PP10 <- NULL
data2$VOICE1 <- NULL
data2$VOICE2 <- NULL
data2$VOICE3 <- NULL
data2$VOICE4 <- NULL
data2$VOICE5 <- NULL
data2$VOICE6 <- NULL
data2$TC1 <- NULL
data2$TC2 <- NULL
data2$TC3 <- NULL
data2$TC4 <- NULL
data2$TC5 <- NULL
data2$TC6 <- NULL
data2$TC7 <- NULL
data2$TC8 <- NULL
data2$TC9 <- NULL
data2$TC10 <- NULL
data2$OCBI1 <- NULL
data2$OCBI2 <- NULL
data2$OCBI3 <- NULL
data2$OCBI4 <- NULL
data2$OCBI5 <- NULL
data2$OCBI6 <- NULL
data2$OCBI7 <- NULL
data2$OCBO1 <- NULL
data2$OCBO2 <- NULL
data2$OCBO3 <- NULL
data2$OCBO4 <- NULL
data2$OCBO5 <- NULL
data2$OCBO6 <- NULL
data2$OCBO7 <- NULL
data2$IRB1 <- NULL
data2$IRB2 <- NULL
data2$IRB3 <- NULL
data2$IRB4 <- NULL
data2$IRB5 <- NULL
data2$IRB6 <- NULL
data2$IRB7 <- NULL
data2$PA1 <- NULL
data2$PA2 <- NULL
data2$PA3 <- NULL
data2$PA4 <- NULL
data2$PA5 <- NULL
data2$PA6 <- NULL
data2$PA7 <- NULL
data2$PA8 <- NULL
data2$PA9 <- NULL
data2$PA10 <- NULL
data2$NA1 <- NULL
data2$NA2 <- NULL
data2$NA3 <- NULL
data2$NA4 <- NULL
data2$NA5 <- NULL
data2$NA6 <- NULL
data2$NA7 <- NULL
data2$NA8 <- NULL
data2$NA9 <- NULL
data2$NA10 <- NULL
```
#Results
##Did We Capture a Momentary Mood State?
To determine if we captured momentary mood state, we examined the convergent and discriminant validity of our momentary mood measure by modeling positive and negative affectivity as correlates of momentary mood within a multigroup factor analysis.Across both groups, the daily mood measure correlated positively with positive affectivity (Temporally Separated Group: r = .27, p = .008; Same-Time-Point Group: r = .37, p = .002). For negative affectivity, though there was a consistent negative relationship across both conditions, the magnitude of this effect was only statistically significant in the  temporally separated group (Temporally Separated Group: r = -.24, p = .018; Same-Time-Point Group: r = -.12, p = .313), which was not expected. Across both condition, there was a negative correlation linking momentary mood state negative affect (r = -.23, p < .001). 
```{r}
affect.m <- '
  PA =~ PAp1 + PAp2 + PAp3 
  NA =~ Nap1 + Nap2 + Nap3 + Nap4 + Nap5
  MOOD =~ MOOD_T1
'
affect.cfa <- cfa(affect.m, estimator = "MLM", data = data2, group = "COND", std.lv = TRUE)
#Group 1 is the temporally separted condition
#Group 2 is the same-time-point condition
summary(affect.cfa, standardized = TRUE, fit.measures = TRUE)
```
To examine the temporal instability of momentary mood, we examined the test-retest correlation for individuals given this measure twice (r = .44, p < .001), which indicated a moderate to large effect linking momentary mood state separated by a one week separation of time.
```{r}
a <- data2$MOOD_T1
b <- data2$MOOD_T2
boxplot(a,b)
#No, but the correlation isn't particularly strong.
cor(a,b, use="complete")
library(car)
scatterplot(a, b)
```

```{r}
#CONTROL
initial.sem.c  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4 + PPp5
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3 + TCp4 + TCp5
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ PR
  TC ~ PR
  IRB ~ PR
  OCBI ~ PR
  OCBO ~ PR
#Method Factors
  PA =~ PAp1 + PAp2 + PAp3
  Na =~ Nap1 + Nap2 + Nap3 + Nap4 + Nap5
  M =~ MOOD_T1
  Neg =~ IRBp2 + OCBOp3
'
##Obtain unstandardized factor loadings and residual residual variances to levels found in the initial CFA, which should be fixed in the baseline models. 
initial.c <- cfa(initial.sem.c, estimator = "MLM", data=control, std.lv = TRUE)
summary(initial.c, standardized = TRUE, fit.measures = TRUE)

#Control Baseline
#Fix factor loadings and residual variances to initial levels.
baseline.sem.c  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4 + PPp5
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3 + TCp4 + TCp5
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ PR
  TC ~ PR
  IRB ~ PR
  OCBI ~ PR
  OCBO ~ PR
#Define Method Factors
  PA =~ 4.660*PAp1 + 1.898*PAp2 + 0.794*PAp3
  Na =~ 1.994*Nap1 + 1.509*Nap2 + 1.319*Nap3 + 0.685*Nap4 + 1.388*Nap5
  M =~ 1.332*MOOD_T1
  Neg =~ 0.875*IRBp2 + 1.905*OCBOp3
#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 
#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO  
#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO 
#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M
#Method Regressions
  M ~ PA + Na
#Fixed residuals
  PAp1 ~~ 0.955*PAp1
  PAp2 ~~ 2.552*PAp2
  PAp3 ~~ 0.382*PAp3
  Nap1 ~~ 2.580*Nap1
  Nap2 ~~ 0.898*Nap2
  Nap3 ~~ 1.373*Nap3
  Nap4 ~~ 0.348*Nap4
  Nap5 ~~ 0.872*Nap5
  IRBp2 ~~ 1.401*IRBp2
  OCBOp3 ~~ 2.062*OCBOp3
  MOOD_T1 ~~ 0*MOOD_T1
'
baseline.c <- cfa(baseline.sem.c, estimator = "MLM", data=control, std.lv = TRUE)
summary(baseline.c, standardized = TRUE, fit.measures = TRUE)

#Method C
methodc.sem.c  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4 + PPp5
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3 + TCp4 + TCp5
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ PR
  TC ~ PR
  IRB ~ PR
  OCBI ~ PR
  OCBO ~ PR
#Define Method Factors
  PA =~ 4.660*PAp1 + 1.898*PAp2 + 0.794*PAp3 + v1*PPp1 + v1*PPp2 + v1*PPp3 + v1*PPp4 + v1*PPp5 + v1*VCp1 + v1*VCp2 + v1*VCp3 + v1*TCp1 + v1*TCp2 + v1*TCp3 + v1*TCp4 + v1*TCp5 + v1*IRBp1 + v1*IRBp2 + v1*IRBp3 + v1*OCBIp1 + v1*OCBIp2 + v1*OCBIp3 + v1*OCBOp1 + v1*OCBOp2 + v1*OCBOp3
  Na =~ 1.994*Nap1 + 1.509*Nap2 + 1.319*Nap3 + 0.685*Nap4 + 1.388*Nap5 + v2*PPp1 + v2*PPp2 + v2*PPp3 + v2*PPp4 + v2*PPp5 + v2*VCp1 + v2*VCp2 + v2*VCp3 + v2*TCp1 + v2*TCp2 + v2*TCp3 + v2*TCp4 + v2*TCp5 + v2*IRBp1 + v2*IRBp2 + v2*IRBp3 + v2*OCBIp1 + v2*OCBIp2 + v2*OCBIp3 + v2*OCBOp1 + v2*OCBOp2 + v2*OCBOp3
  M =~ 1.332*MOOD_T1 + v3*PPp1 + v3*PPp2 + v3*PPp3 + v3*PPp4 + v3*PPp5 + v3*VCp1 + v3*VCp2 + v3*VCp3 + v3*TCp1 + v3*TCp2 + v3*TCp3 + v3*TCp4 + v3*TCp5 + v3*IRBp1 + v3*IRBp2 + v3*IRBp3 + v3*OCBIp1 + v3*OCBIp2 + v3*OCBIp3 + v3*OCBOp1 + v3*OCBOp2 + v3*OCBOp3
  Neg =~ 0.875*IRBp2 + 1.905*OCBOp3
#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 
#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO  
#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO  
#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M
#Method Regressions
  M ~ PA + Na
#Fixed residuals
  PAp1 ~~ 0.955*PAp1
  PAp2 ~~ 2.552*PAp2
  PAp3 ~~ 0.382*PAp3
  Nap1 ~~ 2.580*Nap1
  Nap2 ~~ 0.898*Nap2
  Nap3 ~~ 1.373*Nap3
  Nap4 ~~ 0.348*Nap4
  Nap5 ~~ 0.872*Nap5
  IRBp2 ~~ 1.401*IRBp2
  OCBOp3 ~~ 2.062*OCBOp3
  MOOD_T1 ~~ 0*MOOD_T1
'
methodc.sem.c <- cfa(methodc.sem.c, estimator = "MLM", data=control, std.lv = TRUE)
summary(methodc.sem.c, standardized = TRUE, fit.measures = TRUE)
anova(baseline.c,methodc.sem.c)
#Baseline is statistically significant from model c. 

#METHOD U
methodu.sem.c  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4 + PPp5
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3 + TCp4 + TCp5
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ PR
  TC ~ PR
  IRB ~ PR
  OCBI ~ PR
  OCBO ~ PR
#Define Method Factors
  PA =~ 4.660*PAp1 + 1.898*PAp2 + 0.794*PAp3 + PPp1 + PPp2 + PPp3 + PPp4 + PPp5 + VCp1 + VCp2 + VCp3 + TCp1 + TCp2 + TCp3 + TCp4 + TCp5 + IRBp1 + OCBIp1 + OCBIp2 + OCBIp3 + OCBOp1 + OCBOp2
  Na =~ 1.994*Nap1 + 1.509*Nap2 + 1.319*Nap3 + 0.685*Nap4 + 1.388*Nap5 + PPp1 + PPp2 + IRBp2 + OCBOp3
  M =~ 1.332*MOOD_T1
#+ OCBIp3
  Neg =~ 0.875*IRBp2 + 1.905*OCBOp3
#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 
#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO  
#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO  
#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M
#Method Regressions
  M ~ PA + Na
#Fixed residuals
  PAp1 ~~ 0.955*PAp1
  PAp2 ~~ 2.552*PAp2
  PAp3 ~~ 0.382*PAp3
  Nap1 ~~ 2.580*Nap1
  Nap2 ~~ 0.898*Nap2
  Nap3 ~~ 1.373*Nap3
  Nap4 ~~ 0.348*Nap4
  Nap5 ~~ 0.872*Nap5
  IRBp2 ~~ 1.401*IRBp2
  OCBOp3 ~~ 2.062*OCBOp3
  MOOD_T1 ~~ 0*MOOD_T1
'
methodu.sem.c <- cfa(methodu.sem.c, estimator = "MLM", data=control, std.lv = TRUE)
summary(methodu.sem.c, standardized = TRUE, fit.measures = TRUE)
anova(methodu.sem.c,methodc.sem.c)
anova(baseline.method.model.c, method.sem.model.u)
#Method U is superior to baseline model. Congeneric CMV is present. 

#METHOD R
methodr.sem.c  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4 + PPp5
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3 + TCp4 + TCp5
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ 1.212*PR
  TC ~ 1.165*PR
  IRB ~ 0.416*PR
  OCBI ~ 0.419*PR
  OCBO ~ 0.435*PR
#Define Method Factors
  PA =~ 4.660*PAp1 + 1.898*PAp2 + 0.794*PAp3 + PPp1 + PPp2 + PPp3 + PPp4 + PPp5 + VCp1 + VCp2 + VCp3 + TCp1 + TCp2 + TCp3 + TCp4 + TCp5 + IRBp1 + OCBIp1 + OCBIp2 + OCBIp3 + OCBOp1 + OCBOp2
  Na =~ 1.994*Nap1 + 1.509*Nap2 + 1.319*Nap3 + 0.685*Nap4 + 1.388*Nap5 + PPp1 + PPp2 + IRBp2 + OCBOp3
  M =~ 1.332*MOOD_T1 + OCBIp3
  Neg =~ 0.875*IRBp2 + 1.905*OCBOp3
#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 
#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO  
#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO  
#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M
#Method Regressions
  M ~ PA + Na
#Fixed residuals
  PAp1 ~~ 0.955*PAp1
  PAp2 ~~ 2.552*PAp2
  PAp3 ~~ 0.382*PAp3
  Nap1 ~~ 2.580*Nap1
  Nap2 ~~ 0.898*Nap2
  Nap3 ~~ 1.373*Nap3
  Nap4 ~~ 0.348*Nap4
  Nap5 ~~ 0.872*Nap5
  IRBp2 ~~ 1.401*IRBp2
  OCBOp3 ~~ 2.062*OCBOp3
  MOOD_T1 ~~ 0*MOOD_T1
'
methodr.sem.c <- cfa(methodr.sem.c, estimator = "MLM", data=control, std.lv = TRUE)
summary(methodr.sem.c, standardized = TRUE, fit.measures = TRUE)
anova(methodr.sem.c,methodu.sem.c)
#NOTE: Method effects attributable to positive affectivity, negative affectivity, mood, and negative item wording were observed. However, a test for method effects attributable to consistency motifs was not observed.
```
```{r}
#TREATMENT
initial.t  <-  '
#Substantive Factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4 + PPp5
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3 + TCp4 + TCp5
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ PR
  TC ~ PR
  IRB ~ PR
  OCBI ~ PR
  OCBO ~ PR
#Method Factors
  PA =~ PAp1 + PAp2 + PAp3
  Na =~ Nap1 + Nap2 + Nap3 + Nap4 + Nap5
  M =~ MOOD_T1
  Neg =~ IRBp2 + OCBOp3
'
##Obtain unstandardized factor loadings and residual residual variances to levels found in the initial CFA, which should be fixed in the baseline models. 
initial.sem.t <- cfa(initial.t, estimator = "MLM", data=treatment, std.lv = TRUE)
summary(initial.sem.t, standardized = TRUE, fit.measures = TRUE)

baseline.t  <-  '
  PR =~ PPp1 + PPp2 + PPp3 + PPp4 + PPp5
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3 + TCp4 + TCp5
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ PR
  TC ~ PR
  IRB ~ PR
  OCBI ~ PR
  OCBO ~ PR
#Define Method Factors
  PA =~ 4.019*PAp1 + 1.864*PAp2 + 0.801*PAp3
  Na =~ 2.302*Nap1 + 1.728*Nap2 + 1.558*Nap3 + 0.908*Nap4 + 1.692*Nap5
  M =~ 1.297*MOOD_T1
  Neg =~ 2.072*IRBp2 + 3.182*OCBOp3
#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 
#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO  
#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO 
#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M
#Method Regressions
  M ~ PA + Na
#Fixed residuals
  PAp1 ~~ 3.091*PAp1
  PAp2 ~~ 2.483*PAp2
  PAp3 ~~ 0.258*PAp3
  Nap1 ~~ 3.013*Nap1
  Nap2 ~~ 0.588*Nap2
  Nap3 ~~ 0.876*Nap3
  Nap4 ~~ 0.335*Nap4
  Nap5 ~~ 0.623*Nap5
  IRBp2 ~~ 1.341*IRBp2
  OCBOp3 ~~ 2.563*OCBOp3
  MOOD_T1 ~~ 0*MOOD_T1
'
baseline.sem.t <- cfa(baseline.t, estimator = "MLM", data=treatment, std.lv = TRUE)
summary(baseline.sem.t, standardized = TRUE, fit.measures = TRUE)
anova(baseline.sem.t, initial.sem.t)
#Sig. Diff.

#Method C
methodc.t  <-  '
  PR =~ PPp1 + PPp2 + PPp3 + PPp4 + PPp5
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3 + TCp4 + TCp5
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ PR
  TC ~ PR
  IRB ~ PR
  OCBI ~ PR
  OCBO ~ PR
#Define Method Factors
  PA =~ 4.019*PAp1 + 1.864*PAp2 + 0.801*PAp3 + v1*PPp1 + v1*PPp2 + v1*PPp3 + v1*PPp4 + v1*PPp5 + v1*VCp1 + v1*VCp2 + v1*VCp3 + v1*TCp1 + v1*TCp2 + v1*TCp3 + v1*TCp4 + v1*TCp5 + v1*IRBp1 + v1*IRBp2 + v1*IRBp3 + v1*OCBIp1 + v1*OCBIp2 + v1*OCBIp3 + v1*OCBOp1 + v1*OCBOp2 + v1*OCBOp3
  Na =~ 2.302*Nap1 + 1.728*Nap2 + 1.558*Nap3 + 0.908*Nap4 + 1.692*Nap5 + v2*PPp1 + v2*PPp2 + v2*PPp3 + v2*PPp4 + v2*PPp5 + v2*VCp1 + v2*VCp2 + v2*VCp3 + v2*TCp1 + v2*TCp2 + v2*TCp3 + v2*TCp4 + v2*TCp5 + v2*IRBp1 + v2*IRBp2 + v2*IRBp3 + v2*OCBIp1 + v2*OCBIp2 + v2*OCBIp3 + v2*OCBOp1 + v2*OCBOp2 + v2*OCBOp3
  M =~ 1.297*MOOD_T1 + v3*PPp1 + v3*PPp2 + v3*PPp3 + v3*PPp4 + v3*PPp5 + v3*VCp1 + v3*VCp2 + v3*VCp3 + v3*TCp1 + v3*TCp2 + v3*TCp3 + v3*TCp4 + v3*TCp5 + v3*IRBp1 + v3*IRBp2 + v3*IRBp3 + v3*OCBIp1 + v3*OCBIp2 + v3*OCBIp3 + v3*OCBOp1 + v3*OCBOp2 + v3*OCBOp3
  Neg =~ 2.072*IRBp2 + 3.182*OCBOp3
#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 
#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO  
#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO 
#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M
#Method Regressions
  M ~ PA + Na
#Fixed residuals
  PAp1 ~~ 3.091*PAp1
  PAp2 ~~ 2.483*PAp2
  PAp3 ~~ 0.258*PAp3
  Nap1 ~~ 3.013*Nap1
  Nap2 ~~ 0.588*Nap2
  Nap3 ~~ 0.876*Nap3
  Nap4 ~~ 0.335*Nap4
  Nap5 ~~ 0.623*Nap5
  IRBp2 ~~ 1.341*IRBp2
  OCBOp3 ~~ 2.563*OCBOp3
  MOOD_T1 ~~ 0*MOOD_T1
'
methodc.sem.t <- cfa(methodc.t, estimator = "MLM", data=treatment, std.lv = TRUE)
summary(methodc.sem.t, standardized = TRUE, fit.measures = TRUE)
anova(methodc.sem.t,baseline.sem.t)
#Heywood cases emerged (2; IRBp1, OCBOp3). They were non-significant.

#Method U
methodu.t  <-  '
  PR =~ PPp1 + PPp2 + PPp3 + PPp4 + PPp5
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3 + TCp4 + TCp5
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ PR
  TC ~ PR
  IRB ~ PR
  OCBI ~ PR
  OCBO ~ PR
#Define Method Factors
  PA =~ 4.019*PAp1 + 1.864*PAp2 + 0.801*PAp3 + PPp1 + PPp2 + PPp3 + PPp4 + PPp5 + VCp1 + VCp2 + VCp3 + TCp1 + TCp2 +TCp3 + TCp4 + TCp5 + IRBp1 + OCBIp1 + OCBIp2 + OCBIp3 + OCBOp1 + OCBOp2
  Na =~ 2.302*Nap1 + 1.728*Nap2 + 1.558*Nap3 + 0.908*Nap4 + 1.692*Nap5 + PPp2 + PPp3 + PPp4 + PPp5 + VCp2 + TCp1 + TCp2 + TCp5 + IRBp1 + IRBp2 + OCBIp2 + OCBOp1 + OCBOp3
  M =~ 1.297*MOOD_T1
  Neg =~ 2.072*IRBp2 + 3.182*OCBOp3
#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 
#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO  
#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO 
#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M
#Method Regressions
  M ~ PA + Na
#Fixed residuals
  PAp1 ~~ 3.091*PAp1
  PAp2 ~~ 2.483*PAp2
  PAp3 ~~ 0.258*PAp3
  Nap1 ~~ 3.013*Nap1
  Nap2 ~~ 0.588*Nap2
  Nap3 ~~ 0.876*Nap3
  Nap4 ~~ 0.335*Nap4
  Nap5 ~~ 0.623*Nap5
  IRBp2 ~~ 1.341*IRBp2
  OCBOp3 ~~ 2.563*OCBOp3
  MOOD_T1 ~~ 0*MOOD_T1
'
methodu.sem.t <- cfa(methodu.t, estimator = "MLM", data=treatment, std.lv = TRUE)
summary(methodu.sem.t, standardized = TRUE, fit.measures = TRUE)
anova(methodu.sem.t,methodc.sem.t)

#METHOD R
methodr.t  <-  '
  PR =~ PPp1 + PPp2 + PPp3 + PPp4 + PPp5
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3 + TCp4 + TCp5
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Substantive Regressions
  VC ~ 1.294*PR
  TC ~ 0.883*PR
  IRB ~ 0.382*PR
  OCBI ~ 0.607*PR
  OCBO ~ 0.523*PR
#Define Method Factors
  PA =~ 4.019*PAp1 + 1.864*PAp2 + 0.801*PAp3 + PPp1 + PPp2 + PPp3 + PPp4 + PPp5 + VCp1 + VCp2 + VCp3 + TCp1 + TCp2 +TCp3 + TCp4 + TCp5 + IRBp1 + OCBIp1 + OCBIp2 + OCBIp3 + OCBOp1 + OCBOp2
  Na =~ 2.302*Nap1 + 1.728*Nap2 + 1.558*Nap3 + 0.908*Nap4 + 1.692*Nap5 + PPp2 + PPp3 + PPp4 + PPp5 + VCp2 + TCp1 + TCp2 + TCp5 + IRBp1 + IRBp2 + OCBIp2 + OCBOp1 + OCBOp3
  M =~ 1.297*MOOD_T1
  Neg =~ 2.072*IRBp2 + 3.182*OCBOp3
#Fix PA covariances to zero. 
  PA ~~ 0*PR
  PA ~~ 0*VC
  PA ~~ 0*TC
  PA ~~ 0*IRB
  PA ~~ 0*OCBI
  PA ~~ 0*OCBO  
  PA ~~ 0*Na 
#Fix NA covariances to zero.
  Na ~~ 0*PR
  Na ~~ 0*VC
  Na ~~ 0*TC
  Na ~~ 0*IRB
  Na ~~ 0*OCBI
  Na ~~ 0*OCBO  
#Fix all mood covariances to zero.
  M ~~ 0*PR
  M ~~ 0*VC
  M ~~ 0*TC
  M ~~ 0*IRB
  M ~~ 0*OCBI
  M ~~ 0*OCBO 
#Fix all negative item wording covariances to zero.
  Neg ~~ 0*PR
  Neg ~~ 0*VC
  Neg ~~ 0*TC
  Neg ~~ 0*IRB
  Neg ~~ 0*OCBI
  Neg ~~ 0*OCBO
  Neg ~~ 0*PA
  Neg ~~ 0*Na
  Neg ~~ 0*M
#Method Regressions
  M ~ PA + Na
#Fixed residuals
  PAp1 ~~ 3.091*PAp1
  PAp2 ~~ 2.483*PAp2
  PAp3 ~~ 0.258*PAp3
  Nap1 ~~ 3.013*Nap1
  Nap2 ~~ 0.588*Nap2
  Nap3 ~~ 0.876*Nap3
  Nap4 ~~ 0.335*Nap4
  Nap5 ~~ 0.623*Nap5
  IRBp2 ~~ 1.341*IRBp2
  OCBOp3 ~~ 2.563*OCBOp3
  MOOD_T1 ~~ 0*MOOD_T1
'
methodr.sem.t <- cfa(methodr.t, estimator = "MLM", data=control, std.lv = TRUE)
summary(methodr.sem.t, standardized = TRUE, fit.measures = TRUE)
anova(methodu.sem.t,methodr.sem.t)
```
```{r}
parameterEstimates(methodC.c)
#        lhs op     rhs label   est    se      z pvalue ci.lower ci.upper
#144      PR ~~     IRB       0.279 0.068  4.128  0.000    0.147    0.412
#145      PR ~~    OCBI       0.335 0.099  3.383  0.001    0.141    0.529
#146      PR ~~    OCBO       0.306 0.100  3.069  0.002    0.111    0.502
#148      VC ~~     IRB       0.306 0.097  3.160  0.002    0.116    0.495
#149      VC ~~    OCBI       0.401 0.103  3.870  0.000    0.198    0.603
#150      VC ~~    OCBO       0.340 0.102  3.351  0.001    0.141    0.539
#151      TC ~~     IRB       0.189 0.066  2.853  0.004    0.059    0.319
#152      TC ~~    OCBI       0.376 0.076  4.922  0.000    0.227    0.526
#153      TC ~~    OCBO       0.285 0.083  3.430  0.001    0.122    0.447

parameterEstimates(methodC.t)
#        lhs op     rhs label   est    se      z pvalue ci.lower ci.upper
#144      PR ~~     IRB       0.339 0.134  2.527  0.012    0.076    0.601
#145      PR ~~    OCBI       0.541 0.126  4.299  0.000    0.294    0.788
#146      PR ~~    OCBO       0.437 0.135  3.241  0.001    0.173    0.702
#148      VC ~~     IRB       0.446 0.112  3.970  0.000    0.226    0.666
#149      VC ~~    OCBI       0.621 0.112  5.564  0.000    0.402    0.839
#150      VC ~~    OCBO       0.515 0.122  4.212  0.000    0.276    0.755
#151      TC ~~     IRB       0.476 0.102  4.646  0.000    0.275    0.676
#152      TC ~~    OCBI       0.716 0.063 11.339  0.000    0.592    0.840
#153      TC ~~    OCBO       0.588 0.093  6.345  0.000    0.406    0.769
```
# Discussion
  Our results speak to a more nuanced view of method variance that has been offered by Spector, Rosen, Richardson, Williams, and Johnson (2016), who urge researchers to focus less on broad solutions to CMV  (e.g., broad set of remedies for proximal method effects and statistical controls that assume unidimensional method effects) and more more on the extraneuous effects that are introduced by the measurement strategy employed in an investigation, which they refer to as either common or unique method variance. To explain by way of example, studies seeking to estimate the effect of attitudinal factors on behaviors should consider sources of variance that are common to all measures in the study (e.g., mood and social desirability) as well as factors unique to specific measures in the study (e.g., response sets might affect self-reports of attitudes while impression management might affect reports of sensitive behaviors, like deviance). Either common or unique sources of variance could dominate a particular study, or they could also cancel each other out. Either way, they introduce variation into the measures utilized in an investigation, which can result in either inflated or attentuated effect size estimates. They also encourage research into the developmentof a knowledge base upon which to rely in deciding how method variance could be addressed in investigations, which will require accumulating knowledge about soruces of methods to consider and control in a study. To this point, while our investigation focused on certain common method variance sources (e.g., consistency motifs, affectivity, mood), we did not exhaustively study the role played by all possible sources (e.g., social desirability). We also neglected to address the role played by uncommon method variance sources (e.g., impression management, which seems relevant for self-reports of IRB, limited information, which seems relevant for self-reports of behavior; and response sets, which seems relevant for proactive personalitiy). Therefore, we encourage future reserach to investigate the role played by these sources of nuissance variance. 
  Additionally, some of our method effects were assumed to be measured without error (e.g., consistency motifs and daily mood). Clearly, this false. [Could we conduct a sensitivity analysis to address this concern? I might need help here.]
  Our item blocking strategy may have also played a role [Discuss that article that Alice shared].
  
```{r}
#To diagnoose the source of model mis-fit, we consulted the modificaiton indices. It is worth noting that of the 48 MI values that were greater than 10 (a commonly used cutoff for determinig which residuals to correlate), 32 were observed in the non-remedied condition and 16 were observed in the non-remedied condition. This suggests that proximal remedies appear to reduce the number of correlated residuals in one's data, perhaps by as much as 50%. 

#Given our desire to present credible parameter estimates, both substantive and methodological, we decided to adopt a correlated residuals strategy. Specifically, we allowed residuals to corrrelate if (1) they occurred for items within the same scale, (2) correlated across both conditions and (3) if freeing these residual covariances would improve model-data fit. Though conducted post-hoc, we followed this strategy because these residual correlations would appear to be a persistent problem with either the measure or a self-report administration of this measure (or possibly both). In other words, our study allows us to examine which residual covariances are cross-validated. However, residuals that do not correlate acorss conditions could be correlated for condition specific reasons (e.g., item order effects). Though freeing these residual covariances might also be principled (see Cole et al., 2007), doing so in our study would act contrary to our goal of testing the invariance of substantive parameters under different proximal CMV conditions. This decision making process resulted in specifying/allowing for four residual covariances in our model, which did provide a significant improvement in model data fit (p < .001) and generally improved upon model-data fit statistics (X^2^ = 2724.47, df = 2020, p < .001; rCFI = .87, rTLI = .86, rRMSEA = .055, SRMR = .081).

```{r}
cfa.model.m<- '
  PR =~ PP1 + PP2 + PP3 + PP4 + PP5 + PP6 + PP7 + PP8 + PP9 + PP10 
  VC =~ VC1 + VC2 + VC3 + VC4 + VC5 + VC6
  TC =~ TC1 + TC2 + TC3 + TC4 + TC5 + TC6 + TC7 + TC8 + TC9 + TC10
  IRB =~ IRB1 + IRB2 + IRB3 + IRB4 + IRB5 
  OCBI =~ OCBI1 + OCBI2 + OCBI3 + OCBI4 + OCBI5 + OCBI6 + OCBI7
  OCBO =~ OCBO1 + OCBO2 + OCBO6 + OCBO7
OCBI1~~OCBI2
OCBO6~~OCBO7
PP2~~PP4
PP5~~PP9
'

model1.m <- cfa(cfa.model.m, estimator = "MLM", data=data, group="COND")
summary(model1.m, standardized = TRUE, fit.measures = TRUE)
anova(model1,model1.m)
```

We proceded to test measurement invariance by imposing equality constraints on (1) item factor loadings (weak invariance), (2) item intercepts (strong invariance), and lastly (3) latent covariances.

```{r}
#Measurement invariance.
model2 <- cfa(cfa.model.m, estimator = "MLM", data=data, group="COND", group.equal=c("loadings"))
summary(model2,fit.measures=TRUE, standardized=TRUE)
anova(model1.m, model2)
```
The results of testing for weak invariance suggested invariant factor loadings (SBX^2^dif = 48.42, df = 45, p = .34). In other words, proximal causes of method variance did not appear to cause items to become systematically more discriminating for their respective latent constructs, contrary to what was observed by Steinberg (1994). Having satisfied weak invariance, we proceeded to test for strong invariance by examing invariant item intercepts.

```{r}
#Measurement invariance.
model3 <- cfa(cfa.model.m, estimator = "MLM", data=data, group="COND", group.equal=c("loadings","intercepts"))
summary(model3, fit.measures=TRUE, standardized=TRUE)
anova(model2, model3)
```
The results of testing for strong invariance suggested non-invariant item intercepts (SBX^2^dif = 255.65, df = 40, p < .001), indicating non-invariance of item factor loadings, invariances, or both. Examining the modification indices allows us to inspect the pattern of proximally caused method variance. 

```{r}
lavTestScore(model3)
#Free the intercepts (p<.05).
#             			G1		G2
#p133 #.040 - PP2	5.455		6.013
#p142 #.032 - VC1	4.660		4.164
#p146 #.032 - VC5	3.788		4.382
#p149 #.013 - TC2	4.553		4.369
#p155 #.018 - TC8	4.213		4.438
#p156 #.004 - TC9	3.846		3.732
#p165 #.030 - OCBI1	4.763		4.468
#p166 #.016 - OCBI2	4.293		4.754

#Free the factor loadings (p< .05). 
#p2 #.013 - PP2
#p6 #.048 - PP6
#p19 #.043 - TC3
#p25 #.011 - TC9
#p37 #.004 - OCBI4
#p45 #.017 - OCBO5
#p50 #.004 - OCBO3

```

Examining the modification indices revealed the presence of eight non-invariant intercepts (p < .05), the pattern of which was not consistent with the contaminating influences of proximally caused method variance. For two scales (voice and OCBI), for which there were two non-invariant intercepts, these intercepts were such that the overall impact on the scale mean would be negligable (i.e., the direction of the intercept were in opposition for a scale, canceling each other out). By contrast, for proactive personality, a single item contained a higher intercept (PP2). For taking charge, there were three invariant intercepts in inconsistent directions (two driving the scale mean higher in the remedied condition, one diriving the scale mean higher in the control condition). It should be noted that the observed scale means do not vary across conditions [note: I ran t-tests for this, and can provide the code if needed], so though some item intercepts were non-invariant, the magnitude of this non-invariance was not large enough to produce differences in the scale means. 

Given such negligable influence of these non-invariant parameters, we freed the non-invariant intercepts and tested partial scalar invariance. Initial testing suggested that certain factor loadings would also need to be freed for partial scalar invariance to be achieved. 

```{r}
#Initial testing. Still a significant difference, though small (p < .01)
model3a <- cfa(cfa.model.m, estimator = "MLM", data=data, group="COND", group.equal=c("loadings","intercepts"), group.partial=c("PP2~1","VC1~1","VC5~1","TC2~1","TC8~1","TC9~1", "OCBI1~1", "OCBI2~1"))
summary(model3a, fit.measures=TRUE, standardized=TRUE)
anova(model2,model3a)
```

Freeing seven non-invariant factor loadings provided evidence of partial scalar invariance. 
```{r}
#Freeing factor loadings.
model3b <- cfa(cfa.model.m, estimator = "MLM", data=data, group="COND", group.equal=c("loadings","intercepts"), group.partial=c("PP2~1","VC1~1","VC5~1","TC2~1","TC8~1","TC9~1", "OCBI1~1", "OCBI2~1", "PR=~PP2", "PR=~PP6", "TC=~TC3", "TC=~TC9","OCBI=~OCBI4", "OCBO=~OCBO5","NMETHOD=~OCBO3"))
summary(model3b, fit.measures=TRUE, standardized=TRUE)
anova(model3b,model2)
```

Once these non-invariant elements of the model were freed, the results achieved non-significance (S-B X^2^ diff = -26.62, df = 25, p = 1.00). Thus our findings support partial strong invariance. Thus, we proceded on to testing invariance of latent variable covariances.
```{r}
model4 <- cfa(cfa.model.m,estimator = "MLM", data, group="COND", group.equal=c("loadings","intercepts","lv.covariances"), group.partial=c("PP2~1","VC1~1","VC5~1","TC2~1","TC8~1","TC9~1", "OCBI1~1", "OCBI2~1", "PR=~PP2", "PR=~PP6", "TC=~TC3", "TC=~TC9","OCBI=~OCBI4", "OCBO=~OCBO5","NMETHOD=~OCBO3"))
summary(model4, fit.measures=TRUE, standardized=TRUE)
anova(model3b, model4)
```

The results supported invariant latent variable covariances (S-B X^2^ diff = 7.55, df = 15, p = .94). The fit of the overall model was still mixed, with some fit indices suggesting unacceptable fit (X^2^ = 2786.30, df = 2105, p < .001; rCFI = 0.88, rTLI = 0.87, SRMR = .084) and another suggesting acceptable fit (rRMSEA = 0.053). 
```{r}
#Lavaan
require(lavaan)
require(semTools)

#Substantive model
substantive.model  <-  '
#Substantive factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
'
#Overall
s.model <- cfa(substantive.model, estimator = "MLM", data=data, group="COND", std.lv = TRUE)
summary(s.model, standardized = TRUE, fit.measures = TRUE)

#Control
s.model.c <- cfa(substantive.model, estimator = "MLM", data=control, std.lv = TRUE)
summary(s.model.c, standardized = TRUE, fit.measures = TRUE)
##Notes: All parcels reflected their respective factor and in the expected direction. 

#Treatment
s.model.t <- cfa(substantive.model, estimator = "MLM", data=treatment, std.lv = TRUE)
summary(s.model.t, standardized = TRUE, fit.measures = TRUE)
##Notes: One of the in-role behavior parcels did not reflect their respective factor (IRBp3, p = .068). This could be due to reduced internal consistency that would be expected as a function of randomizing items within a scale. 

#Regression effects
regression.model  <-  '
#Substantive factors
  PR =~ PPp1 + PPp2 + PPp3 + PPp4
  VC =~ VCp1 + VCp2 + VCp3
  TC =~ TCp1 + TCp2 + TCp3
  IRB =~ IRBp1 + IRBp2 + IRBp3
  OCBI =~ OCBIp1 + OCBIp2 + OCBIp3
  OCBO =~ OCBOp1 + OCBOp2 + OCBOp3
#Regressions
  VC ~ PR
  TC ~ PR
  IRB ~ PR
  OCBI ~ PR
  OCBO ~ PR
'
#Overall
r.model.o <- cfa(regression.model, estimator = "MLM", data=data, group="COND", std.lv = TRUE)
summary(r.model.o, standardized = TRUE, fit.measures = TRUE)

#Control
r.model.c <- cfa(regression.model, estimator = "MLM", data=control, std.lv = TRUE)
summary(r.model.c, standardized = TRUE, fit.measures = TRUE)
##Notes: All parcels reflected their respective factor and in the expected direction. 

#Treatment
r.model.t <- cfa(regression.model, estimator = "MLM", data=treatment, std.lv = TRUE)
summary(r.model.t, standardized = TRUE, fit.measures = TRUE)
##Notes: One of the in-role behavior parcels did not reflect their respective factor (IRBp3, p = .068). This could be due to reduced internal consistency that would be expected as a function of randomizing items within a scale. 

```
```
\newpage
# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
